\documentclass[../main.tex]{subfiles}

\begin{document}
\subsection{Datasets}
Numerous data sets related to this area have been used in the existing literature.

\subsubsection{CLEF-TAR (2017, 2018, 2019)}

CLEF-TAR is a dataset that was released as part of CLEF eTASK 2, and is available on github\footnote{https://github.com/CLEF-TAR/tar} \cite{kanoulas_clef_2017, kanoulas_clef_2018, kanoulas_clef_2019}. Originally designed with document ranking as the primary focus, the information contained within the data set allows for the subprocess simulation of the title and abstract selection of the SR procedure, using published real-world Cochrane SRs. Each year, this data set was incrementally updated and Table \ref{tab:training_dataset_clef} outlines the scope of the issue and succinctly highlights the presence of a large imbalance of the TIR class. Diagnostic test accuracy SRs (DTA) summarise a test accuracy, while intervention reviews assess the effectiveness/safety of a treatment, vaccine, device, preventative measure, procedure, or policy. Delineation between the types of SRs is not required for research within this Ph.d.


\begin{table*}
    \centering
    \begin{tabular}{|c|c|c|c|c|c|}
    \hline
        Dataset & Total SRs & Type(s) of SR & T & TR & TR/T\\   \hline
        CLEF 2017 & 50 & DTA & 269628 & 4661  & 0.017 \\   \hline
        CLEF 2018 & 50 & DTA & 266657 & 4351 & 0.016\\   \hline
             CLEF   2019 & 80 & DTA & 485153 & 8315 & 0.017\\   \hline
               CLEF  2019 & 80 & Intervention & 31644 &  448 & 0.014 \\   \hline
Synergy & 26 & Not Applicable & 169288 &  2834 & 0.017 \\   \hline
    \end{tabular}
    \caption{Training Dataset sizes for the TAR datasets}
    \label{tab:training_dataset_clef}
\end{table*}

Of importance, the CLEF dataset did not provide the titles or abstracts for each research found in the Identification Phase, rather relying on the users to download them for experimentation. This is an important oversight of the data set as titles and abstracts can be updated or retracted post-publication, meaning, fair comparison across time might become increasingly challenging. Within this Ph.D. I intend to use this recently collected source \(2024\) of titles/abstracts that have been collected as part of other work in this area, which has extracted all titles and abstracts for \(\textbf{T}\) within the CLEF dataset \cite{goharian_reproducibility_2024}\footnote{https://github.com/ielab/goldilocks-reproduce}.

\subsubsection{Synergy Dataset}
The Synergy dataset \cite{de_bruin_synergy_2023}, while less frequently used in the literature, offers a more contemporary collection of SRs\footnote{https://github.com/asreview/synergy-dataset/tree/master}. This data set comprises 26 SRs that span multiple domains, with a predominant focus on the medical field (20 out of 26 reviews). Reviews included in this data set range from 2002 to 2020, potentially providing more recent information compared to the CLEF data set.
The Synergy data set features diverse domains, allowing cross-domain analysis despite its primary focus on medical reviews. It also includes an expanded variable set. In addition to the basic information found in the CLEF dataset, Synergy incorporates authorship details, referenced works, and publication years, all sourced from the OpenAlex API.
Due to its more recent compilation and limited use in existing research, this dataset could be used to externally validate pre-trained language models.The inclusion of SRs from nonmedical domains, such as computer science, allows evaluations on the transferability of TAR approaches across different fields.
Synergy's TR/T ratio of 0.017 is consistent with the class imbalance observed in the CLEF datasets, making it suitable for comparative studies and model evaluation in the context of title and abstract selection tasks.

\subsubsection{TREC Total Recall Track Dataset (2015, 2016)}
The TREC Total Recall Track produced data sets specifically designed for high-recall retrieval tasks, similar to those encountered in SRs\footnote{https://trec.nist.gov/data/total-recall/}\cite{roegiest_trec_2015, grossman_trec_2016}. This data set simulates scenarios where the goal is to find all or nearly all relevant documents in a collection, which aligns closely with the objectives of the title and abstract screening phase in SRs. The data set includes a corpus of documents, topics (which can be seen as analogous to research questions in SRs), and relevance judgments.

\subsubsection{Jeb Bush Emails Dataset}
The Jeb Bush Emails dataset is an unconventional choice for TAR research, originally consisting of emails released by former Florida Governor Jeb Bush\footnote{https://ab21www.s3.amazonaws.com/JebBushEmails-Text.7z}. This data set is suitable for TAR experiments because of its large size and the presence of both relevant and irrelevant documents. Although not directly related to SRs, it provides a real-world corpus that can be used to simulate document classification tasks inherent in the SR process.

\subsubsection{RCV1-v2 Dataset}
The RCV1-v2 (Reuters Corpus Volume 1, Version 2) is a large, manually categorised newswire data set \cite{lewis_rcv1_2004} that was published by Reuters between August 20, 1996, and August 19, 1997\footnote{https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/datasets/\_rcv1.py}. The dataset features 804,414 documents with multi-label classification across 103 topic categories, organised in a hierarchy. The documents are provided in XML format with rich metadata and the content is primarily English news stories covering a wide range of topics.
Although not originally designed for SRs, RCV1-v2 has been used in various text classification and information retrieval tasks.  In the context of SRs and TAR, the use of the RCV1-v2 data set lies in the simulation of approaches on a large-scale dataset to test the scalability and efficiency of screening algorithms and to evaluate any potential transferability of the approaches.

RCV1-v2 dataset is adapted for use in AL by denoting all documents as \textbf{$T$}, \textbf{$T_R$} as all documents having a specific label, and those without it, as by treating the entire corpus as \textbf{$T$}, \textbf{$T_{IR}$}, we can approximate the binary classification challenge of title and abstract Screening within SRs.

\end{document}

