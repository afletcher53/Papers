\relax 
\providecommand \babel@aux [2]{\global \let \babel@toc \@gobbletwo }
\@nameuse{bbl@beforestart}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\abx@aux@refcontext{none/global//global/global/global}
\abx@aux@cite{0}{BMCMedicalResearch}
\abx@aux@segm{0}{0}{BMCMedicalResearch}
\abx@aux@cite{0}{yasunaga2022linkbertpretraininglanguagemodels}
\abx@aux@segm{0}{0}{yasunaga2022linkbertpretraininglanguagemodels}
\abx@aux@cite{0}{goharian_reproducibility_2024}
\abx@aux@segm{0}{0}{goharian_reproducibility_2024}
\abx@aux@cite{0}{guDomainSpecificLanguageModel2021}
\abx@aux@segm{0}{0}{guDomainSpecificLanguageModel2021}
\abx@aux@cite{0}{jinWhatDiseaseDoes2020}
\abx@aux@segm{0}{0}{jinWhatDiseaseDoes2020}
\abx@aux@cite{0}{hendrycksMeasuringMassiveMultitask2021}
\abx@aux@segm{0}{0}{hendrycksMeasuringMassiveMultitask2021}
\providecommand \oddpage@label [2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Leveraging Citation Networks for Medical TAR}{1}{subsection.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.1}Relation analysis improves CAL TAR performance}{1}{subsubsection.0.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Comparison of document processing in traditional BERT versus LinkBERT. Traditional BERT (top) randomly groups documents into context windows, while LinkBERT (bottom) uses citation relationships to create meaningful document groupings for pretraining. The citation-based grouping ensures that semantically related documents are processed together during masked language modeling tasks.}}{2}{figure.1}\protected@file@percent }
\newlabel{fig:linkbert-comparison}{{1}{2}{Comparison of document processing in traditional BERT versus LinkBERT. Traditional BERT (top) randomly groups documents into context windows, while LinkBERT (bottom) uses citation relationships to create meaningful document groupings for pretraining. The citation-based grouping ensures that semantically related documents are processed together during masked language modeling tasks}{figure.1}{}}
\abx@aux@cite{0}{lefebvre2011cochrane}
\abx@aux@segm{0}{0}{lefebvre2011cochrane}
\abx@aux@cite{0}{akers2009systematic}
\abx@aux@segm{0}{0}{akers2009systematic}
\abx@aux@cite{0}{briscoeConductReportingCitation2019}
\abx@aux@segm{0}{0}{briscoeConductReportingCitation2019}
\abx@aux@cite{0}{MECIRManualCochrane}
\abx@aux@segm{0}{0}{MECIRManualCochrane}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.2}Direct citation network mining within medicine research}{3}{subsubsection.0.1.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Performance comparison across different collections and models}}{4}{table.1}\protected@file@percent }
\newlabel{tab:results}{{1}{4}{Performance comparison across different collections and models}{table.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Average R-precision of each FPT epoch for CLEF dataset}}{4}{table.2}\protected@file@percent }
\newlabel{tab:results-average}{{2}{4}{Average R-precision of each FPT epoch for CLEF dataset}{table.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Results from literature search on citation index arxiv and pubmed demonstrating absence of related works, ran on 13th November 2024}}{5}{figure.2}\protected@file@percent }
\newlabel{fig:search-results}{{2}{5}{Results from literature search on citation index arxiv and pubmed demonstrating absence of related works, ran on 13th November 2024}{figure.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.3}Extending current citation network mining approaches}{6}{subsubsection.0.1.3}\protected@file@percent }
\newlabel{fig:indirect-citation}{{A.3}{7}{Extending current citation network mining approaches}{subsubsection.0.1.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Precision of relevant documents within pools using BCS, FCS and both together against that of the entire document collection}}{7}{table.3}\protected@file@percent }
\newlabel{tab:citation-network-mining}{{3}{7}{Precision of relevant documents within pools using BCS, FCS and both together against that of the entire document collection}{table.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.4}Research Question 1}{7}{subsubsection.0.1.4}\protected@file@percent }
\abx@aux@cite{0}{llm4g}
\abx@aux@segm{0}{0}{llm4g}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.5}Graph Neural Networks}{8}{subsubsection.0.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.6}LLMs and citation network mining}{8}{subsubsection.0.1.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Node feature enrichment process using LLM and LM}}{9}{figure.3}\protected@file@percent }
\newlabel{fig:llm4g}{{3}{9}{Node feature enrichment process using LLM and LM}{figure.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.7}Research Question 2}{9}{subsubsection.0.1.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Notes on Graph Neural Networks}{10}{subsection.0.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {C}Message Passing Neural Networks}{10}{subsection.0.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {C.1}Message}{11}{subsubsection.0.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {C.2}Aggregate}{11}{subsubsection.0.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {C.3}Update}{11}{subsubsection.0.3.3}\protected@file@percent }
\abx@aux@read@bbl@mdfivesum{nobblfile}
\abx@aux@read@bblrerun
\gdef \@abspage@last{11}
