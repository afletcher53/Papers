\documentclass[../main.tex]{subfiles}

\begin{document}

\subsection{Citations in medical research}

Medical research communication follows a standardised format and occurs primarily through publication \cite{BMCMedicalResearch}. A component of that research is a literature review that contains established facts about a topic. These established facts are corroborated with references to their source of evidence. Citations adhere to a standard format and are typically listed at the end of the research article.

It follows, then, that relations between research can be elicited from the analysis of these ciations, with the assumption being that studies that cite, or are cited, by a research article are relevant to that research.

\subsubsection{Improvements in encoder performance using dzirect citations}

Medical CAL TAR has already benefited from citation utilisation. To date, the most performant encoder model ($BioLinkBERT_{base}$) \cite{yasunaga2022linkbertpretraininglanguagemodels} on the CLEF dataset in a CAL setting, leveraged citations between research to achieve state-of-the-art within the CAL process \cite{goharian_reproducibility_2024}. The $LinkBERT$ approach was to view a pertaining corpus as a graph of documents, with each document being a vertex and hyperlinks forming edges between documents. These linked documents were then placed within the same context, which was different from that of $BERT$ random document allocation, in which no linkage between documents within a context window is required. $LinkBERT$ differs from curriculum learning, where a model is provided with examples of increasing difficulty, as the context windows were not ordered by difficulty. A domain-specific variant of $LinkBERT$, $BioLinkBERT$, was created, which was pretrained only on  PubMed articles, with linkage of documents being determined through citations of that research\footnote{https://huggingface.co/michiyasunaga/BioLinkBERT-base}. Models were then trained using standard masked language modelling and next-sentence prediction. The performance of a base model (100M parameters) and a large model (340M parameters) were compared to $PubMedBert$\footnote{https://huggingface.co/microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext} in BLURB\cite{guDomainSpecificLanguageModel2021}, MedQA-USMLE\cite{jinWhatDiseaseDoes2020}, and MMLU-professional medicine (medical-specific downstream benchmark tasks)\cite{hendrycksMeasuringMassiveMultitask2021}. $BioLinkBERT_{large}$ achieved state-of-the-art on all reported benchmarks, with an improvement in the BLURB score of 3.2\% above PubMedBERT. 
\begin{figure}[t]
    \centering
    \begin{tikzpicture}[
        doc/.style={rectangle, draw, fill=blue!20, minimum width=2.5cm, minimum height=1cm},
        window/.style={rectangle, draw, fill=purple!20, minimum width=4cm, minimum height=1cm},
        mlm/.style={rectangle, draw, fill=orange!20, minimum width=4cm, minimum height=0.8cm},
        node distance=2cm,
        scale=0.9
    ]
    % Traditional BERT (top)
    \node[align=center] at (-3,5) {\Large Traditional BERT};
    \node[doc] (a1) at (-6,3.5) {Cancer Paper};
    \node[doc] (a2) at (0,3.5) {Climate Paper};
    \node[window] (b1) at (-3,2) {Random Context};
    \node[mlm] (m1) at (-3,0.5) {MLM: The [MASK] study};
    
    \draw[->] (a1) -- (b1);
    \draw[->] (a2) -- (b1);
    \draw[->] (b1) -- (m1);
    
    % LinkBERT (bottom) - with increased horizontal spacing
    \node[align=center] at (-3,-1) {\Large LinkBERT};
    \node[doc] (c1) at (-8,-2.5) {Cancer Study};
    \node[doc] (c2) at (-3,-2.5) {Biomarkers};
    \node[doc] (c3) at (2,-2.5) {Treatment};
    
    \node[window] (d1) at (-6,-4) {Linked Context};
    \node[mlm] (n1) at (-6,-5.5) {MLM: Cancer [MASK]};
    
    \node[window] (d2) at (0,-4) {Linked Context};
    \node[mlm] (n2) at (0,-5.5) {MLM: Treatment [MASK]};
    
    % Citations with curved arrows
    \draw[->, bend angle=30] (c1) to[bend left] node[midway,above] {\small cites} (c2);
    \draw[->, bend angle=30] (c2) to[bend left] node[midway,above] {\small cites} (c3);
    
    % Context connections with slight bends to avoid overlap
    \draw[->] (c1) -- (d1);
    \draw[->] (c2) to[bend right=15] (d1);
    \draw[->] (c2) to[bend left=15] (d2);
    \draw[->] (c3) -- (d2);
    \draw[->] (d1) -- (n1);
    \draw[->] (d2) -- (n2);
    
    \end{tikzpicture}
    \caption{Comparison of document processing in traditional BERT versus LinkBERT. Traditional BERT (top) randomly groups documents into context windows, while LinkBERT (bottom) uses citation relationships to create meaningful document groupings for pretraining. The citation-based grouping ensures that semantically related documents are processed together during masked language modeling tasks.}
    \label{fig:linkbert-comparison}
\end{figure}


In the previously reported Goldilock Reproduce study, where an Enocder CAL approach was used, $BioLinkBERT_{base}$ formed the classifier. This author recreated their experiment with $BioBERT_{large}$\footnote{https://huggingface.co/michiyasunaga/BioLinkBERT-large} as a classifier model and achieved higher performance in R-Precision in 7 of 12 datasets/policy combinations.  The Friedman test for individual datasets found significant differences between the FPT epochs 4 out of 12 times. However, when considering all datasets together, there was no significant difference between the FPT epochs and R-precision for relancy selection policy or uncertainty selection policy. This indicates that the ``Goldilocks problem'', which was previously reported in the literature in non-medical domains was not apparent when using the large BiolinkBERT model for the CLEF dataset within a CAL process, indicating that further pretraining of models was unneccesary as it does not produce an statistically significant improvement in R-precision. The average R-precision of each FPT epoch is reported in Table \ref{tab:results-average}, with the highest R-precision for relevancy selection policy being 0.847 at further pretrain epoch 2 and the highest R-precision for uncertainty being  0.832 at further pretrain epoch 1. 

Key findings from this research is that an optimal pretraining epoch is unlikely to be found within the CLEF dataset and hence not a viable avenue for future research. In terms of experimental design, certain hyperparameters were chosen without clear reasoning (such as batch size being 25, fine tuning for 20 epochs and stopping after 501 documents labelled). This limitation is thought to be a barrier to improving the performance of the encoder CAL process within that experimental framework, given that reported R-Precision values are already close to the natural ceiling of 1 (with R-Precision reaching 0.945 in some cases). In datasets where there is worse performance, (0.82 R-precision for relevancy, 0.791 R-precision for uncertainty in the CLEF 2019 dataset) comprehension of why is obsfucated by the use of LLMs. Furturemore, using a more performance/larger model is likely to be fruitful future research, however it is dependent on the availability and development of superior models. This undertaken research 
however highlight that leveraging citations themselves was valuable to the CAL process.

A fundamental question emerges from this research: Is contextual understanding of references truly necessary for effective CAL? Several factors suggest that citation networks alone might be sufficient and potentially superior. First, citations themselves represent a form of knowledge distillation, where domain experts have already identified meaningful relationships between documents. Second, analysing reference networks is computationally more efficient than processing full textual contexts. Third, citation network models tend to be more stable when updated, compared to contextual models. Fourth, the contextualization of citation networks may actually introduce noise into what would otherwise be clear citation signals.

While $BioLinkBERT$ represents a sophisticated approach that combines citation networks with contextual language understanding, this integration presents both advantages and limitations. The model's ability to capture complex semantic relationships between documents is valuable, but the contextual processing introduces potential inefficiencies. During pretraining, when linked documents are placed in the same context, the model must process all content within those documents—including sections that may be tangential or unrelated to the citing paper's specific reference. This contextual noise could potentially dilute the precision of the more direct relationships that citations inherently represent. In contrast, pure citation links directly capture intentional scholarly connections made by domain experts, providing a cleaner signal without the additional complexity of processing potentially irrelevant contextual information.
\begin{table}[htbp]
    \centering
    \footnotesize
    \setlength{\tabcolsep}{4pt}
    \begin{tabular}{l>{\raggedright\arraybackslash}p{1.2cm}ccccc}
    \hline
    \textbf{Collection} & \textbf{Dataset size} & \textbf{Model} & \multicolumn{2}{c}{\textbf{R-Precision (↑)}} & \multicolumn{2}{c}{\textbf{Friedman (p)}} \\
    \cline{4-7}
    & & & \textbf{Rel.} & \textbf{Unc.} & \textbf{Rel.} & \textbf{Unc.} \\
    \hline
    \multirow{6}{*}{\makecell[l]{Clef 2019\\dta test}} & 
    \multirow{6}{*}{8} & BiolinkBert-Base-ep0 & \textbf{0.909} & \textbf{0.857} & \multicolumn{2}{c}{---} \\
    & & BiolinkBert-Large-ep0 & 0.897 & 0.803 & \multirow{5}{*}{0.914} & \multirow{5}{*}{0.632} \\
    & & BiolinkBert-Large-ep1 & 0.827 & 0.832 & & \\
    & & BiolinkBert-Large-ep2 & 0.812 & 0.774 & & \\
    & & BiolinkBert-Large-ep5 & 0.841 & 0.814 & & \\
    & & BiolinkBert-Large-ep10 & 0.881 & 0.846 & & \\
    \hline
    \multirow{6}{*}{\makecell[l]{Clef 2017\\test}} & 
    \multirow{6}{*}{30} & BiolinkBert-Base-ep0 & 0.812 & 0.794 & \multicolumn{2}{c}{---} \\
    & & BiolinkBert-Large-ep0 & 0.828 & 0.797 & \multirow{5}{*}{\textbf{\textless0.05}} & \multirow{5}{*}{\textbf{\textless0.05}} \\
    & & BiolinkBert-Large-ep1 & 0.826 & \textbf{0.827} & & \\
    & & BiolinkBert-Large-ep2 & \textbf{0.858} & 0.804 & & \\
    & & BiolinkBert-Large-ep5 & 0.827 & 0.777 & & \\
    & & BiolinkBert-Large-ep10 & 0.799 & 0.757 & & \\
    \hline
    \multirow{6}{*}{\makecell[l]{Clef 2017\\train}} & 
    \multirow{6}{*}{20} & BiolinkBert-Base-ep0 & \textbf{0.838} & 0.761 & \multicolumn{2}{c}{---} \\
    & & BiolinkBert-Large-ep0 & 0.778 & 0.765 & \multirow{5}{*}{\textbf{\textless0.05}} & \multirow{5}{*}{0.28} \\
    & & BiolinkBert-Large-ep1 & 0.808 & 0.789 & & \\
    & & BiolinkBert-Large-ep2 & 0.767 & 0.701 & & \\
    & & BiolinkBert-Large-ep5 & 0.816 & 0.786 & & \\
    & & BiolinkBert-Large-ep10 & 0.827 & \textbf{0.796} & & \\
    \hline
    \multirow{6}{*}{\makecell[l]{Clef 2018\\test}} & 
    \multirow{6}{*}{30} & BiolinkBert-Base-ep0 & 0.794 & 0.780 & \multicolumn{2}{c}{---} \\
    & & BiolinkBert-Large-ep0 & 0.789 & 0.774 & \multirow{5}{*}{0.52} & \multirow{5}{*}{0.50} \\
    & & BiolinkBert-Large-ep1 & \textbf{0.812} & 0.790 & & \\
    & & BiolinkBert-Large-ep2 & 0.797 & \textbf{0.791} & & \\
    & & BiolinkBert-Large-ep5 & 0.763 & 0.773 & & \\
    & & BiolinkBert-Large-ep10 & 0.763 & 0.769 & & \\
    \hline
    \multirow{6}{*}{\makecell[l]{Clef 2019\\DTA int.\\train}} & 
    \multirow{6}{*}{20} & BiolinkBert-Base-ep0 & 0.939 & 0.923 & \multicolumn{2}{c}{---} \\
    & & BiolinkBert-Large-ep0 & 0.939 & 0.902 & \multirow{5}{*}{0.78} & \multirow{5}{*}{0.50} \\
    & & BiolinkBert-Large-ep1 & 0.941 & 0.935 & & \\
    & & BiolinkBert-Large-ep2 & 0.948 & 0.921 & & \\
    & & BiolinkBert-Large-ep5 & 0.952 & 0.945 & & \\
    & & BiolinkBert-Large-ep10 & \textbf{0.945} & \textbf{0.947} & & \\
    \hline
    \multirow{6}{*}{\makecell[l]{Clef 2019\\DTA int.\\test}} & 
    \multirow{6}{*}{20} & BiolinkBert-Base-ep0 & \textbf{0.934} & \textbf{0.900} & \multicolumn{2}{c}{---} \\
    & & BiolinkBert-Large-ep0 & 0.899 & 0.856 & \multirow{5}{*}{0.87} & \multirow{5}{*}{\textbf{\textless0.05}} \\
    & & BiolinkBert-Large-ep1 & 0.904 & 0.840 & & \\
    & & BiolinkBert-Large-ep2 & 0.909 & 0.878 & & \\
    & & BiolinkBert-Large-ep5 & 0.882 & 0.835 & & \\
    & & BiolinkBert-Large-ep10 & 0.865 & 0.841 & & \\
    \hline
   
    \end{tabular}
    \caption{Performance comparison across different collections and models}
    \label{tab:results}
\end{table}

\begin{table}[htbp]
    \centering
    \caption{Average R-precision of each FPT epoch for CLEF dataset}
    \begin{tabular}{l>{\raggedright\arraybackslash}p{1.2cm}ccccc}
        \textbf{Policy} & \textbf{ep0} & \textbf{ep1} & \textbf{ep2} & \textbf{ep5} & \textbf{ep10} \\
        \hline
        Uncertainty & 0.813 & 0.832 & 0.813 & 0.815 & 0.814 \\
        Relevancy & 0.840 & 0.845 & 0.847 & 0.842 & 0.835 \\
    \hline

    \end{tabular}
    \label{tab:results-average}
\end{table}



\subsubsection{Direct citation network mining within medicine research}

Despite these more complex citation network mining approaches being somewhat performant, simpler, more robust approaches to citation network mining already exist within medicial research. Let G be a citation graph where:

\begin{itemize}
    \item $D_i$ represents a research article of interest as a vertex in G
    \item $D_{ip}$ represents the set of articles referenced by $D_i$
    \item $D_{if}$ represents the set of articles that reference $D_i$
    \item Both sets are subsets of G: $D_{ip}, D_{if} \subset G$
    \item $D_{ip} \cap D_{if} = \emptyset$, so searching both sets will provide different relevant articles
\end{itemize}

Relevancy is defined as a function $R: D \rightarrow [0,1]$, where:

\begin{itemize}
    \item 0 denotes no relevance
    \item 1 denotes maximum relevance
    \item For any set of documents $D_{set}$, relevancy is defined as $R(D_{set}) = {R(d) | d \in D_{set}}$
\end{itemize}

Two primary search approaches are defined:
\begin{itemize}
    \item Backward citation searching (BCS): examining all articles in $D_{ip}$\cite{lefebvre2011cochrane, akers2009systematic}
    \item Forward citation searching (FCS): examining all articles in $D_{if}$*\footnote{FCS involves using a citation index to identify studies that cite a source study. A citation index is a database of scholarly articles and their citations, such as PubMed, Google Scholar, Scopus or OpenAlex}
\end{itemize}

These approaches are both simple and effective, inherently respecting temporal relationships between research articles (i.e., you cannot cite a paper that has yet to be published). More so, BCS and FCS are recommended to be used in the identification phase of Cochrane systematic review generation, with 87\% of Cochrane reviews between November 2016 to January 2017 reporting the use of BCS and 9\% reporting the use of FCS\cite{briscoeConductReportingCitation2019}. Recommendations regarding BCS/FCS in the the search stage are found within the Cochrane Handbook, with BCS being mandatory (C30), and FCS not being mentioned \cite{MECIRManualCochrane}. No such recommendations exist for BCS and FCS use within the screening phase within the Cochrane Handbook. 
Additionally, there is a paucity of research outlining the use of BCS and FCS within the active learning process when searching multiple citation indexes (Search Strategy outlined in Figure \ref{fig:search-results}). An important clarification is required to understand the novality of this approach. As previously mentioned, this PhD is focusing on the title and abstract phase within the screening phase of systematic review generation. BCS and FCS is traditionally used in the identification phase only (see Figure \ref{fig:selection_and_screening} for overview of phases of the systematic review process), with title and abstracts being screened to reduce the work required in the more costly full text screening phase. This is not a limiting factor for comptuation, and hence restriction of this phase to just title and abstracts is redundant. This amendment to the screening process is also not intended to remove the full text screening phase, as each phase has a different requirement, with title and abstract screening being focused only on relevancy to the topic, and full text screening being focused on quality and eligibility of the research.

\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}[
        node distance = 0.5cm,
        box/.style={rectangle, draw, text width=4cm, minimum height=1cm, align=center},
        query/.style={rectangle, draw, fill=gray!10, text width=4cm, minimum height=2.5cm, align=center}
    ]
        % Search queries side by side
        \node[query] (q1) {Search Query 1:\\
            (``backward citation'' OR\\``citation analysis'')\\
            AND ``active learning''};
        \node[query] (q2) [right=0.5cm of q1] {Search Query 2:\\
            (``citation graph'' OR\\``citation network'') AND\\(``active learning'' OR\\``interactive learning'')};
        \node[query] (q3) [right=0.5cm of q2] {Search Query 3:\\
            (``bibliometric analysis'' OR\\``reference mining'') AND\\(``selective sampling'' OR\\``query learning'')};
            
        % Results box below
        \node[box] (results) [below=1cm of q2] {No Related Documents Found};
            
        % Arrows
        \draw[-stealth] (q1) -- (results);
        \draw[-stealth] (q2) -- (results);
        \draw[-stealth] (q3) -- (results);
        
    \end{tikzpicture}
    \caption{Results from literature search on citation index arxiv and pubmed demonstrating absence of related works, ran on 13th November 2024}
    \label{fig:search-results}
\end{figure}
\begin{center}
\begin{tikzpicture}[
    > = {Stealth[scale=1.2]},
    vertex/.style = {circle, draw, minimum size=1.2cm, inner sep=1pt},
    ref_edge/.style = {->, thick, blue},
    label_style/.style = {font=\small}
]

% Main document Di
\node[vertex, fill=yellow!20] (Di) at (0,0) {$D_i$};

% Past references (Dip) - More fanned out
\node[vertex] (Dip1) at (-3,2.5) {$D_{ip1}$};
\node[vertex] (Dip2) at (0,3) {$D_{ip2}$};
\node[vertex] (Dip3) at (3,2.5) {$D_{ip3}$};

% Future references (Dif) - More fanned out
\node[vertex] (Dif1) at (-3,-2.5) {$D_{if1}$};
\node[vertex] (Dif2) at (0,-3) {$D_{if2}$};
\node[vertex] (Dif3) at (3,-2.5) {$D_{if3}$};

% Edges for past references
\draw[ref_edge] (Di) -- (Dip1);
\draw[ref_edge] (Di) -- (Dip2);
\draw[ref_edge] (Di) -- (Dip3);

% Edges for future references
\draw[ref_edge] (Dif1) -- (Di);
\draw[ref_edge] (Dif2) -- (Di);
\draw[ref_edge] (Dif3) -- (Di);

% Labels for sets
% \node[label_style] at (-4,2) {$D_{ip}$ (Referenced by $D_i$)};
% \node[label_style] at (-4,-2) {$D_{if}$ (References to $D_i$)};

% Relevancy function examples
\node[label_style] at (5,2.5) {$R(D_{ip1}) = 1$};
\node[label_style] at (5,2) {$R(D_{ip2}) = 1$};
\node[label_style] at (5,1.5) {$R(D_{ip3}) = 0$};
\node[label_style] at (5,-1.5) {$R(D_{if1}) = 1$};
\node[label_style] at (5,-2) {$R(D_{if2}) = 0$};
\node[label_style] at (5,-2.5) {$R(D_{if3}) = 0$};

% Time arrow (vertical)
\draw[{Stealth[scale=1.5]}->, thick] (7,3.5) -- (7,-3.5);
\node[label_style] at (7,3.8) {Past};
\node[label_style] at (7,-3.8) {Future};

\end{tikzpicture}
\end{center}

While the citation network mining approach could be used to group articles of interest within a context window, similar to that of $BioLinkBERT$ (which used FCS, not BCS), pretraining a model from scratch using this approach is obviously outside the possibility of this PhD project, due to hardware constraints. However, citation network mining can be used to augment the CAL process in ways that overcome some of the limitatitions of this process. 

Firstly, CAL requires labelled data to train a classifier model, which is assumed to perform better with more data points. Encoder CAL approaches suffer disproportionately to that of feature-based CAL approaches due to their need for larger amounts of training data to effectively learn meaningful representations. This is because encoder models like BERT need to learn complex contextual relationships between words and concepts, whereas feature-based models can rely on simpler statistical patterns. When working with limited labeled data in the early stages of screening, encoder models may struggle to generalise well, potentially leading to suboptimal performance in identifying relevant documents. As discussed in the Encoder CAL process, often a single sample seed document is used during the first epoch for fine-tuning. A better approach would be to exhaust the citation network of that seed document first for labelling, before using revealed relevant documents to fine-tune the model, potentially resulting in a more performant model at the earlier stages of screening with less oracle cost. This potentially overcomes limitations of the Encoder CAL process, and also reduces computational costs (as citation network mining is less computationally expensive than fine-tuning). 

Early work performed by the Author on a subset of the CLEF dataset demonstrates that exhausting the BCS network of a seed document resulted in greater R-Precision values when compared to that of the Encoder CAL process at similar points. The logical, and simple augmentation of the encoder CAL approach would be to exhaust both BCS and FCS networks of a seed document prior to initiating the encoder CAL process. 
\subsubsection{Extending current citation network mining approaches}



One of the issues with citation network mining is that it is not able to identify indirect citations. For example, if $D_i$ references $D_{ip1}$ and $D_{ip1}$ references $D_{ip2}$, then $D_i$ and $D_{ip2}$ are related, even though $D_i$ does not cite $D_{ip2}$ directly. This limitation would prevent the use of citation network mining to replace the encoder CAL process entirely. Other citation network mining approaches have been proposed to address this issue:


\begin{center}
    \begin{tikzpicture}[
        > = {Stealth[scale=1.2]},
        vertex/.style = {circle, draw, minimum size=1.2cm, inner sep=1pt},
        ref_edge/.style = {->, thick, blue},
        indirect_edge/.style = {->, thick, red, dashed},
        label_style/.style = {font=\small}
    ]
    
    % Main document Di
    \node[vertex, fill=yellow!20] (Di) at (0,0) {$D_i$};
    
    % Direct reference
    \node[vertex] (Dip1) at (-2,2) {$D_{ip1}$};
    
    % Indirect reference
    \node[vertex] (Dip2) at (2,4) {$D_{ip2}$};
    
    % Add edges
    \draw[ref_edge] (Di) -- (Dip1) node[midway, left] {Direct};
    \draw[ref_edge] (Dip1) -- (Dip2) node[midway, right] {Direct};
    \draw[indirect_edge] (Di) to[bend right] node[midway, right] {Indirect} (Dip2);
    
    % Time arrow (vertical)
    \draw[{Stealth[scale=1.5]}->, thick] (4,4.5) -- (4,-1);
    \node[label_style] at (4,4.8) {Past};
    \node[label_style] at (4,-1.3) {Future};
    
    % Legend
    \node[label_style] at (6,2) {Legend:};
    \draw[ref_edge] (5,1.5) -- (7,1.5) node[right] {Direct citation};
    \draw[indirect_edge] (5,1) -- (7,1) node[right] {Indirect citation};
    
    \end{tikzpicture}
    \end{center}

    \hl{Section about adding other features to the network, i.e. Author, topic}

    \subsubsection{Graph Neural Networks}
    
    \subsubsection{LLMs and citation network mining}

    The motivation for using LLMs and graph networks is to combine the structurality of graph citation networks with the ability of the LLMs to comprehend the semantic meaning of documents. As outlined, citation network graph analysis occurs above the document level through utilisation of extracted features about documents. LLMs are a natural replacement for extraction of features, as they possess the ability to understand semantic meaning of documents. The ultimate goal to use LLMs and graph networks is to complment and enhance isuses with the other. 

    Research has been conducted into the use of LLMs within the graph neural networks, and has developed a robust taxonomy for categorising the use of LLMs within graph networks \cite{llm4g}. 
    
    The first application of LLms within graph networks is to use LLM as an enchancer. Typically graph neural networks encode text into nodes using simple bag-of-words, skip-gram or TF-IDF. LLMs are able to encode text into nodes using more complex features, such as semantic meaning, which can be used within the graph neural networks. This can be further subdivided into explanation based and embedding based enchancers. 
    
    Explanation based enhancers query an LLM using prompting to capture higher level features about documents, which is used to enrich node representations prior to processing with a graph neural network, with the process being abstracted in in Figure \ref{fig:llm4g}. The approach used by https://arxiv.org/pdf/2305.19523 was to prompt GPT 3-5 with the abstract and text of a document along with a questions about that document using a zero-shot approach. The LLM reponse then forms features which are amended to the original node representations. Issues with this approach is that this requires domain specific knowledge, as features which are deemed important (and hence prompt used) are dependent on the domain of the research. It was performant on the pubmed domain, scoring greater node classification accuracy using this approach (0.9618 $\pm$ 0.0053) than utilising an LLM alone on pubmed data (0.9494 $\pm$ 0.0046).   

    \begin{figure}[h]
        \centering
        \begin{tikzpicture}[node distance=3cm,auto]
        
        % Define styles
        \tikzstyle{node} = [circle, minimum width=2.5cm, minimum height=2.5cm, draw=black!50, thick]
        \tikzstyle{process} = [rectangle, rounded corners, minimum width=2cm, minimum height=1cm, draw=black!50, thick]
        \tikzstyle{features} = [text width=2cm, align=left]
        
        % Initial node
        \node[node] (initial) {
            \begin{tabular}{c}
                Initial Node\\[2pt]
                h\textsubscript{orig}\\[2pt]
                \scriptsize{[Title \& Abstract]}
            \end{tabular}
        };
        
        % Process box
        \node[process, right of=initial, xshift=3cm] (process) {
            \begin{tabular}{c}
                LLM \\Prompting
            \end{tabular}
        };
        
        % Enriched node
        \node[node, right of=process, xshift=4cm] (enriched) {
            \begin{tabular}{c}
                Enriched Node\\[2pt]
                [h\textsubscript{orig}, h\textsubscript{pred}, h\textsubscript{expl}]\\[2pt]
                \scriptsize{[combined features]}
            \end{tabular}
        };
        
        % Arrows
        \draw[->, thick] (initial) -- (process) node[midway, above] {text input};
        \draw[->, thick] (process) -- (enriched) node[midway, above] {enriched features};
        

        
        \end{tikzpicture}
        \caption{Node feature enrichment process using LLM and LM}
        \label{fig:llm4g}
        \end{figure}

    
    \subsection{Notes on Graph Neural Networks}


  A node is represented by a feature matrix, which contains information about the document. This \textbf{Node feature matrix}, $X$, which has the dimensions of $m$ (the number of nodes) and $n$ (the number of features). $X \in \mathbb{R}^{m \times n}$. X does not have to be a square matrix, and does not encode any information about the structure of the graph. 

  Consider 3 research papers as nodes, with features: [Author, Title Length, Abstract Length, Citation Count]
  $X = \begin{bmatrix}
  \text{"Smith"} & 82 & 500 & 45 \\
  \text{"Johnson"} & 95 & 475 & 23 \\
  \text{"Zhang"} & 67 & 612 & 89
  \end{bmatrix}$
  Where $X \in \mathbb{R}^{3 \times 4}$ represents:
  
  3 papers (rows)
  4 features per paper (columns)
  Mixed data types (categorical and numerical)

  Structural information is encoded in the \textbf{adjacency matrix}, $A$, which has the dimensions of $m$ (the number of nodes) and $m$ (the number of nodes). $A \in \mathbb{R}^{m \times m}$. A encodes information about the structure of the graph, and is used to determine relationships between nodes. Conventionally the source nodes are the rows, and the destination nodes the columns of the matrix. 1 indicates an edge between the source node $u$ and destination node $v$. Note that there is a choice to make here, with the diagonal of the matrix being 0 or 1. This choice is based on wheter you consider the source node to be connected to itself. In cases where the representation of the node is dependent on itself and adjacent nodes, the diagonal should be set to 1. In the scenario of citation networks, the diagonal should be set to 1, as a paper is likely to reference and build upon its own findings throughout. By setting the diagonal to 0, it is akin to attempting to predict the representation of the node base only on its adjacent nodes, which is not the case in citation networks. If an adjaceny metrix is symmetric around it's diagonal, then the graph is undirected, otherwise it is directed (i.e. $U$ is connected to $V$ and $V$ is conencted to $U$). In citation networks, this is not the case, as because paper A cites paper B, it does not mean the reverse is true.    

  Consider the same 3 research papers, with the following adjacency matrix:
  $A = \begin{bmatrix}
  0 & 1 & 0 \\
  0 & 0 & 1 \\
  1 & 0 & 0
  \end{bmatrix}$
  Which represents the following graph:

  \begin{center}
    \begin{tikzpicture}[
        > = {Stealth[scale=1.2]},
        vertex/.style = {circle, draw, minimum size=1.2cm, inner sep=1pt},
        ref_edge/.style = {->, thick, blue},
        label_style/.style = {font=\small}
    ]

    \node[vertex] (Smith) at (0,0) {Smith};
    \node[vertex] (Johnson) at (2,0) {Johnson};
    \node[vertex] (Zhang) at (1,2) {Zhang};

    \draw[ref_edge] (Smith) -- (Johnson);
    \draw[ref_edge] (Zhang) -- (Johnson);
    \draw[ref_edge] (Smith) -- (Zhang);
    \end{tikzpicture}
    \end{center}

    With both $X$ and $A$ defined, we can numerically represent the graph. The node feature matrix $X$ is the initial/input node features, with our goal for learning on graphs to learn node embeddings $H \in \mathbb{R}^{N\times D}$ where $D$ is a chosen hidden dimension size. 


    \subsection{Message Passing Neural Networks}

    We need an approach that can work with the graph structure, which has variable number of nodes and edge conenctions between nodes. Historically with the CNN architecture, the input size was fixed, and the network was able to learn spatial invariance through the use of convolutional filters that were invariant to the location of the feature in the input. With graph structured data, the number of nodes and connections between nodes can vary for each graph, and spatial invariance is not invariant to the location of the feature in the graph. 

    Message Passing Neural Networks (MPNNs) are a type of graph neural network that can learn spatial invariance through the use of message passing between nodes. The basic idea of MPNNs is to iteratively update node representations by passing messages between connected nodes. This process is repeated for a fixed number of iterations, or until convergence.

    The process is defined as follows:
    \begin{itemize}
        \item Message: every node decides how to send information to neighboring nodes it is conencted to by edges
        \item Aggregate: nodes recieve messages from all their neighbors, who also passed messages and decides how to combine the information from all of its neighbors. 
        \item Update: each node decies how to combine neighbourhood information with its own information and updates it embedding for the next timestep. 
    \end{itemize}

    By doing this we have nodes pass each other information and disseminate information around the graph, allowing the network to learn spatial invariance.  This can be repeated for a fixed number of iterations ($K$), with the larger the value of $K$, the more the more diffuse the information around the graph becomes.

    Each section of the MPNN process in more detail:

    \subsubsection{Message}

    The source node $U$ will pass a message $m_{uv}$ to the destination node $V$. The message depends on the GNN architecture with the easiest example message being passed being $U$ node's feature $h_u$ vector to $V$.

    \subsubsection{Aggregate}

    The destination node $V$ will recieve messages from all its neighbouring nodes, and needs to decide how to combine the information from all of its neighbours. This is typically done using a sum, average or max pooling of the messages from all neighbouring nodes. It is important that the aggregation function has to be a permutation invariant function, as the order of the messages should not affect the output. 

    This gives us a combined neighbourhood node embedding, denoted as $h_{N(V)}$, where $N(V)$ is the set of all neighbouring nodes to $V$, meaning all nodes connected to $V$ by an edge. 

    $h_{N(v)}^{k+1} = AGGREGATE({h_u^k, \forall u \in N(v)})$

    \subsubsection{Update}

    Each node updates its own embedding based on the combined neighbourhood embedding and its own embedding from the previous timestep. 

    $h_v^{k+1} = \sigma(W \cdot CONCAT(h_v^k, h_{N(v)}^{k+1}))$

    \end{document}