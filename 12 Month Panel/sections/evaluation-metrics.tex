\documentclass[../main.tex]{subfiles}
\begin{document}
\subsection{Evaluation Metrics}
Evaluation metrics for SR TAR process can be categorised between assessing how well a classifier minimised the relevant documents excluded by the classifier with a set work budget (i.e. effectiveness) or the reduction in the reviewer's workload by excluding the maximum number of irrelevant documents while maintaining recall (efficiency).  The majority of the research produced within this will focus on improving the effectiveness of AL models within the medical TAR domain. The author chooses not to optimise the computational efficiency between approaches, rather to improve the final result achieved. This is for numerous reasons; however, the main two are that as computer processing increases, these practical limitation concerns become less and improvement in effectiveness will have a greater impact on SR usefulness than maximising efficiency.  The author aims to report the time taken to run the algorithms, time complexity and the hardware that ran upon, so that comparison to time taken by humans undertaken can occur.

\subsubsection{Recall@k}
In the context of SRs, achieving high recall is more critical than high precision. Recall represents the proportion of relevant documents correctly identified among all truly relevant documents \cite{omara-eves_using_2015}. This focus on recall may seem counterintuitive, but it is crucial for two reasons. First, each missed document could potentially contain significant information for the SR. Second, the initial screening is followed by a more precise full-text review (as outlined in the PRISMA workflow, Figure \ref{fig:selection_and_screening}), where precision is emphasised.
Although maximising recall is important, it is not practical to aim for 100\% due to diminishing returns. As recall approaches higher levels, the computational cost of screening additional documents increases substantially, often yielding minimal benefit. To balance effectiveness and efficiency, researchers of TAR for SR commonly use and consider recall @ 95\% as useful (that is, k = 0.95). This measure indicates the recovery achieved when 95\% relevant documents are recovered, striking a pragmatic balance between comprehensive coverage and resource use. A higher recall@k is considered a more effective approach.
Recall is calculated via:

\begin{equation}
\text{recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}
\end{equation}
Recall@95\% is calculated via:
\begin{equation}
\text{recall@95\%} = \frac{\text{TP}}{\text{TP} + \text{FN}}
\end{equation}
Where recall is calculated once an AL classifier achieves 95\% TP.

A small point on nomenclature: Historically, recall has been referred to in the medical literature as sensitivity. These are two different terms for the same metric, and the use of either term depends on the domain. Additionally, in the legal domain, there might be references to recall@75\% which is not as useful for the medical domain. Legal domains often prioritise based on cost-effectiveness, time-constraints and proportionability, which medical reviews require as close to absolute information as possible \cite{tsafnat_systematic_2014}.

\subsubsection{R-Precision}
This effectiveness metric determines, given the \textbf{$T_R$}, what proportion of documents returned by the approach within the total number of relevant documents were actually relevant \cite{manning_introduction_2008}. The best score for R-precision is 1 (i.e., all relevant documents were returned in the top \textbf{$T_R$} position). It allows for an adaptive cutoff for \textbf{$T_R$}, which adapts to the SR, and also considers precision. Note that this evaluation metric can only be used when the \textbf{$T_R$} for a query is known. It is calculated via:

\begin{equation}
\text{R-Precision} = \frac{\text{Relevant Documents in top } T_R}{T_R}
\end{equation}
\subsubsection{Work Saved Over Sampling (WSS@k)}

WSS@k is an efficiency metric that would be valuable to report on to enable other researchers in the field to compare their approaches to mine and, if appropriate, improve upon. Again, k (recall) is typically set to 0.95. This metric evaluates the work saved over random sampling, with a higher WSS@k being more efficient and is calculated via:


\begin{equation}
\text{WSS} = \frac{\text{TN} + \text{FN}}{\text{T} - (1 - \text{Recall})}
\end{equation}
% \begin{equation*}
% \lightshadowbox{\text{WSS} = \frac{\text{(TN + FN)}}{\text{T - (1 - Recall)}}
% \end{equation*}

This can also be expressed as:
\begin{equation}
\text{WSS} = \frac{\text{TN} + \text{FN}}{\text{T} - 1 + \frac{\text{TP}}{\text{TP} + \text{FN}}}
\end{equation}
% \begin{equation*}
% \lightshadowbox{\text{WSS} = \frac{\text{TN + FN}}{\text{T} - 1 + TP/(\text{TP + FN})}
% \end{equation*}
\end{document}