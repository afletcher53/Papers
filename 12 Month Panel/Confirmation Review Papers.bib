
@online{BMCMedicalResearch,
	title = {{BMC} Medical Research Methodology},
	url = {https://bmcmedresmethodol.biomedcentral.com/submission-guidelines/preparing-your-manuscript/research-article},
	abstract = {Publish your healthcare research with {BMC} Medical Research Methodology, with 3.9 Impact Factor and 18 days to first decision.



Focusing on manuscripts ...},
	titleaddon = {{BioMed} Central},
	urldate = {2024-11-13},
	langid = {english},
	file = {Snapshot:/home/aaronfletcher/snap/zotero-snap/common/Zotero/storage/YNTDE9DF/research-article.html:text/html},
}

@article{lefebvre2011cochrane,
	title = {Cochrane handbook for systematic reviews of interventions},
	journaltitle = {Oxfordshire, {UK}: The Cochrane Collaboration},
	author = {Lefebvre, Carol and Manheimer, E and Glanville, J and Higgins, JPT and Green, S and {others}},
	date = {2011},
}

@misc{yasunaga2022linkbertpretraininglanguagemodels,
	title = {{LinkBERT}: Pretraining language models with document links},
	url = {https://arxiv.org/abs/2203.15827},
	author = {Yasunaga, Michihiro and Leskovec, Jure and Liang, Percy},
	date = {2022},
	eprinttype = {arxiv},
	eprint = {2203.15827 [cs.CL]},
}

@article{akers2009systematic,
	title = {Systematic reviews: {CRD}’s guidance for undertaking reviews in health care},
	journaltitle = {University of York},
	author = {Akers, Jo and Aguiar-Ibáñez, R and Baba-Akbari, A},
	date = {2009},
}

@misc{hino_active_2022,
	title = {Active Learning by Query by Committee with Robust Divergences},
	url = {http://arxiv.org/abs/2211.10013},
	abstract = {Active learning is a widely used methodology for various problems with high measurement costs. In active learning, the next object to be measured is selected by an acquisition function, and measurements are performed sequentially. The query by committee is a well-known acquisition function. In conventional methods, committee disagreement is quantified by the Kullback–Leibler divergence. In this paper, the measure of disagreement is defined by the Bregman divergence, which includes the Kullback–Leibler divergence as an instance, and the dual \${\textbackslash}gamma\$-power divergence. As a particular class of the Bregman divergence, the \${\textbackslash}beta\$-divergence is considered. By deriving the influence function, we show that the proposed method using \${\textbackslash}beta\$-divergence and dual \${\textbackslash}gamma\$-power divergence are more robust than the conventional method in which the measure of disagreement is defined by the Kullback–Leibler divergence. Experimental results show that the proposed method performs as well as or better than the conventional method.},
	publisher = {{arXiv}},
	author = {Hino, Hideitsu and Eguchi, Shinto},
	urldate = {2024-08-16},
	date = {2022-11},
	doi = {10.48550/arXiv.2211.10013},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{cai_came_2023,
	title = {{CAME}: Competitively Learning a Mixture-of-Experts Model for First-stage Retrieval},
	url = {http://arxiv.org/abs/2311.02834},
	shorttitle = {{CAME}},
	abstract = {The first-stage retrieval aims to retrieve a subset of candidate documents from a huge collection both effectively and efficiently. Since various matching patterns can exist between queries and relevant documents, previous work tries to combine multiple retrieval models to find as many relevant results as possible. The constructed ensembles, whether learned independently or jointly, do not care which component model is more suitable to an instance during training. Thus, they cannot fully exploit the capabilities of different types of retrieval models in identifying diverse relevance patterns. Motivated by this observation, in this paper, we propose a Mixture-of-Experts ({MoE}) model consisting of representative matching experts and a novel competitive learning mechanism to let the experts develop and enhance their expertise during training. Specifically, our {MoE} model shares the bottom layers to learn common semantic representations and uses differently structured upper layers to represent various types of retrieval experts. Our competitive learning mechanism has two stages: (1) a standardized learning stage to train the experts equally to develop their capabilities to conduct relevance matching; (2) a specialized learning stage where the experts compete with each other on every training instance and get rewards and updates according to their performance to enhance their expertise on certain types of samples. Experimental results on three retrieval benchmark datasets show that our method significantly outperforms the state-of-the-art baselines.},
	publisher = {{arXiv}},
	author = {Cai, Yinqiong and Fan, Yixing and Bi, Keping and Guo, Jiafeng and Chen, Wei and Zhang, Ruqing and Cheng, Xueqi},
	urldate = {2024-08-16},
	date = {2023-11},
	keywords = {Computer Science - Information Retrieval},
}

@article{tsafnat_systematic_2014,
	title = {Systematic review automation technologies},
	volume = {3},
	issn = {2046-4053},
	url = {https://doi.org/10.1186/2046-4053-3-74},
	doi = {10.1186/2046-4053-3-74},
	abstract = {Systematic reviews, a cornerstone of evidence-based medicine, are not produced quickly enough to support clinical practice. The cost of production, availability of the requisite expertise and timeliness are often quoted as major contributors for the delay. This detailed survey of the state of the art of information systems designed to support or automate individual tasks in the systematic review, and in particular systematic reviews of randomized controlled clinical trials, reveals trends that see the convergence of several parallel research projects.},
	pages = {74},
	number = {1},
	journaltitle = {Systematic Reviews},
	author = {Tsafnat, Guy and Glasziou, Paul and Choong, Miew Keen and Dunn, Adam and Galgani, Filippo and Coiera, Enrico},
	urldate = {2024-08-12},
	date = {2014-07},
	keywords = {Information extraction, Information retrieval, Process automation, Systematic reviews},
}

@misc{behnamghader_llm2vec_2024,
	title = {{LLM}2Vec: Large Language Models Are Secretly Powerful Text Encoders},
	url = {http://arxiv.org/abs/2404.05961},
	shorttitle = {{LLM}2Vec},
	abstract = {Large decoder-only language models ({LLMs}) are the state-of-the-art models on most of today's {NLP} tasks and benchmarks. Yet, the community is only slowly adopting these models for text embedding tasks, which require rich contextualized representations. In this work, we introduce {LLM}2Vec, a simple unsupervised approach that can transform any decoder-only {LLM} into a strong text encoder. {LLM}2Vec consists of three simple steps: 1) enabling bidirectional attention, 2) masked next token prediction, and 3) unsupervised contrastive learning. We demonstrate the effectiveness of {LLM}2Vec by applying it to 3 popular {LLMs} ranging from 1.3B to 7B parameters and evaluate the transformed models on English word- and sequence-level tasks. We outperform encoder-only models by a large margin on word-level tasks and reach a new unsupervised state-of-the-art performance on the Massive Text Embeddings Benchmark ({MTEB}). Moreover, when combining {LLM}2Vec with supervised contrastive learning, we achieve state-of-the-art performance on {MTEB} among models that train only on publicly available data. Our strong empirical results and extensive analysis demonstrate that {LLMs} can be effectively transformed into universal text encoders in a parameter-efficient manner without the need for expensive adaptation or synthetic {GPT}-4 generated data.},
	publisher = {{arXiv}},
	author = {{BehnamGhader}, Parishad and Adlakha, Vaibhav and Mosbach, Marius and Bahdanau, Dzmitry and Chapados, Nicolas and Reddy, Siva},
	urldate = {2024-08-06},
	date = {2024-04},
	doi = {10.48550/arXiv.2404.05961},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{wolff_enriched_2024,
	title = {Enriched {BERT} Embeddings for Scholarly Publication Classification},
	url = {http://arxiv.org/abs/2405.04136},
	abstract = {With the rapid expansion of academic literature and the proliferation of preprints, researchers face growing challenges in manually organizing and labeling large volumes of articles. The {NSLP} 2024 {FoRC} Shared Task I addresses this challenge organized as a competition. The goal is to develop a classifier capable of predicting one of 123 predefined classes from the Open Research Knowledge Graph ({ORKG}) taxonomy of research fields for a given article.This paper presents our results. Initially, we enrich the dataset (containing English scholarly articles sourced from {ORKG} and {arXiv}), then leverage different pre-trained language Models ({PLMs}), specifically {BERT}, and explore their efficacy in transfer learning for this downstream task. Our experiments encompass feature-based and fine-tuned transfer learning approaches using diverse {PLMs}, optimized for scientific tasks, including {SciBERT}, {SciNCL}, and {SPECTER}2. We conduct hyperparameter tuning and investigate the impact of data augmentation from bibliographic databases such as {OpenAlex}, Semantic Scholar, and Crossref. Our results demonstrate that fine-tuning pre-trained models substantially enhances classification performance, with {SPECTER}2 emerging as the most accurate model. Moreover, enriching the dataset with additional metadata improves classification outcomes significantly, especially when integrating information from S2AG, {OpenAlex} and Crossref. Our best-performing approach achieves a weighted F1-score of 0.7415. Overall, our study contributes to the advancement of reliable automated systems for scholarly publication categorization, offering a potential solution to the laborious manual curation process, thereby facilitating researchers in efficiently locating relevant resources.},
	publisher = {{arXiv}},
	author = {Wolff, Benjamin and Seidlmayer, Eva and Förstner, Konrad U.},
	urldate = {2024-08-06},
	date = {2024-05},
	doi = {10.48550/arXiv.2405.04136},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{ostendorff_enriching_2019,
	title = {Enriching {BERT} with Knowledge Graph Embeddings for Document Classification},
	url = {http://arxiv.org/abs/1909.08402},
	abstract = {In this paper, we focus on the classification of books using short descriptive texts (cover blurbs) and additional metadata. Building upon {BERT}, a deep neural language model, we demonstrate how to combine text representations with metadata and knowledge graph embeddings, which encode author information. Compared to the standard {BERT} approach we achieve considerably better results for the classification task. For a more coarse-grained classification using eight labels we achieve an F1- score of 87.20, while a detailed classification using 343 labels yields an F1-score of 64.70. We make the source code and trained models of our experiments publicly available},
	publisher = {{arXiv}},
	author = {Ostendorff, Malte and Bourgonje, Peter and Berger, Maria and Moreno-Schneider, Julian and Rehm, Georg and Gipp, Bela},
	urldate = {2024-08-06},
	date = {2019-09},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Information Retrieval},
}

@misc{ostendorff_enriching_2019-1,
	title = {Enriching {BERT} with Knowledge Graph Embeddings for Document Classification},
	url = {http://arxiv.org/abs/1909.08402},
	abstract = {In this paper, we focus on the classification of books using short descriptive texts (cover blurbs) and additional metadata. Building upon {BERT}, a deep neural language model, we demonstrate how to combine text representations with metadata and knowledge graph embeddings, which encode author information. Compared to the standard {BERT} approach we achieve considerably better results for the classification task. For a more coarse-grained classification using eight labels we achieve an F1- score of 87.20, while a detailed classification using 343 labels yields an F1-score of 64.70. We make the source code and trained models of our experiments publicly available},
	publisher = {{arXiv}},
	author = {Ostendorff, Malte and Bourgonje, Peter and Berger, Maria and Moreno-Schneider, Julian and Rehm, Georg and Gipp, Bela},
	urldate = {2024-08-06},
	date = {2019-09},
	doi = {10.48550/arXiv.1909.08402},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Information Retrieval},
}

@misc{priem_openalex_2022,
	title = {{OpenAlex}: A fully-open index of scholarly works, authors, venues, institutions, and concepts},
	url = {http://arxiv.org/abs/2205.01833},
	shorttitle = {{OpenAlex}},
	abstract = {{OpenAlex} is a new, fully-open scientific knowledge graph ({SKG}), launched to replace the discontinued Microsoft Academic Graph ({MAG}). It contains metadata for 209M works (journal articles, books, etc); 2013M disambiguated authors; 124k venues (places that host works, such as journals and online repositories); 109k institutions; and 65k Wikidata concepts (linked to works via an automated hierarchical multi-tag classifier). The dataset is fully and freely available via a web-based {GUI}, a full data dump, and high-volume {REST} {API}. The resource is under active development and future work will improve accuracy and coverage of citation information and author/institution parsing and deduplication.},
	publisher = {{arXiv}},
	author = {Priem, Jason and Piwowar, Heather and Orr, Richard},
	urldate = {2024-06-14},
	date = {2022-06},
	doi = {10.48550/arXiv.2205.01833},
	keywords = {Computer Science - Digital Libraries},
}

@misc{zaheer_big_2021,
	title = {Big Bird: Transformers for Longer Sequences},
	url = {http://arxiv.org/abs/2007.14062},
	shorttitle = {Big Bird},
	abstract = {Transformers-based models, such as {BERT}, have been one of the most successful deep learning models for {NLP}. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, {BigBird}, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that {BigBird} is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having \$O(1)\$ global tokens (such as {CLS}), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, {BigBird} drastically improves performance on various {NLP} tasks such as question answering and summarization. We also propose novel applications to genomics data.},
	publisher = {{arXiv}},
	author = {Zaheer, Manzil and Guruganesh, Guru and Dubey, Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and Ahmed, Amr},
	urldate = {2024-08-06},
	date = {2021-01},
	doi = {10.48550/arXiv.2007.14062},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{beltagy_longformer_2020,
	title = {Longformer: The Long-Document Transformer},
	url = {http://arxiv.org/abs/2004.05150},
	shorttitle = {Longformer},
	abstract = {Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms {RoBERTa} on long document tasks and sets new state-of-the-art results on {WikiHop} and {TriviaQA}. We finally introduce the Longformer-Encoder-Decoder ({LED}), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the {arXiv} summarization dataset.},
	publisher = {{arXiv}},
	author = {Beltagy, Iz and Peters, Matthew E. and Cohan, Arman},
	urldate = {2024-08-06},
	date = {2020-12},
	doi = {10.48550/arXiv.2004.05150},
	keywords = {Computer Science - Computation and Language},
}

@article{li_comparative_2023,
	title = {A comparative study of pretrained language models for long clinical text},
	volume = {30},
	issn = {1527-974X},
	url = {https://doi.org/10.1093/jamia/ocac225},
	doi = {10.1093/jamia/ocac225},
	abstract = {Clinical knowledge-enriched transformer models (eg, {ClinicalBERT}) have state-of-the-art results on clinical natural language processing ({NLP}) tasks. One of the core limitations of these transformer models is the substantial memory consumption due to their full self-attention mechanism, which leads to the performance degradation in long clinical texts. To overcome this, we propose to leverage long-sequence transformer models (eg, Longformer and {BigBird}), which extend the maximum input sequence length from 512 to 4096, to enhance the ability to model long-term dependencies in long clinical texts.Inspired by the success of long-sequence transformer models and the fact that clinical notes are mostly long, we introduce 2 domain-enriched language models, Clinical-Longformer and Clinical-{BigBird}, which are pretrained on a large-scale clinical corpus. We evaluate both language models using 10 baseline tasks including named entity recognition, question answering, natural language inference, and document classification tasks.The results demonstrate that Clinical-Longformer and Clinical-{BigBird} consistently and significantly outperform {ClinicalBERT} and other short-sequence transformers in all 10 downstream tasks and achieve new state-of-the-art results.Our pretrained language models provide the bedrock for clinical {NLP} using long texts. We have made our source code available at https://github.com/luoyuanlab/Clinical-Longformer, and the pretrained models available for public download at: https://huggingface.co/yikuan8/Clinical-Longformer.This study demonstrates that clinical knowledge-enriched long-sequence transformers are able to learn long-term dependencies in long clinical text. Our methods can also inspire the development of other domain-enriched long-sequence transformers.},
	pages = {340--347},
	number = {2},
	journaltitle = {Journal of the American Medical Informatics Association},
	author = {Li, Yikuan and Wehbe, Ramsey M and Ahmad, Faraz S and Wang, Hanyin and Luo, Yuan},
	urldate = {2024-08-06},
	date = {2023-02},
}

@misc{fletcher_afletcher53bert-clef-diff_2024,
	title = {afletcher53/bert-clef-diff},
	url = {https://github.com/afletcher53/bert-clef-diff},
	author = {Fletcher, Aaron},
	urldate = {2024-08-04},
	date = {2024-08},
}

@misc{xu_forget_2020,
	title = {Forget Me Not: Reducing Catastrophic Forgetting for Domain Adaptation in Reading Comprehension},
	url = {http://arxiv.org/abs/1911.00202},
	shorttitle = {Forget Me Not},
	abstract = {The creation of large-scale open domain reading comprehension data sets in recent years has enabled the development of end-to-end neural comprehension models with promising results. To use these models for domains with limited training data, one of the most effective approach is to first pretrain them on large out-of-domain source data and then fine-tune them with the limited target data. The caveat of this is that after fine-tuning the comprehension models tend to perform poorly in the source domain, a phenomenon known as catastrophic forgetting. In this paper, we explore methods that overcome catastrophic forgetting during fine-tuning without assuming access to data from the source domain. We introduce new auxiliary penalty terms and observe the best performance when a combination of auxiliary penalty terms is used to regularise the fine-tuning process for adapting comprehension models. To test our methods, we develop and release 6 narrow domain data sets that could potentially be used as reading comprehension benchmarks.},
	publisher = {{arXiv}},
	author = {Xu, Y. and Zhong, X. and Yepes, A. J. J. and Lau, J. H.},
	urldate = {2024-08-04},
	date = {2020-11},
	doi = {10.48550/arXiv.1911.00202},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{vaswani_attention_2023,
	title = {Attention Is All You Need},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 {BLEU} on the {WMT} 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 {BLEU}. On the {WMT} 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art {BLEU} score of 41.8 after training for 3.5 days on eight {GPUs}, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	publisher = {{arXiv}},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	urldate = {2024-08-04},
	date = {2023-08},
	doi = {10.48550/arXiv.1706.03762},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@patent{cormack_systems_2016,
	title = {Systems and methods for conducting a highly autonomous technology-assisted review classification},
	url = {https://patents.google.com/patent/US20160371261A1/en},
	holder = {Individual},
	abstract = {Systems and methods for classifying electronic information are provided by way of a Technology-Assisted Review (“{TAR}”) process, specifically an “Auto-{TAR}” process that limits discretionary choices in an information classification effort, while still achieving superior results. In certain embodiments, Auto-{TAR} selects an initial relevant document from a document collection, selects a number of other documents from the document collection and assigns them a default classification, trains a classifier using a training set made up of the selected relevant document and the documents assigned a default classification, scores documents in the document collection and determines if a stopping criteria is met. If a stopping criteria has not been met, the process sorts the documents according to scores, selects a batch of documents from the collection for further review, receives user coding decisions for them, and re-trains a classifier using the received user coding decisions and an adjusted training set.},
	type = {patentus},
	number = {20160371261A1},
	author = {Cormack, Gordon V. and Grossman, Maura R.},
	urldate = {2024-08-04},
	date = {2016-12},
	langid = {english},
	keywords = {document, documents, relevant, review, stopping criteria},
}

@article{ferdinands_performance_2023,
	title = {Performance of active learning models for screening prioritization in systematic reviews: a simulation study into the Average Time to Discover relevant records},
	volume = {12},
	doi = {10.1186/s13643-023-02257-7},
	shorttitle = {Performance of active learning models for screening prioritization in systematic reviews},
	abstract = {Background Conducting a systematic review demands a significant amount of effort in screening titles and abstracts. To accelerate this process, various tools that utilize active learning have been proposed. These tools allow the reviewer to interact with machine learning software to identify relevant publications as early as possible. The goal of this study is to gain a comprehensive understanding of active learning models for reducing the workload in systematic reviews through a simulation study. Methods The simulation study mimics the process of a human reviewer screening records while interacting with an active learning model. Different active learning models were compared based on four classification techniques (naive Bayes, logistic regression, support vector machines, and random forest) and two feature extraction strategies ({TF}-{IDF} and doc2vec). The performance of the models was compared for six systematic review datasets from different research areas. The evaluation of the models was based on the Work Saved over Sampling ({WSS}) and recall. Additionally, this study introduces two new statistics, Time to Discovery ({TD}) and Average Time to Discovery ({ATD}). Results The models reduce the number of publications needed to screen by 91.7 to 63.9\% while still finding 95\% of all relevant records ({WSS}@95). Recall of the models was defined as the proportion of relevant records found after screening 10\% of of all records and ranges from 53.6 to 99.8\%. The {ATD} values range from 1.4\% till 11.7\%, which indicate the average proportion of labeling decisions the researcher needs to make to detect a relevant record. The {ATD} values display a similar ranking across the simulations as the recall and {WSS} values. Conclusions Active learning models for screening prioritization demonstrate significant potential for reducing the workload in systematic reviews. The Naive Bayes + {TF}-{IDF} model yielded the best results overall. The Average Time to Discovery ({ATD}) measures performance of active learning models throughout the entire screening process without the need for an arbitrary cut-off point. This makes the {ATD} a promising metric for comparing the performance of different models across different datasets.},
	journaltitle = {Systematic Reviews},
	author = {Ferdinands, Gerbrich and Schram, Raoul and de Bruin, Jonathan and Bagheri, Ayoub and Oberski, Daniel and Tummers, Lars and Teijema, Jelle and Schoot, Rens},
	date = {2023-06},
}

@book{manning_introduction_2008,
	edition = {1},
	title = {Introduction to Information Retrieval},
	isbn = {978-0-521-86571-5 978-0-511-80907-1},
	url = {https://www.cambridge.org/core/product/identifier/9780511809071/type/book},
	abstract = {Class-tested and coherent, this textbook teaches classical and web information retrieval, including web search and the related areas of text classification and text clustering from basic concepts. It gives an up-to-date treatment of all aspects of the design and implementation of systems for gathering, indexing, and searching documents; methods for evaluating systems; and an introduction to the use of machine learning methods on text collections. All the important ideas are explained using examples and figures, making it perfect for introductory courses in information retrieval for advanced undergraduates and graduate students in computer science. Based on feedback from extensive classroom experience, the book has been carefully structured in order to make teaching more natural and effective. Slides and additional exercises (with solutions for lecturers) are also available through the book's supporting website to help course instructors prepare their lectures.},
	publisher = {Cambridge University Press},
	author = {Manning, Christopher D. and Raghavan, Prabhakar and Schütze, Hinrich},
	urldate = {2024-08-03},
	date = {2008-07},
	doi = {10.1017/CBO9780511809071},
}

@article{omara-eves_using_2015,
	title = {Using text mining for study identification in systematic reviews: a systematic review of current approaches},
	volume = {4},
	issn = {2046-4053},
	url = {https://doi.org/10.1186/2046-4053-4-5},
	doi = {10.1186/2046-4053-4-5},
	shorttitle = {Using text mining for study identification in systematic reviews},
	abstract = {The large and growing number of published studies, and their increasing rate of publication, makes the task of identifying relevant studies in an unbiased way for inclusion in systematic reviews both complex and time consuming. Text mining has been offered as a potential solution: through automating some of the screening process, reviewer time can be saved. The evidence base around the use of text mining for screening has not yet been pulled together systematically; this systematic review fills that research gap. Focusing mainly on non-technical issues, the review aims to increase awareness of the potential of these technologies and promote further collaborative research between the computer science and systematic review communities.},
	pages = {5},
	number = {1},
	journaltitle = {Systematic Reviews},
	author = {O’Mara-Eves, Alison and Thomas, James and {McNaught}, John and Miwa, Makoto and Ananiadou, Sophia},
	urldate = {2024-08-02},
	date = {2015-01},
	keywords = {Automation, Review efficiency, Screening, Study selection, Text mining},
}

@article{cohen_reducing_2006,
	title = {Reducing Workload in Systematic Review Preparation Using Automated Citation Classification},
	volume = {13},
	issn = {1067-5027},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1447545/},
	doi = {10.1197/jamia.M1929},
	abstract = {Objective: To determine whether automated classification of document citations can be useful in reducing the time spent by experts reviewing journal articles for inclusion in updating systematic reviews of drug class efficacy for treatment of disease., Design: A test collection was built using the annotated reference files from 15 systematic drug class reviews. A voting perceptron-based automated citation classification system was constructed to classify each article as containing high-quality, drug class–specific evidence or not. Cross-validation experiments were performed to evaluate performance., Measurements: Precision, recall, and F-measure were evaluated at a range of sample weightings. Work saved over sampling at 95\% recall was used as the measure of value to the review process., Results: A reduction in the number of articles needing manual review was found for 11 of the 15 drug review topics studied. For three of the topics, the reduction was 50\% or greater., Conclusion: Automated document citation classification could be a useful tool in maintaining systematic reviews of the efficacy of drug therapy. Further work is needed to refine the classification system and determine the best manner to integrate the system into the production of systematic reviews.},
	pages = {206--219},
	number = {2},
	journaltitle = {Journal of the American Medical Informatics Association : {JAMIA}},
	author = {Cohen, A.M. and Hersh, W.R. and Peterson, K. and Yen, Po-Yin},
	urldate = {2024-08-01},
	date = {2006},
	pmid = {16357352},
	note = {tex.pmcid: {PMC}1447545},
}

@article{lewis_rcv1_2004,
	title = {{RCV}1: A New Benchmark Collection for Text Categorization Research},
	volume = {5},
	issn = {{ISSN} 1533-7928},
	url = {https://www.jmlr.org/papers/v5/lewis04a.html},
	shorttitle = {{RCV}1},
	abstract = {Reuters Corpus Volume I ({RCV}1) is an archive of over 800,000 manually categorized newswire stories recently made available by Reuters, Ltd. for research purposes. Use of this data for research on text categorization requires a detailed understanding of the real world constraints under which the data was produced. Drawing on interviews with Reuters personnel and access to Reuters documentation, we describe the coding policy and quality control procedures used in producing the {RCV}1 data, the intended semantics of the hierarchical category taxonomies, and the corrections necessary to remove errorful data. We refer to the original data as {RCV}1-v1, and the corrected data as {RCV}1-v2. We benchmark several widely used supervised learning methods on {RCV}1-v2, illustrating the collection's properties, suggesting new directions for research, and providing baseline results for future studies. We make available detailed, per-category experimental results, as well as corrected versions of the category assignments and taxonomy structures, via online appendices.},
	pages = {361--397},
	issue = {Apr},
	journaltitle = {Journal of Machine Learning Research},
	author = {Lewis, David D. and Yang, Yiming and Rose, Tony G. and Li, Fan},
	urldate = {2024-07-31},
	date = {2004},
}

@misc{noauthor_papers_nodate,
	title = {Papers with Code - {RCV}1 Dataset},
	url = {https://paperswithcode.com/dataset/rcv1},
	abstract = {The {RCV}1 dataset is a benchmark dataset on text categorization. It is a collection of newswire articles producd by Reuters in 1996-1997. It contains 804,414 manually labeled newswire documents, and categorized with respect to three controlled vocabularies: industries, topics and regions.},
	urldate = {2024-07-31},
	langid = {english},
}

@inproceedings{roegiest_trec_2015,
	title = {{TREC} 2015 Total Recall Track Overview},
	volume = {500-319},
	url = {https://trec.nist.gov/pubs/trec24/papers/Overview-TR.pdf},
	series = {{NIST} Special Publication},
	booktitle = {Proceedings of The Twenty-Fourth Text {REtrieval} Conference, {TREC} 2015, Gaithersburg, Maryland, {USA}, November 17-20, 2015},
	publisher = {National Institute of Standards and Technology ({NIST})},
	author = {Roegiest, Adam and Cormack, Gordon V. and Clarke, Charles L. A. and Grossman, Maura R.},
	editor = {Voorhees, Ellen M. and Ellis, Angela},
	urldate = {2024-07-31},
	date = {2015},
}

@inproceedings{grossman_trec_2016,
	title = {{TREC} 2016 Total Recall Track Overview},
	url = {https://www.semanticscholar.org/paper/TREC-2016-Total-Recall-Track-Overview-Grossman-Cormack/126240dedd75626fd736f0485d06f1f516517e54},
	abstract = {The primary purpose of the Total Recall Track is to evaluate, through controlled simulation, methods designed to achieve very high recall – as close as practicable to 100\% – with a human assessor in the loop. Motivating applications include, among others, electronic discovery in legal proceedings [3], systematic review in evidencebased medicine [6], and the creation of fully labeled test collections for information retrieval (“{IR}”) evaluation [5]. A secondary, but no less important, purpose is to develop a sandboxed virtual test environment within which {IR} systems may be tested, while preventing the disclosure of sensitive test data to participants. At the same time, the test environment also operates as a “black box,” affording participants confidence that their proprietary systems cannot easily be reverse engineered. The task to be solved in the Total Recall Track is the following:},
	author = {Grossman, Maura R. and Cormack, G. and Roegiest, Adam},
	urldate = {2024-07-31},
	date = {2016},
}

@misc{de_bruin_synergy_2023,
	title = {{SYNERGY} - Open machine learning dataset on study selection in systematic reviews},
	url = {https://dataverse.nl/citation?persistentId=doi:10.34894/HE6NAQ},
	abstract = {{SYNERGY} is a free and open dataset on study selection in systematic reviews, comprising 169,288 academic works from 26 systematic reviews. Only 2,834 (1.67\%) of the academic works in the binary classified dataset are included in the systematic reviews. This makes the {SYNERGY} dataset a unique dataset for the development of information retrieval algorithms, especially for sparse labels. Due to the many available variables available per record (i.e. titles, abstracts, authors, references, topics), this dataset is useful for researchers in {NLP}, machine learning, network analysis, and more. In total, the dataset contains 82,668,134 trainable data points. The easiest way to get the {SYNERGY} dataset is via the synergy-dataset Python package. See https://github.com/asreview/synergy-dataset for all information.},
	publisher = {{DataverseNL}},
	author = {De Bruin, Jonathan and Ma, Yongchao and Ferdinands, Gerbrich and Teijema, Jelle and Van De Schoot, Rens},
	editora = {De Bruin, Jonathan and Van De Schoot, Rens},
	editoratype = {collaborator},
	urldate = {2024-07-31},
	date = {2023},
	doi = {10.34894/HE6NAQ},
}

@article{grossman_technology-assisted_2010,
	title = {Technology-Assisted Review in E-Discovery Can Be More Effective and More Efficient than Exhaustive Manual Review Annual Survey},
	volume = {17},
	url = {https://heinonline.org/HOL/P?h=hein.journals/jolt17&i=471},
	pages = {[i]--48},
	number = {3},
	journaltitle = {Richmond Journal of Law and Technology},
	author = {Grossman, Maura R. and Cormack, Gordon V.},
	urldate = {2024-07-31},
	date = {2010},
}

@misc{noauthor_tar2017-tarclef_nodate,
	title = {tar/2017-{TAR}/{CLEF} 2017 Technologically Assisted Reviews in Empirical Medicine Overview.pdf at master · {CLEF}-{TAR}/tar · {GitHub}},
	url = {https://github.com/CLEF-TAR/tar},
	urldate = {2024-07-31},
}

@article{kanoulas_clef_2019,
	title = {{CLEF} 2019 technology assisted reviews in empirical medicine overview},
	volume = {2380},
	issn = {1613-0073},
	url = {https://strathprints.strath.ac.uk/71253/},
	abstract = {Systematic reviews are a widely used method to provide an overview over the current scientific consensus, by bringing together multiple studies in a systematic, reliable, and transparent way. The large and growing number of published studies, and their increasing rate of publication, makes the task of identifying all relevant studies in an unbiased way both complex and time consuming to the extent that jeopardizes the validity of their findings and the ability to inform policy and practice in a timely manner. The {CLEF} 2019 e-Health {TAR} Lab accommodated two tasks. Task 1 focused on retrieving relevant studies from {PubMed} without the use of a Boolean query, while Task 2 focused on the efficient and effective ranking of studies during the abstract and title screening phase of conducting a systematic review. In the 2019 lab we also expanded upon the type of systematics reviews considered. Hence, beyond Diagnostic Test Accuracy reviews, we also included Intervention, Prognosis, and Qualitative systematic reviews. We constructed a benchmark collection of 31 reviews published by Cochrane, and the corresponding relevant and irrelevant articles found by the original Boolean query. Three teams participated in Task 2, submitting automatic and semi-automatic runs, using information retrieval and machine learning algorithms over a variety of text representations, in a batch and iterative manner. This paper reports both the methodology used to construct the benchmark collection, and the results of the evaluation.},
	journaltitle = {{CEUR} Workshop Proceedings},
	author = {Kanoulas, Evangelos and Li, Dan and Azzopardi, Leif and Spijker, Rene},
	urldate = {2024-07-31},
	date = {2019-09},
	langid = {english},
	note = {tex.copyright: cc\_by},
}

@article{kanoulas_clef_2018,
	title = {{CLEF} 2018 technologically assisted reviews in empirical medicine overview: 19th Working Notes of {CLEF} Conference and Labs of the Evaluation Forum, {CLEF} 2018},
	volume = {2125},
	issn = {1613-0073},
	url = {http://www.scopus.com/inward/record.url?scp=85051077484&partnerID=8YFLogxK},
	shorttitle = {{CLEF} 2018 technologically assisted reviews in empirical medicine overview},
	abstract = {Conducting a systematic review is a widely used method to obtain an overview over the current scientific consensus on a topic of interest, by bringing together multiple studies in a reliable, transparent way. The large and growing number of published studies, and their increasing rate of publication, makes the task of identifying all relevant studies in an unbiased way both complex and time consuming to the extent that jeopardizes the validity of their findings and the ability to inform policy and practice in a timely manner. The {CLEF} 2018 e-Health Technology Assisted Reviews in Empirical Medicine task aims at evaluating search algorithms that seek to identify all studies relevant for conducting a systematic review in empirical medicine. The task had a focus on Diagnostic Test Accuracy ({DTA}) reviews, and consisted of two subtasks: 1) given a number of relevance criteria as described in a systematic review protocol, search a large medical database of article abstracts ({PubMed}) to find the studies to be included in the review, and 2) given the article abstracts retrieved by a carefully designed Boolean Query, prioritize them to reduce the effort required by experts to screen the abstracts for inclusion in the review. Seven teams participated in the task, with a total of 12 runs submitted for subtask 1 and 19 runs for subtask 2. This paper reports both the methodology used to construct the benchmark collection, and the results of the evaluation.},
	journaltitle = {{CEUR} Workshop Proceedings},
	author = {Kanoulas, Evangelos and Li, Dan and Azzopardi, Leif and Spijker, Rene},
	urldate = {2024-07-31},
	date = {2018-07},
	keywords = {{TAR}, {PubMed}, relevance feedback, active learning, evaluation, systematic reviews, text classification, benchmarking, Cochrane, diagnostic test accuracy, {DTA}, e-health, high recall, information retrieval, technology assisted reviews, test collection},
}

@article{kanoulas_clef_2017,
	title = {{CLEF} 2017 technologically assisted reviews in empirical medicine overview},
	volume = {1866},
	issn = {1613-0073},
	url = {http://ceur-ws.org/Vol-1866/},
	abstract = {Systematic reviews are a widely used method to provide an overview over the current scientific consensus, by bringing together multiple studies in a reliable, transparent way. The large and growing number of published studies, and their increasing rate of publication, makes the task of identifying all relevant studies in an unbiased way both complex and time consuming to the extent that jeopardizes the validity of their findings and the ability to inform policy and practice in a timely manner. The {CLEF} 2017 e-Health Lab Task 2 focuses on the efficient and effective ranking of studies during the abstract and title screening phase of conducting Diagnostic Test Accuracy systematic reviews. We constructed a benchmark collection of fifty such reviews and the corresponding relevant and irrelevant articles found by the original Boolean query. Fourteen teams participated in the task, submitting 68 automatic and semi-automatic runs, using information retrieval and machine learning algorithms over a variety of text representations, in a batch and iterative manner. This paper reports both the methodology used to construct the benchmark collection, and the results of the evaluation.},
	pages = {1--29},
	journaltitle = {{CEUR} Workshop Proceedings},
	author = {Kanoulas, Evangelos and Li, Dan and Azzopardi, Leif and Spijker, Rene},
	urldate = {2024-07-31},
	date = {2017-09},
	keywords = {Active learning, {TAR}, Information retrieval, Systematic reviews, Evaluation, Text classification},
}

@book{goeuriot_clef_2017,
	title = {{CLEF} 2017 {eHealth} Evaluation Lab Overview},
	isbn = {978-3-319-65812-4},
	abstract = {In this paper we provide an overview of the fifth edition of the {CLEF} {eHealth} evaluation lab. {CLEF} {eHealth} 2017 continues our evaluation resource building efforts around the easing and support of patients, their next-of-kins, clinical staff, and health scientists in understanding, accessing, and authoring {eHealth} information in a multilingual setting. This year’s lab offered three tasks: Task 1 on multilingual information extraction to extend from last year’s task on French corpora, Task 2 on technologically assisted reviews in empirical medicine as a new pilot task, and Task 3 on patient-centered information retrieval ({IR}) building on the 2013-16 {IR} tasks. In total 32 teams took part in these tasks (11 in Task 1, 14 in Task 2, and 7 in Task 3). We also continued the replication track from 2016. Herein, we describe the resources created for these tasks, evaluation methodology adopted and provide a brief summary of participants of this year’s challenges and results obtained. As in previous years, the organizers have made data and tools associated with the lab tasks available for future research and development.},
	author = {Goeuriot, Lorraine and Kelly, Liadh and Suominen, Hanna and Névéol, Aurélie and Robert, Aude and Kanoulas, Evangelos and Spijker, René and Palotti, João and Zuccon, Guido},
	date = {2017-08},
	doi = {10.1007/978-3-319-65813-1_26},
}

@incollection{goharian_reproducibility_2024,
	location = {Cham},
	title = {A Reproducibility Study of Goldilocks: Just-Right Tuning of {BERT} for {TAR}},
	volume = {14611},
	isbn = {978-3-031-56065-1 978-3-031-56066-8},
	url = {https://link.springer.com/10.1007/978-3-031-56066-8_13},
	shorttitle = {A Reproducibility Study of Goldilocks},
	pages = {132--146},
	booktitle = {Advances in Information Retrieval},
	publisher = {Springer Nature Switzerland},
	author = {Mao, Xinyu and Koopman, Bevan and Zuccon, Guido},
	editor = {Goharian, Nazli and Tonellotto, Nicola and He, Yulan and Lipani, Aldo and {McDonald}, Graham and Macdonald, Craig and Ounis, Iadh},
	urldate = {2024-07-31},
	date = {2024},
	langid = {english},
	doi = {10.1007/978-3-031-56066-8_13},
}

@misc{noauthor_ielabgoldilocks-reproduce_2024,
	title = {ielab/goldilocks-reproduce},
	url = {https://github.com/ielab/goldilocks-reproduce},
	publisher = {ielab},
	urldate = {2024-07-31},
	date = {2024-06},
}

@misc{noauthor_all_cleanjsonlzip_nodate,
	title = {all\_clean.jsonl.zip},
	url = {https://drive.google.com/file/u/1/d/1kppExc6Wo81sCPYI2hkSsxO-ekgD2Qcc/view?usp=drive_link&usp=embed_facebook},
	urldate = {2024-07-31},
}

@misc{noauthor_github_nodate,
	title = {{GitHub} - {CLEF}-{TAR}/tar: Technologically Assisted Reviews in Empirical Medicine},
	url = {https://github.com/CLEF-TAR/tar/tree/master},
	urldate = {2024-07-31},
}

@report{settles_active_2009,
	title = {Active Learning Literature Survey},
	url = {https://minds.wisconsin.edu/handle/1793/60660},
	abstract = {The key idea behind active learning is that a machine learning algorithm can achieve greater accuracy with fewer labeled training instances if it is allowed to choose the training data from which is learns. An active learner may ask queries in the form of unlabeled instances to be labeled by an oracle (e.g., a human annotator). Active learning is well-motivated in many modern machine learning problems, where unlabeled data may be abundant but labels are difficult, time-consuming, or expensive to obtain. This report provides a general introduction to active learning and a survey of the literature. This includes a discussion of the scenarios in which queries can be formulated, and an overview of the query strategy frameworks proposed in the literature to date. An analysis of the empirical and theoretical evidence for active learning, a summary of several problem setting variants, and a discussion of related topics in machine learning research are also presented.},
	institution = {University of Wisconsin-Madison Department of Computer Sciences},
	type = {Technical Report},
	author = {Settles, Burr},
	urldate = {2024-07-31},
	date = {2009},
	langid = {english},
}

@inproceedings{cormack_evaluation_2014,
	location = {Gold Coast Queensland Australia},
	title = {Evaluation of machine-learning protocols for technology-assisted review in electronic discovery},
	isbn = {978-1-4503-2257-7},
	url = {https://dl.acm.org/doi/10.1145/2600428.2609601},
	doi = {10.1145/2600428.2609601},
	pages = {153--162},
	booktitle = {Proceedings of the 37th international {ACM} {SIGIR} conference on Research \& development in information retrieval},
	publisher = {{ACM}},
	author = {Cormack, Gordon V. and Grossman, Maura R.},
	urldate = {2024-07-31},
	date = {2014-07},
	langid = {english},
}

@misc{ren_survey_2021,
	title = {A Survey of Deep Active Learning},
	url = {http://arxiv.org/abs/2009.00236},
	abstract = {Active learning ({AL}) attempts to maximize the performance gain of the model by marking the fewest samples. Deep learning ({DL}) is greedy for data and requires a large amount of data supply to optimize massive parameters, so that the model learns how to extract high-quality features. In recent years, due to the rapid development of internet technology, we are in an era of information torrents and we have massive amounts of data. In this way, {DL} has aroused strong interest of researchers and has been rapidly developed. Compared with {DL}, researchers have relatively low interest in {AL}. This is mainly because before the rise of {DL}, traditional machine learning requires relatively few labeled samples. Therefore, early {AL} is difficult to reflect the value it deserves. Although {DL} has made breakthroughs in various fields, most of this success is due to the publicity of the large number of existing annotation datasets. However, the acquisition of a large number of high-quality annotated datasets consumes a lot of manpower, which is not allowed in some fields that require high expertise, especially in the fields of speech recognition, information extraction, medical images, etc. Therefore, {AL} has gradually received due attention. A natural idea is whether {AL} can be used to reduce the cost of sample annotations, while retaining the powerful learning capabilities of {DL}. Therefore, deep active learning ({DAL}) has emerged. Although the related research has been quite abundant, it lacks a comprehensive survey of {DAL}. This article is to fill this gap, we provide a formal classification method for the existing work, and a comprehensive and systematic overview. In addition, we also analyzed and summarized the development of {DAL} from the perspective of application. Finally, we discussed the confusion and problems in {DAL}, and gave some possible development directions for {DAL}.},
	publisher = {{arXiv}},
	author = {Ren, Pengzhen and Xiao, Yun and Chang, Xiaojun and Huang, Po-Yao and Li, Zhihui and Gupta, Brij B. and Chen, Xiaojiang and Wang, Xin},
	urldate = {2024-07-31},
	date = {2021-12},
	doi = {10.48550/arXiv.2009.00236},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{pearce_understanding_2021,
	title = {Understanding Softmax Confidence and Uncertainty},
	url = {http://arxiv.org/abs/2106.04972},
	abstract = {It is often remarked that neural networks fail to increase their uncertainty when predicting on data far from the training distribution. Yet naively using softmax confidence as a proxy for uncertainty achieves modest success in tasks exclusively testing for this, e.g., out-of-distribution ({OOD}) detection. This paper investigates this contradiction, identifying two implicit biases that do encourage softmax confidence to correlate with epistemic uncertainty: 1) Approximately optimal decision boundary structure, and 2) Filtering effects of deep networks. It describes why low-dimensional intuitions about softmax confidence are misleading. Diagnostic experiments quantify reasons softmax confidence can fail, finding that extrapolations are less to blame than overlap between training and {OOD} data in final-layer representations. Pre-trained/fine-tuned networks reduce this overlap.},
	publisher = {{arXiv}},
	author = {Pearce, Tim and Brintrup, Alexandra and Zhu, Jun},
	urldate = {2024-07-31},
	date = {2021-06},
	doi = {10.48550/arXiv.2106.04972},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{guo_calibration_2017,
	title = {On Calibration of Modern Neural Networks},
	url = {http://arxiv.org/abs/1706.04599},
	abstract = {Confidence calibration – the problem of predicting probability estimates representative of the true correctness likelihood – is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling – a single-parameter variant of Platt Scaling – is surprisingly effective at calibrating predictions.},
	publisher = {{arXiv}},
	author = {Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q.},
	urldate = {2024-07-31},
	date = {2017-08},
	doi = {10.48550/arXiv.1706.04599},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{wang_new_2014,
	title = {A new active labeling method for deep learning},
	url = {https://ieeexplore.ieee.org/document/6889457/?arnumber=6889457},
	doi = {10.1109/IJCNN.2014.6889457},
	abstract = {Deep learning has been shown to achieve outstanding performance in a number of challenging real-world applications. However, most of the existing works assume a fixed set of labeled data, which is not necessarily true in real-world applications. Getting labeled data is usually expensive and time consuming. Active labelling in deep learning aims at achieving the best learning result with a limited labeled data set, i.e., choosing the most appropriate unlabeled data to get labeled. This paper presents a new active labeling method, {AL}-{DL}, for cost-effective selection of data to be labeled. {AL}-{DL} uses one of three metrics for data selection: least confidence, margin sampling, and entropy. The method is applied to deep learning networks based on stacked restricted Boltzmann machines, as well as stacked autoencoders. In experiments on the {MNIST} benchmark dataset, the method outperforms random labeling consistently by a significant margin.},
	pages = {112--119},
	booktitle = {2014 International Joint Conference on Neural Networks ({IJCNN})},
	author = {Wang, Dan and Shang, Yi},
	urldate = {2024-07-31},
	date = {2014-07},
	keywords = {Classification algorithms, Entropy, Labeling, Measurement, Neural networks, Training, Uncertainty},
}

@inproceedings{lewis_sequential_1994,
	location = {London},
	title = {A Sequential Algorithm for Training Text Classifiers},
	isbn = {978-1-4471-2099-5},
	doi = {10.1007/978-1-4471-2099-5_1},
	abstract = {The ability to cheaply train text classifiers is critical to their use in information retrieval, content analysis, natural language processing, and other tasks involving data which is partly or fully textual. An algorithm for sequential sampling during machine learning of statistical classifiers was developed and tested on a newswire text categorization task. This method, which we call uncertainty sampling, reduced by as much as 500-fold the amount of training data that would have to be manually classified to achieve a given level of effectiveness.},
	pages = {3--12},
	booktitle = {{SIGIR} ’94},
	publisher = {Springer},
	author = {Lewis, David D. and Gale, William A.},
	editor = {Croft, Bruce W. and van Rijsbergen, C. J.},
	date = {1994},
	langid = {english},
}

@misc{noauthor_sequential_nodate,
	title = {A Sequential Algorithm for Training Text Classifiers {\textbar} {SpringerLink}},
	url = {https://link.springer.com/chapter/10.1007/978-1-4471-2099-5_1},
	urldate = {2024-07-31},
}

@article{argamon-engelson_committee-based_1999,
	title = {Committee-Based Sample Selection for Probabilistic Classifiers},
	volume = {11},
	issn = {1076-9757},
	url = {http://arxiv.org/abs/1106.0220},
	doi = {10.1613/jair.612},
	abstract = {In many real-world learning tasks, it is expensive to acquire a sufficient number of labeled examples for training. This paper investigates methods for reducing annotation cost by `sample selection'. In this approach, during training the learning program examines many unlabeled examples and selects for labeling only those that are most informative at each stage. This avoids redundantly labeling examples that contribute little new information. Our work follows on previous research on Query By Committee, extending the committee-based paradigm to the context of probabilistic classification. We describe a family of empirical methods for committee-based sample selection in probabilistic classification models, which evaluate the informativeness of an example by measuring the degree of disagreement between several model variants. These variants (the committee) are drawn randomly from a probability distribution conditioned by the training set labeled so far. The method was applied to the real-world natural language processing task of stochastic part-of-speech tagging. We find that all variants of the method achieve a significant reduction in annotation cost, although their computational efficiency differs. In particular, the simplest variant, a two member committee with no parameters to tune, gives excellent results. We also show that sample selection yields a significant reduction in the size of the model used by the tagger.},
	pages = {335--360},
	journaltitle = {Journal of Artificial Intelligence Research},
	author = {Argamon-Engelson, S. and Dagan, I.},
	urldate = {2024-07-31},
	date = {1999-11},
	keywords = {Computer Science - Artificial Intelligence},
}

@article{angluin_queries_1988,
	title = {Queries and Concept Learning},
	volume = {2},
	issn = {1573-0565},
	url = {https://doi.org/10.1023/A:1022821128753},
	doi = {10.1023/A:1022821128753},
	abstract = {We consider the problem of using queries to learn an unknown concept. Several types of queries are described and studied: membership, equivalence, subset, superset, disjointness, and exhaustiveness queries. Examples are given of efficient learning methods using various subsets of these queries for formal domains, including the regular languages, restricted classes of context-free languages, the pattern languages, and restricted types of prepositional formulas. Some general lower bound techniques are given. Equivalence queries are compared with Valiant's criterion of probably approximately correct identification under random sampling.},
	pages = {319--342},
	number = {4},
	journaltitle = {Machine Learning},
	author = {Angluin, Dana},
	urldate = {2024-07-31},
	date = {1988-04},
	langid = {english},
	keywords = {Concept learning, queries, supervised learning},
}

@article{artstein_survey_2008,
	title = {Survey Article: Inter-Coder Agreement for Computational Linguistics},
	volume = {34},
	url = {https://aclanthology.org/J08-4004},
	doi = {10.1162/coli.07-034-R2},
	shorttitle = {Survey Article},
	pages = {555--596},
	number = {4},
	journaltitle = {Computational Linguistics},
	author = {Artstein, Ron and Poesio, Massimo},
	urldate = {2024-07-31},
	date = {2008},
}

@inproceedings{hoi_batch_2006,
	location = {New York, {NY}, {USA}},
	title = {Batch mode active learning and its application to medical image classification},
	isbn = {978-1-59593-383-6},
	url = {https://doi.org/10.1145/1143844.1143897},
	doi = {10.1145/1143844.1143897},
	series = {{ICML} '06},
	abstract = {The goal of active learning is to select the most informative examples for manual labeling. Most of the previous studies in active learning have focused on selecting a single unlabeled example in each iteration. This could be inefficient since the classification model has to be retrained for every labeled example. In this paper, we present a framework for "batch mode active learning" that applies the Fisher information matrix to select a number of informative examples simultaneously. The key computational challenge is how to efficiently identify the subset of unlabeled examples that can result in the largest reduction in the Fisher information. To resolve this challenge, we propose an efficient greedy algorithm that is based on the property of submodular functions. Our empirical studies with five {UCI} datasets and one real-world medical image classification show that the proposed batch mode active learning algorithm is more effective than the state-of-the-art algorithms for active learning.},
	pages = {417--424},
	booktitle = {Proceedings of the 23rd international conference on Machine learning},
	publisher = {Association for Computing Machinery},
	author = {Hoi, Steven C. H. and Jin, Rong and Zhu, Jianke and Lyu, Michael R.},
	urldate = {2024-07-30},
	date = {2006-06},
}

@article{smith_less_2018,
	title = {Less is more: Sampling chemical space with active learning},
	volume = {148},
	issn = {0021-9606},
	url = {https://doi.org/10.1063/1.5023802},
	doi = {10.1063/1.5023802},
	shorttitle = {Less is more},
	abstract = {The development of accurate and transferable machine learning ({ML}) potentials for predicting molecular energetics is a challenging task. The process of data generation to train such {ML} potentials is a task neither well understood nor researched in detail. In this work, we present a fully automated approach for the generation of datasets with the intent of training universal {ML} potentials. It is based on the concept of active learning ({AL}) via Query by Committee ({QBC}), which uses the disagreement between an ensemble of {ML} potentials to infer the reliability of the ensemble’s prediction. {QBC} allows the presented {AL} algorithm to automatically sample regions of chemical space where the {ML} potential fails to accurately predict the potential energy. {AL} improves the overall fitness of {ANAKIN}-{ME} ({ANI}) deep learning potentials in rigorous test cases by mitigating human biases in deciding what new training data to use. {AL} also reduces the training set size to a fraction of the data required when using naive random sampling techniques. To provide validation of our {AL} approach, we develop the {COmprehensive} Machine-learning Potential ({COMP}6) benchmark (publicly available on {GitHub}) which contains a diverse set of organic molecules. Active learning-based {ANI} potentials outperform the original random sampled {ANI}-1 potential with only 10\% of the data, while the final active learning-based model vastly outperforms {ANI}-1 on the {COMP}6 benchmark after training to only 25\% of the data. Finally, we show that our proposed {AL} technique develops a universal {ANI} potential ({ANI}-1x) that provides accurate energy and force predictions on the entire {COMP}6 benchmark. This universal {ML} potential achieves a level of accuracy on par with the best {ML} potentials for single molecules or materials, while remaining applicable to the general class of organic molecules composed of the elements {CHNO}.},
	pages = {241733},
	number = {24},
	journaltitle = {The Journal of Chemical Physics},
	author = {Smith, Justin S. and Nebgen, Ben and Lubbers, Nicholas and Isayev, Olexandr and Roitberg, Adrian E.},
	urldate = {2024-07-30},
	date = {2018-05},
}

@article{nussbaumer-streit_resource_2021,
	title = {Resource use during systematic review production varies widely: a scoping review},
	volume = {139},
	issn = {0895-4356},
	url = {https://www.sciencedirect.com/science/article/pii/S0895435621001712},
	doi = {10.1016/j.jclinepi.2021.05.019},
	shorttitle = {Resource use during systematic review production varies widely},
	abstract = {Objective We aimed to map the resource use during systematic review ({SR}) production and reasons why steps of the {SR} production are resource intensive to discover where the largest gain in improving efficiency might be possible. Study design and setting We conducted a scoping review. An information specialist searched multiple databases (e.g., Ovid {MEDLINE}, Scopus) and implemented citation-based and grey literature searching. We employed dual and independent screenings of records at the title/abstract and full-text levels and data extraction. Results We included 34 studies. Thirty-two reported on the resource use—mostly time; four described reasons why steps of the review process are resource intensive. Study selection, data extraction, and critical appraisal seem to be very resource intensive, while protocol development, literature search, or study retrieval take less time. Project management and administration required a large proportion of {SR} production time. Lack of experience, domain knowledge, use of collaborative and {SR}-tailored software, and good communication and management can be reasons why {SR} steps are resource intensive. Conclusion Resource use during {SR} production varies widely. Areas with the largest resource use are administration and project management, study selection, data extraction, and critical appraisal of studies.},
	pages = {287--296},
	journaltitle = {Journal of Clinical Epidemiology},
	author = {Nussbaumer-Streit, B. and Ellen, M. and Klerings, I. and Sfetcu, R. and Riva, N. and Mahmić-Kaknjo, M. and Poulentzas, G. and Martinez, P. and Baladia, E. and Ziganshina, L. E. and Marqués, M. E. and Aguilar, L. and Kassianos, A. P. and Frampton, G. and Silva, A. G. and Affengruber, L. and Spjker, R. and Thomas, J. and Berg, R. C. and Kontogiani, M. and Sousa, M. and Kontogiorgis, C. and Gartlehner, G.},
	urldate = {2024-07-29},
	date = {2021-11},
	keywords = {Costs, Efficient, Evidence synthesis, Personnel, Resources, Time},
}

@article{antunes_preoperative_nodate,
	title = {Preoperative statin therapy for adults undergoing cardiac surgery - Marques Antunes, M - 2024 {\textbar} Cochrane Library},
	issn = {1465-1858},
	url = {https://www.cochranelibrary.com/cdsr/doi/10.1002/14651858.CD008493.pub5/full},
	author = {Antunes, Miguel Marques and Nunes-Ferreira, Afonso and Duarte, Gonçalo S. and Melo, Ryan Gouveia e and Rodrigues, Bárbara Sucena and Guerra, Nuno C. and Nobre, Angelo and Pinto, Fausto J. and Costa, João and Caldeira, Daniel},
	urldate = {2024-07-29},
	langid = {american},
}

@article{felizardo_visual_2012,
	title = {A visual analysis approach to validate the selection review of primary studies in systematic reviews},
	volume = {54},
	issn = {0950-5849},
	url = {https://www.sciencedirect.com/science/article/pii/S0950584912000742},
	doi = {10.1016/j.infsof.2012.04.003},
	abstract = {Context Systematic Literature Reviews ({SLRs}) are an important component to identify and aggregate research evidence from different empirical studies. Despite its relevance, most of the process is conducted manually, implying additional effort when the Selection Review task is performed and leading to reading all studies under analysis more than once. Objective We propose an approach based on Visual Text Mining ({VTM}) techniques to assist the Selection Review task in {SLR}. It is implemented into a {VTM} tool (Revis), which is freely available for use. Method We have selected and implemented appropriate visualization techniques into our approach and validated and demonstrated its usefulness in performing real {SLRs}. Results The results have shown that employment of {VTM} techniques can successfully assist in the Selection Review task, speeding up the entire {SLR} process in comparison to the conventional approach. Conclusion {VTM} techniques are valuable tools to be used in the context of selecting studies in the {SLR} process, prone to speed up some stages of {SLRs}.},
	pages = {1079--1091},
	number = {10},
	journaltitle = {Information and Software Technology},
	author = {Felizardo, Katia R. and Andery, Gabriel F. and Paulovich, Fernando V. and Minghim, Rosane and Maldonado, José C.},
	urldate = {2024-07-29},
	date = {2012-10},
	keywords = {Citation document map, Content document map, Information visualization, Systematic Literature Review ({SLR}), Visual Text Mining ({VTM})},
}

@article{giummarra_evaluation_2020,
	title = {Evaluation of text mining to reduce screening workload for injury-focused systematic reviews},
	volume = {26},
	issn = {1353-8047, 1475-5785},
	url = {https://injuryprevention.bmj.com/content/26/1/55},
	doi = {10.1136/injuryprev-2019-043247},
	abstract = {Introduction Text mining to support screening in large-scale systematic reviews has been recommended; however, their suitability for reviews in injury research is not known. We examined the performance of text mining in supporting the second reviewer in a systematic review examining associations between fault attribution and health and work-related outcomes after transport injury. Methods Citations were independently screened in Abstrackr in full (reviewer 1; 10 559 citations), and until no more citations were predicted to be relevant (reviewer 2; 1809 citations, 17.1\%). All potentially relevant full-text articles were assessed by reviewer 1 (555 articles). Reviewer 2 used text mining (Wordstat, {QDA} Miner) to reduce assessment to full-text articles containing ≥1 fault-related exposure term (367 articles, 66.1\%). Results Abstrackr offered excellent workload savings: 82.7\% of citations did not require screening by reviewer 2, and total screening time was reduced by 36.6\% compared with traditional dual screening of all citations. Abstrackr predictions had high specificity (83.7\%), and low false negatives (0.3\%), but overestimated citation relevance, probably due to the complexity of the review with multiple outcomes and high imbalance of relevant to irrelevant records, giving low sensitivity (29.7\%) and precision (14.5\%). Text mining of full-text articles reduced the number needing to be screened by 33.9\%, and reduced total full-text screening time by 38.7\% compared with traditional dual screening. Conclusions Overall, text mining offered important benefits to systematic review workflow, but should not replace full screening by one reviewer, especially for complex reviews examining multiple health or injury outcomes. Trial registration number {CRD}42018084123.},
	pages = {55--60},
	number = {1},
	journaltitle = {Injury Prevention},
	author = {Giummarra, Melita J. and Lau, Georgina and Gabbe, Belinda J.},
	urldate = {2024-07-29},
	date = {2020-02},
	langid = {english},
	pmid = {31451565},
	note = {tex.copyright: © Author(s) (or their employer(s)) 2020. No commercial re-use. See rights and permissions. Published by {BMJ}.},
	keywords = {systematic reviews, injury, research methods, road trauma, text mining, transport injury},
}

@article{shemilt_use_2016,
	title = {Use of cost-effectiveness analysis to compare the efficiency of study identification methods in systematic reviews},
	volume = {5},
	issn = {2046-4053},
	doi = {10.1186/s13643-016-0315-4},
	abstract = {{BACKGROUND}: Meta-research studies investigating methods, systems, and processes designed to improve the efficiency of systematic review workflows can contribute to building an evidence base that can help to increase value and reduce waste in research. This study demonstrates the use of an economic evaluation framework to compare the costs and effects of four variant approaches to identifying eligible studies for consideration in systematic reviews. {METHODS}: A cost-effectiveness analysis was conducted using a basic decision-analytic model, to compare the relative efficiency of 'safety first', 'double screening', 'single screening' and 'single screening with text mining' approaches in the title-abstract screening stage of a 'case study' systematic review about undergraduate medical education in {UK} general practice settings. Incremental cost-effectiveness ratios ({ICERs}) were calculated as the 'incremental cost per citation 'saved' from inappropriate exclusion' from the review. Resource use and effect parameters were estimated based on retrospective analysis of 'review process' meta-data curated alongside the 'case study' review, in conjunction with retrospective simulation studies to model the integrated use of text mining. Unit cost parameters were estimated based on the 'case study' review's project budget. A base case analysis was conducted, with deterministic sensitivity analyses to investigate the impact of variations in values of key parameters. {RESULTS}: Use of 'single screening with text mining' would have resulted in title-abstract screening workload reductions (base case analysis) of {\textgreater}60 \% compared with other approaches. Across modelled scenarios, the 'safety first' approach was, consistently, equally effective and less costly than conventional 'double screening'. Compared with 'single screening with text mining', estimated {ICERs} for the two non-dominated approaches (base case analyses) ranged from £1975 ('single screening' without a 'provisionally included' code) to £4427 ('safety first' with a 'provisionally included' code) per citation 'saved'. Patterns of results were consistent between base case and sensitivity analyses. {CONCLUSIONS}: Alternatives to the conventional 'double screening' approach, integrating text mining, warrant further consideration as potentially more efficient approaches to identifying eligible studies for systematic reviews. Comparable economic evaluations conducted using other systematic review datasets are needed to determine the generalisability of these findings and to build an evidence base to inform guidance for review authors.},
	pages = {140},
	number = {1},
	journaltitle = {Systematic Reviews},
	author = {Shemilt, Ian and Khan, Nada and Park, Sophie and Thomas, James},
	date = {2016-08},
	pmid = {27535658},
	note = {tex.pmcid: {PMC}4989498},
	keywords = {Humans, Cost-Benefit Analysis, Data Mining, Patient Safety, Research Design},
}

@article{ghasemi_scientific_2022,
	title = {Scientific Publishing in Biomedicine: A Brief History of Scientific Journals},
	volume = {21},
	issn = {1726-913X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10024814/},
	doi = {10.5812/ijem-131812},
	shorttitle = {Scientific Publishing in Biomedicine},
	abstract = {Scientific publishing, with about 350-year historical background, has played a central role in advancing science by disseminating new findings, generalizing accepted theories, and sharing novel ideas. The number of scientific journals has exponentially grown from 10 at the end of the 17th century to 100,000 at the end of the 20th century. The publishing landscape has dramatically changed over time from printed journals to online publishing. Although scientific publishing was initially non-commercial, it has become a profitable industry with a significant global financial turnover, reaching \$28 billion in annual revenue before the {COVID}-19 pandemic. However, scientific publishing has encountered several challenges and is suffering from unethical practices and some negative phenomena, like publish-or-perish, driven by the need to survive or get a promotion in academia. Developing a global landscape with collaborative non-commercial journals and platforms is a primary proposed model for the future of scientific publishing. Here, we provide a brief history of the foundation and development of scientific journals and their evolution over time. Furthermore, current challenges and future perspectives of scientific publishing are discussed.},
	pages = {e131812},
	number = {1},
	journaltitle = {International Journal of Endocrinology and Metabolism},
	author = {Ghasemi, Asghar and Mirmiran, Parvin and Kashfi, Khosrow and Bahadoran, Zahra},
	urldate = {2024-07-29},
	date = {2022-12},
	pmid = {36945344},
	note = {tex.pmcid: {PMC}10024814},
}

@article{tawfik_step_2019,
	title = {A step by step guide for conducting a systematic review and meta-analysis with simulation data},
	volume = {47},
	issn = {1349-4147},
	url = {https://doi.org/10.1186/s41182-019-0165-6},
	doi = {10.1186/s41182-019-0165-6},
	abstract = {The massive abundance of studies relating to tropical medicine and health has increased strikingly over the last few decades. In the field of tropical medicine and health, a well-conducted systematic review and meta-analysis ({SR}/{MA}) is considered a feasible solution for keeping clinicians abreast of current evidence-based medicine. Understanding of {SR}/{MA} steps is of paramount importance for its conduction. It is not easy to be done as there are obstacles that could face the researcher. To solve those hindrances, this methodology study aimed to provide a step-by-step approach mainly for beginners and junior researchers, in the field of tropical medicine and other health care fields, on how to properly conduct a {SR}/{MA}, in which all the steps here depicts our experience and expertise combined with the already well-known and accepted international guidance.},
	pages = {46},
	number = {1},
	journaltitle = {Tropical Medicine and Health},
	author = {Tawfik, Gehad Mohamed and Dila, Kadek Agus Surya and Mohamed, Muawia Yousif Fadlelmola and Tam, Dao Ngoc Hien and Kien, Nguyen Dang and Ahmed, Ali Mahmoud and Huy, Nguyen Tien},
	urldate = {2024-07-29},
	date = {2019-08},
	keywords = {Analysis, Data, Extraction, Results, Search, Study},
}

@incollection{noauthor_cochrane_nodate,
	title = {Cochrane Handbook for Systematic Reviews of Interventions},
}

@incollection{howick_front_2011,
	title = {Front Matter},
	isbn = {978-1-4443-4267-3},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781444342673.fmatter},
	abstract = {The prelims comprise: Half-Title Page Dedication Page Title Page Copyright Page Table of Contents Acknowledgments Foreword Preface},
	pages = {i--xiv},
	booktitle = {The Philosophy of Evidence-Based Medicine},
	publisher = {John Wiley \& Sons, Ltd},
	author = {Howick, Jeremy},
	urldate = {2024-07-29},
	date = {2011},
	langid = {english},
	doi = {10.1002/9781444342673.fmatter},
}

@misc{noauthor_philosophy_nodate,
	title = {The philosophy of evidence based medicine - Google Search},
	url = {https://www.google.com/search?q=The+philosophy+of+evidence+based+medicine&oq=The+philosophy+of+evidence+based+medicine&gs_lcrp=EgZjaHJvbWUyBggAEEUYOTIICAEQABgWGB4yDQgCEAAYhgMYgAQYigUyCggDEAAYgAQYogQyCggEEAAYgAQYogQyBggFEEUYPTIGCAYQRRg8MgYIBxBFGDzSAQkxMDU2MmowajmoAgCwAgE&sourceid=chrome&ie=UTF-8},
	urldate = {2024-07-29},
}

@article{kranke_evidence-based_2010,
	title = {Evidence-based practice: how to perform and use systematic reviews for clinical decision-making},
	volume = {27},
	issn = {1365-2346},
	doi = {10.1097/EJA.0b013e32833a560a},
	shorttitle = {Evidence-based practice},
	abstract = {One approach to clinical decision-making requires the integration of the best available research evidence with individual clinical expertise and patient values, and is known as evidence-based medicine ({EBM}). In clinical decision-making with the current best evidence, systematic reviews have an important role. This review article covers the basic principles of systematic reviews and meta-analyses, and their role in the process of evidence-based decision-making. The problems associated with traditional narrative reviews are discussed, as well as the way systematic reviews limit bias associated with the assembly, critical appraisal and synthesis of studies addressing specific clinical questions. The relevant steps in writing a systematic review from the formulation of an initial research question to sensitivity analyses in conjunction with the combined analysis of the pooled data are described. Important issues that need to be considered when appraising a systematic review or meta-analysis are outlined. Some of the terms that are used in the reporting of systematic reviews and meta-analyses, such as relative risk, confidence interval, Forest plot or L'Abbé plot, will be introduced and explained.},
	pages = {763--772},
	number = {9},
	journaltitle = {European Journal of Anaesthesiology},
	author = {Kranke, Peter},
	date = {2010-09},
	pmid = {20523217},
	keywords = {Humans, Statistics as Topic, Clinical Trials as Topic, Decision Making, Decision Support Techniques, Evidence-Based Medicine, Meta-Analysis as Topic, Review Literature as Topic, Risk, Data Interpretation, Statistical},
}

@unpublished{fletcher_predicting_2024,
	title = {Predicting Retracted Research},
	abstract = {Retracting published research is an important safeguard against the dissemination of flawed or fraudulent scientific information. However, detecting problematic research prior to publication remains a challenge. This paper describes the creation of a novel dataset and machine learning models to predict retracted articles. The data set combines information from the Retraction Watch database and the {OpenAlex} {API}, including article metadata, abstracts, and citation metrics. A total of 16,224 articles (8,112 retracted and 8,112 nonretracted) published between 2000-2020 were included. Several machine learning models were trained on this data, with a gradient boosting approach achieving the best precision (0.691). An ablation study revealed that the abstract of the article was the most important feature for classification for the accuracy, recall, and F1 score metric. First Author Countries was the more important feature for feature-based classifers with the Precision Metric. This work demonstrates the potential for using machine learning to assist in identifying problematic research during the peer review process,though further improvements in model performance are needed before practi- cal application. The data set and code are made publicly available to support future work in this area.},
	author = {Fletcher, Aaron {HA} and Stevenson, Mark},
	date = {2024-07},
}

@article{guyatt_grade_2008,
	title = {{GRADE}: an emerging consensus on rating quality of evidence and strength of recommendations},
	volume = {336},
	issn = {0959-8138, 1756-1833},
	url = {https://www.bmj.com/content/336/7650/924},
	doi = {10.1136/bmj.39489.470347.AD},
	shorttitle = {{GRADE}},
	abstract = {{\textless}p{\textgreater}Guidelines are inconsistent in how they rate the quality of evidence and the strength of recommendations. This article explores the advantages of the {GRADE} system, which is increasingly being adopted by organisations worldwide {\textless}/p{\textgreater}},
	pages = {924--926},
	number = {7650},
	journaltitle = {{BMJ} (Clinical research ed.)},
	shortjournal = {{BMJ}},
	author = {Guyatt, Gordon H. and Oxman, Andrew D. and Vist, Gunn E. and Kunz, Regina and Falck-Ytter, Yngve and Alonso-Coello, Pablo and Schünemann, Holger J.},
	urldate = {2024-07-29},
	date = {2008-04},
	langid = {english},
	pmid = {18436948},
	note = {tex.copyright: © {BMJ} Publishing Group Ltd 2008},
}

@misc{noauthor_oxford_nodate,
	title = {Oxford Centre for Evidence-Based Medicine: Levels of Evidence (March 2009)},
	url = {https://www.cebm.ox.ac.uk/resources/levels-of-evidence/oxford-centre-for-evidence-based-medicine-levels-of-evidence-march-2009},
	shorttitle = {Oxford Centre for Evidence-Based Medicine},
	urldate = {2024-07-29},
	langid = {english},
	note = {Type: Web Page},
}

@article{swanson_how_2010,
	title = {How to Practice Evidence-Based Medicine},
	volume = {126},
	issn = {0032-1052},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4389891/},
	doi = {10.1097/PRS.0b013e3181dc54ee},
	abstract = {Evidence-based medicine ({EBM}) is defined as the conscientious, explicit and judicious use of current best evidence, combined with individual clinical expertise and patient preferences and values, in making decisions about the care of individual patients. In an effort to emphasize the importance of {EBM} in plastic surgery, {ASPS} and {PRS} have launched an initiative to improve the understanding of {EBM} concepts and provide tools for implementing {EBM} in practice. Through a series of special articles aimed at educating plastic surgeons, our hope is that readers will be compelled to learn more about {EBM} and incorporate its principles into their own practices. As the first of the series, this article provides a brief overview of the evolution, current application, and practice of {EBM}.},
	pages = {286--294},
	number = {1},
	journaltitle = {Plastic and reconstructive surgery},
	author = {Swanson, Jennifer A. and Schmitz, {DeLaine} and Chung, Kevin C.},
	urldate = {2024-07-29},
	date = {2010-07},
	pmid = {20224459},
	note = {tex.pmcid: {PMC}4389891},
}

@article{chiong_learning_2008,
	title = {Learning game strategy design through iterated Prisoner's Dilemma},
	volume = {32},
	issn = {0952-8091, 1741-5047},
	url = {http://www.inderscience.com/link.php?id=20957},
	doi = {10.1504/IJCAT.2008.020957},
	pages = {216},
	number = {3},
	journaltitle = {International Journal of Computer Applications in Technology},
	author = {Chiong, Raymond and Jankovic, Lubo},
	urldate = {2024-07-14},
	date = {2008},
	langid = {english},
}

@article{molinari_transferring_2022,
	title = {Transferring knowledge between topics in systematic reviews},
	volume = {16},
	issn = {2667-3053},
	url = {https://www.sciencedirect.com/science/article/pii/S2667305322000874},
	doi = {10.1016/j.iswa.2022.200150},
	abstract = {In the medical domain, a systematic review ({SR}) is a well-structured process aimed to review all available literature on a research question. This is however a laborious task, both in terms of money and time. As such, the automation of a {SR} with the aid of technology has received interest in several research communities, among which the Information Retrieval community. In this work, we experiment on the possibility of leveraging previously conducted systematic reviews to train a classifier/ranker which is later applied to a new {SR}. We also investigate on the possibility of pre-training Deep Learning models and eventually tuning them in an Active Learning process. Our results show that the pre-training of these models deliver a good zero-shot (i.e., with no fine-tuning) ranking, achieving an improvement of 79\% for the {MAP} metric, with respect to a standard classifier trained on few in-domain documents. However, the pre-trained deep learning algorithms fail to deliver consistent results when continuously trained in an Active Learning scenario: our analysis shows that using smaller sized models and employing adapter modules might enable an effective active learning training.},
	pages = {200150},
	journaltitle = {Intelligent Systems with Applications},
	author = {Molinari, Alessio and Kanoulas, Evangelos},
	urldate = {2024-06-27},
	date = {2022-11},
	keywords = {Technology-assisted review, Machine learning, Systematic reviews, Deep learning, Transfer learning},
}

@inproceedings{grossman_trec_2016-1,
	title = {{TREC} 2016 Total Recall Track Overview},
	url = {https://www.semanticscholar.org/paper/TREC-2016-Total-Recall-Track-Overview-Grossman-Cormack/126240dedd75626fd736f0485d06f1f516517e54},
	abstract = {The primary purpose of the Total Recall Track is to evaluate, through controlled simulation, methods designed to achieve very high recall – as close as practicable to 100\% – with a human assessor in the loop. Motivating applications include, among others, electronic discovery in legal proceedings [3], systematic review in evidencebased medicine [6], and the creation of fully labeled test collections for information retrieval (“{IR}”) evaluation [5]. A secondary, but no less important, purpose is to develop a sandboxed virtual test environment within which {IR} systems may be tested, while preventing the disclosure of sensitive test data to participants. At the same time, the test environment also operates as a “black box,” affording participants confidence that their proprietary systems cannot easily be reverse engineered. The task to be solved in the Total Recall Track is the following:},
	author = {Grossman, Maura R. and Cormack, G. and Roegiest, Adam},
	urldate = {2024-06-27},
	date = {2016},
}

@inproceedings{althammer_annotating_2023,
	location = {New York, {NY}, {USA}},
	title = {Annotating Data for Fine-Tuning a Neural Ranker? Current Active Learning Strategies are not Better than Random Selection},
	isbn = {9798400704086},
	url = {https://doi.org/10.1145/3624918.3625333},
	doi = {10.1145/3624918.3625333},
	series = {{SIGIR}-{AP} '23},
	shorttitle = {Annotating Data for Fine-Tuning a Neural Ranker?},
	abstract = {Search methods based on Pretrained Language Models ({PLM}) have demonstrated great effectiveness gains compared to statistical and early neural ranking models. However, fine-tuning {PLM}-based rankers requires a great amount of annotated training data. Annotating data involves a large manual effort and thus is expensive, especially in domain specific tasks. In this paper we investigate fine-tuning {PLM}-based rankers under limited training data and budget. We investigate two scenarios: fine-tuning a ranker from scratch, and domain adaptation starting with a ranker already fine-tuned on general data, and continuing fine-tuning on a target dataset. We observe a great variability in effectiveness when fine-tuning on different randomly selected subsets of training data. This suggests that it is possible to achieve effectiveness gains by actively selecting a subset of the training data that has the most positive effect on the rankers. This way, it would be possible to fine-tune effective {PLM} rankers at a reduced annotation budget. To investigate this, we adapt existing Active Learning ({AL}) strategies to the task of fine-tuning {PLM} rankers and investigate their effectiveness, also considering annotation and computational costs. Our extensive analysis shows that {AL} strategies do not significantly outperform random selection of training subsets in terms of effectiveness. We further find that gains provided by {AL} strategies come at the expense of more assessments (thus higher annotation costs) and {AL} strategies underperform random selection when comparing effectiveness given a fixed annotation cost. Our results highlight that “optimal” subsets of training data that provide high effectiveness at low annotation cost do exist, but current mainstream {AL} strategies applied to {PLM} rankers are not capable of identifying them.},
	pages = {139--149},
	booktitle = {Proceedings of the Annual International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval in the Asia Pacific Region},
	publisher = {Association for Computing Machinery},
	author = {Althammer, Sophia and Zuccon, Guido and Hofstätter, Sebastian and Verberne, Suzan and Hanbury, Allan},
	urldate = {2024-06-27},
	date = {2023-11},
}

@misc{ganti_building_2013,
	title = {Building Bridges: Viewing Active Learning from the Multi-Armed Bandit Lens},
	url = {http://arxiv.org/abs/1309.6830},
	shorttitle = {Building Bridges},
	abstract = {In this paper we propose a multi-armed bandit inspired, pool based active learning algorithm for the problem of binary classification. By carefully constructing an analogy between active learning and multi-armed bandits, we utilize ideas such as lower confidence bounds, and self-concordant regularization from the multi-armed bandit literature to design our proposed algorithm. Our algorithm is a sequential algorithm, which in each round assigns a sampling distribution on the pool, samples one point from this distribution, and queries the oracle for the label of this sampled point. The design of this sampling distribution is also inspired by the analogy between active learning and multi-armed bandits. We show how to derive lower confidence bounds required by our algorithm. Experimental comparisons to previously proposed active learning algorithms show superior performance on some standard {UCI} datasets.},
	publisher = {{arXiv}},
	author = {Ganti, Ravi and Gray, Alexander G.},
	urldate = {2024-06-27},
	date = {2013-09},
	doi = {10.48550/arXiv.1309.6830},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{sharma_active_2015,
	location = {Denver, Colorado},
	title = {Active Learning with Rationales for Text Classification},
	url = {https://aclanthology.org/N15-1047},
	doi = {10.3115/v1/N15-1047},
	pages = {441--451},
	booktitle = {Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	publisher = {Association for Computational Linguistics},
	author = {Sharma, Manali and Zhuang, Di and Bilgic, Mustafa},
	editor = {Mihalcea, Rada and Chai, Joyce and Sarkar, Anoop},
	urldate = {2024-06-27},
	date = {2015-05},
}

@inproceedings{huang_active_2010,
	title = {Active Learning by Querying Informative and Representative Examples},
	volume = {23},
	url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/5487315b1286f907165907aa8fc96619-Abstract.html},
	abstract = {Most active learning approaches select either informative or representative unlabeled instances to query their labels. Although several active learning algorithms have been proposed to combine the two criterions for query selection, they are usually ad hoc in finding unlabeled instances that are both informative and representative. We address this challenge by a principled approach, termed {QUIRE}, based on the min-max view of active learning. The proposed approach provides a systematic way for measuring and combining the informativeness and representativeness of an instance. Extensive experimental results show that the proposed {QUIRE} approach outperforms several state-of -the-art active learning approaches.},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Huang, Sheng-jun and Jin, Rong and Zhou, Zhi-Hua},
	urldate = {2024-06-27},
	date = {2010},
}

@article{teijema_active_2023,
	title = {Active learning-based systematic reviewing using switching classification models: the case of the onset, maintenance, and relapse of depressive disorders},
	volume = {8},
	issn = {2504-0537},
	doi = {10.3389/frma.2023.1178181},
	shorttitle = {Active learning-based systematic reviewing using switching classification models},
	abstract = {{INTRODUCTION}: This study examines the performance of active learning-aided systematic reviews using a deep learning-based model compared to traditional machine learning approaches, and explores the potential benefits of model-switching strategies. {METHODS}: Comprising four parts, the study: 1) analyzes the performance and stability of active learning-aided systematic review; 2) implements a convolutional neural network classifier; 3) compares classifier and feature extractor performance; and 4) investigates the impact of model-switching strategies on review performance. {RESULTS}: Lighter models perform well in early simulation stages, while other models show increased performance in later stages. Model-switching strategies generally improve performance compared to using the default classification model alone. {DISCUSSION}: The study's findings support the use of model-switching strategies in active learning-based systematic review workflows. It is advised to begin the review with a light model, such as Naïve Bayes or logistic regression, and switch to a heavier classification model based on a heuristic rule when needed.},
	pages = {1178181},
	journaltitle = {Frontiers in Research Metrics and Analytics},
	author = {Teijema, Jelle Jasper and Hofstee, Laura and Brouwer, Marlies and de Bruin, Jonathan and Ferdinands, Gerbrich and de Boer, Jan and Vizan, Pablo and van den Brand, Sofie and Bockting, Claudi and van de Schoot, Rens and Bagheri, Ayoub},
	date = {2023},
	pmid = {37260784},
	note = {tex.pmcid: {PMC}10227618},
	keywords = {systematic review, active learning, convolutional neural network, model switching, simulations, work saved over sampling},
}

@article{knoblock_active_2006,
	title = {Active Learning with Multiple Views},
	volume = {27},
	issn = {1076-9757},
	url = {http://arxiv.org/abs/1110.1073},
	doi = {10.1613/jair.2005},
	abstract = {Active learners alleviate the burden of labeling large amounts of data by detecting and asking the user to label only the most informative examples in the domain. We focus here on active learning for multi-view domains, in which there are several disjoint subsets of features (views), each of which is sufficient to learn the target concept. In this paper we make several contributions. First, we introduce Co-Testing, which is the first approach to multi-view active learning. Second, we extend the multi-view learning framework by also exploiting weak views, which are adequate only for learning a concept that is more general/specific than the target concept. Finally, we empirically show that Co-Testing outperforms existing active learners on a variety of real world domains such as wrapper induction, Web page classification, advertisement removal, and discourse tree parsing.},
	pages = {203--233},
	journaltitle = {Journal of Artificial Intelligence Research},
	author = {Knoblock, C. A. and Minton, S. and Muslea, I.},
	urldate = {2024-06-27},
	date = {2006-10},
	keywords = {Computer Science - Machine Learning},
}

@misc{akinseloyin_novel_2024,
	title = {A Novel Question-Answering Framework for Automated Abstract Screening Using Large Language Models},
	url = {https://www.medrxiv.org/content/10.1101/2023.12.17.23300102v3},
	abstract = {Objective This paper aims to address the challenges in abstract screening within Systematic Reviews ({SR}) by leveraging the zero-shot capabilities of large language models ({LLMs}). Methods We employ {LLM} to prioritise candidate studies by aligning abstracts with the selection criteria outlined in an {SR} protocol. Abstract screening was transformed into a novel question-answering ({QA}) framework, treating each selection criterion as a question addressed by {LLM}. The framework involves breaking down the selection criteria into multiple questions, properly prompting {LLM} to answer each question, scoring and re-ranking each answer, and combining the responses to make nuanced inclusion or exclusion decisions. Results Large-scale validation was performed on the benchmark of {CLEF} {eHealth} 2019 Task 2: Technology- Assisted Reviews in Empirical Medicine. Focusing on {GPT}-3.5 as a case study, the proposed {QA} framework consistently exhibited a clear advantage over traditional information retrieval approaches and bespoke {BERT}- family models that were fine-tuned for prioritising candidate studies (i.e., from the {BERT} to {PubMedBERT}) across 31 datasets of four categories of {SRs}, underscoring their high potential in facilitating abstract screening. Conclusion Investigation justified the indispensable value of leveraging selection criteria to improve the performance of automated abstract screening. {LLMs} demonstrated proficiency in prioritising candidate studies for abstract screening using the proposed {QA} framework. Significant performance improvements were obtained by re-ranking answers using the semantic alignment between abstracts and selection criteria. This further highlighted the pertinence of utilizing selection criteria to enhance abstract screening.},
	publisher = {{medRxiv}},
	author = {Akinseloyin, Opeoluwa and Jiang, Xiaorui and Palade, Vasile},
	urldate = {2024-06-27},
	date = {2024-06},
	langid = {english},
	doi = {10.1101/2023.12.17.23300102},
	note = {tex.copyright: © 2024, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NonCommercial}-{NoDerivs} 4.0 International), {CC} {BY}-{NC}-{ND} 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
}

@inproceedings{kusa_outcome-based_2023,
	location = {New York, {NY}, {USA}},
	title = {Outcome-based Evaluation of Systematic Review Automation},
	isbn = {9798400700736},
	url = {https://doi.org/10.1145/3578337.3605135},
	doi = {10.1145/3578337.3605135},
	series = {{ICTIR} '23},
	abstract = {Current methods of evaluating search strategies and automated citation screening for systematic literature reviews typically rely on counting the number of relevant publications (i.e. those to be included in the review) and not relevant publications (i.e. those to be excluded). Significant importance is put into promoting the retrieval of all relevant publications through great attention to recall-oriented measures, and demoting the retrieval of non-relevant publications through precision-oriented or cost metrics. This established practice, however, does not accurately reflect the reality of conducting a systematic review, because not all included publications have the same influence on the final outcome of the systematic review. More specifically, if an important publication gets excluded or included, this might significantly change the overall review outcome, while not including or excluding less influential studies may only have a limited impact. However, in terms of evaluation measures, all inclusion and exclusion decisions are treated equally and, therefore, failing to retrieve publications with little to no impact on the review outcome leads to the same decrease in recall as failing to retrieve crucial publications.We propose a new evaluation framework that takes into account the impact of the reported study on the overall systematic review outcome. We demonstrate the framework by extracting review meta-analysis data and estimating outcome effects using predictions from ranking runs on systematic reviews of interventions from {CLEF} {TAR} 2019 shared task. We further measure how closely the obtained outcomes are to the outcomes of the original review if the arbitrary rankings were used. We evaluate 74 runs using the proposed framework and compare the results with those obtained using standard {IR} measures. We find that accounting for the difference in review outcomes leads to a different assessment of the quality of a system than if traditional evaluation measures were used. Our analysis provides new insights into the evaluation of retrieval results in the context of systematic review automation, emphasising the importance of assessing the usefulness of each document beyond binary relevance.},
	pages = {125--133},
	booktitle = {Proceedings of the 2023 {ACM} {SIGIR} International Conference on Theory of Information Retrieval},
	publisher = {Association for Computing Machinery},
	author = {Kusa, Wojciech and Zuccon, Guido and Knoth, Petr and Hanbury, Allan},
	urldate = {2024-06-27},
	date = {2023-08},
}

@article{norman_measuring_2019,
	title = {Measuring the impact of screening automation on meta-analyses of diagnostic test accuracy},
	volume = {8},
	issn = {2046-4053},
	url = {https://doi.org/10.1186/s13643-019-1162-x},
	doi = {10.1186/s13643-019-1162-x},
	abstract = {The large and increasing number of new studies published each year is making literature identification in systematic reviews ever more time-consuming and costly. Technological assistance has been suggested as an alternative to the conventional, manual study identification to mitigate the cost, but previous literature has mainly evaluated methods in terms of recall (search sensitivity) and workload reduction. There is a need to also evaluate whether screening prioritization methods leads to the same results and conclusions as exhaustive manual screening. In this study, we examined the impact of one screening prioritization method based on active learning on sensitivity and specificity estimates in systematic reviews of diagnostic test accuracy.},
	pages = {243},
	number = {1},
	journaltitle = {Systematic Reviews},
	author = {Norman, Christopher R. and Leeflang, Mariska M. G. and Porcher, Raphaël and Névéol, Aurélie},
	urldate = {2024-06-27},
	date = {2019-10},
	keywords = {*Machine learning, *Systematic review as topic, Evidence based medicine, Natural language processing/*methods},
}

@inproceedings{wang_neural_2022,
	title = {Neural Rankers for Effective Screening Prioritisation in Medical Systematic Review Literature Search},
	url = {http://arxiv.org/abs/2212.09017},
	doi = {10.1145/3572960.3572980},
	abstract = {Medical systematic reviews typically require assessing all the documents retrieved by a search. The reason is two-fold: the task aims for “total recall”; and documents retrieved using Boolean search are an unordered set, and thus it is unclear how an assessor could examine only a subset. Screening prioritisation is the process of ranking the (unordered) set of retrieved documents, allowing assessors to begin the downstream processes of the systematic review creation earlier, leading to earlier completion of the review, or even avoiding screening documents ranked least relevant. Screening prioritisation requires highly effective ranking methods. Pre-trained language models are state-of-the-art on many {IR} tasks but have yet to be applied to systematic review screening prioritisation. In this paper, we apply several pre-trained language models to the systematic review document ranking task, both directly and fine-tuned. An empirical analysis compares how effective neural methods compare to traditional methods for this task. We also investigate different types of document representations for neural methods and their impact on ranking performance. Our results show that {BERT}-based rankers outperform the current state-of-the-art screening prioritisation methods. However, {BERT} rankers and existing methods can actually be complementary, and thus, further improvements may be achieved if used in conjunction.},
	pages = {1--10},
	booktitle = {Proceedings of the 26th Australasian Document Computing Symposium},
	author = {Wang, Shuai and Scells, Harrisen and Koopman, Bevan and Zuccon, Guido},
	urldate = {2024-06-27},
	date = {2022-12},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Information Retrieval},
}

@misc{munkhdalai_leave_2024,
	title = {Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention},
	url = {http://arxiv.org/abs/2404.07143},
	shorttitle = {Leave No Context Behind},
	abstract = {This work introduces an efficient method to scale Transformer-based Large Language Models ({LLMs}) to infinitely long inputs with bounded memory and computation. A key component in our proposed approach is a new attention technique dubbed Infini-attention. The Infini-attention incorporates a compressive memory into the vanilla attention mechanism and builds in both masked local attention and long-term linear attention mechanisms in a single Transformer block. We demonstrate the effectiveness of our approach on long-context language modeling benchmarks, 1M sequence length passkey context block retrieval and 500K length book summarization tasks with 1B and 8B {LLMs}. Our approach introduces minimal bounded memory parameters and enables fast streaming inference for {LLMs}.},
	publisher = {{arXiv}},
	author = {Munkhdalai, Tsendsuren and Faruqui, Manaal and Gopal, Siddharth},
	urldate = {2024-06-27},
	date = {2024-04},
	doi = {10.48550/arXiv.2404.07143},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@misc{zhuang_promptreps_2024,
	title = {{PromptReps}: Prompting Large Language Models to Generate Dense and Sparse Representations for Zero-Shot Document Retrieval},
	url = {http://arxiv.org/abs/2404.18424},
	shorttitle = {{PromptReps}},
	abstract = {Utilizing large language models ({LLMs}) for zero-shot document ranking is done in one of two ways: 1) prompt-based re-ranking methods, which require no further training but are only feasible for re-ranking a handful of candidate documents due to computational costs; and 2) unsupervised contrastive trained dense retrieval methods, which can retrieve relevant documents from the entire corpus but require a large amount of paired text data for contrastive training. In this paper, we propose {PromptReps}, which combines the advantages of both categories: no need for training and the ability to retrieve from the whole corpus. Our method only requires prompts to guide an {LLM} to generate query and document representations for effective document retrieval. Specifically, we prompt the {LLMs} to represent a given text using a single word, and then use the last token's hidden states and the corresponding logits associated with the prediction of the next token to construct a hybrid document retrieval system. The retrieval system harnesses both dense text embedding and sparse bag-of-words representations given by the {LLM}. We further explore variations of this core idea that consider the generation of multiple words, and representations that rely on multiple embeddings and sparse distributions. Our experimental evaluation on the {MSMARCO}, {TREC} deep learning and {BEIR} zero-shot document retrieval datasets illustrates that this simple prompt-based {LLM} retrieval method can achieve a similar or higher retrieval effectiveness than state-of-the-art {LLM} embedding methods that are trained with large amounts of unsupervised data, especially when using a larger {LLM}.},
	publisher = {{arXiv}},
	author = {Zhuang, Shengyao and Ma, Xueguang and Koopman, Bevan and Lin, Jimmy and Zuccon, Guido},
	urldate = {2024-06-27},
	date = {2024-06},
	doi = {10.48550/arXiv.2404.18424},
	keywords = {Computer Science - Information Retrieval},
}

@misc{margatina_importance_2022,
	title = {On the Importance of Effectively Adapting Pretrained Language Models for Active Learning},
	url = {http://arxiv.org/abs/2104.08320},
	abstract = {Recent Active Learning ({AL}) approaches in Natural Language Processing ({NLP}) proposed using off-the-shelf pretrained language models ({LMs}). In this paper, we argue that these {LMs} are not adapted effectively to the downstream task during {AL} and we explore ways to address this issue. We suggest to first adapt the pretrained {LM} to the target task by continuing training with all the available unlabeled data and then use it for {AL}. We also propose a simple yet effective fine-tuning method to ensure that the adapted {LM} is properly trained in both low and high resource scenarios during {AL}. Our experiments demonstrate that our approach provides substantial data efficiency improvements compared to the standard fine-tuning approach, suggesting that a poor training strategy can be catastrophic for {AL}.},
	publisher = {{arXiv}},
	author = {Margatina, Katerina and Barrault, Loïc and Aletras, Nikolaos},
	urldate = {2024-06-27},
	date = {2022-03},
	doi = {10.48550/arXiv.2104.08320},
	keywords = {Computer Science - Computation and Language},
}

@article{carvallo_automatic_2020,
	title = {Automatic document screening of medical literature using word and text embeddings in an active learning setting},
	volume = {125},
	issn = {1588-2861},
	url = {https://doi.org/10.1007/s11192-020-03648-6},
	doi = {10.1007/s11192-020-03648-6},
	abstract = {Document screening is a fundamental task within Evidence-based Medicine ({EBM}), a practice that provides scientific evidence to support medical decisions. Several approaches have tried to reduce physicians’ workload of screening and labeling vast amounts of documents to answer clinical questions. Previous works tried to semi-automate document screening, reporting promising results, but their evaluation was conducted on small datasets, which hinders generalization. Moreover, recent works in natural language processing have introduced neural language models, but none have compared their performance in {EBM}. In this paper, we evaluate the impact of several document representations such as {TF}-{IDF} along with neural language models ({BioBERT}, {BERT}, Word2Vec, and {GloVe}) on an active learning-based setting for document screening in {EBM}. Our goal is to reduce the number of documents that physicians need to label to answer clinical questions. We evaluate these methods using both a small challenging dataset ({CLEF} {eHealth} 2017) as well as a larger one but easier to rank (Epistemonikos). Our results indicate that word as well as textual neural embeddings always outperform the traditional {TF}-{IDF} representation. When comparing among neural and textual embeddings, in the {CLEF} {eHealth} dataset the models {BERT} and {BioBERT} yielded the best results. On the larger dataset, Epistemonikos, Word2Vec and {BERT} were the most competitive, showing that {BERT} was the most consistent model across different corpuses. In terms of active learning, an uncertainty sampling strategy combined with a logistic regression achieved the best performance overall, above other methods under evaluation, and in fewer iterations. Finally, we compared the results of evaluating our best models, trained using active learning, with other authors methods from {CLEF} {eHealth}, showing better results in terms of work saved for physicians in the document-screening task.},
	pages = {3047--3084},
	number = {3},
	journaltitle = {Scientometrics},
	author = {Carvallo, Andres and Parra, Denis and Lobel, Hans and Soto, Alvaro},
	urldate = {2024-06-27},
	date = {2020-12},
	langid = {english},
	keywords = {Active learning, Natural language processing, Document screening},
}

@article{molinari_sal_2024,
	title = {{SALτ}: efficiently stopping {TAR} by improving priors estimates},
	volume = {38},
	issn = {1573-756X},
	url = {https://doi.org/10.1007/s10618-023-00961-5},
	doi = {10.1007/s10618-023-00961-5},
	shorttitle = {{SALτ}},
	abstract = {In high recall retrieval tasks, human experts review a large pool of documents with the goal of satisfying an information need. Documents are prioritized for review through an active learning policy, and the process is usually referred to as Technology-Assisted Review ({TAR}). {TAR} tasks also aim to stop the review process once the target recall is achieved to minimize the annotation cost. In this paper, we introduce a new stopping rule called {SAL}\$\$\_{\textbackslash}tau {\textasciicircum}R\$\$({SLD} for Active Learning), a modified version of the Saerens–Latinne–Decaestecker algorithm ({SLD}) that has been adapted for use in active learning. Experiments show that our algorithm stops the review well ahead of the current state-of-the-art methods, while providing the same guarantees of achieving the target recall.},
	pages = {535--568},
	number = {2},
	journaltitle = {Data Mining and Knowledge Discovery},
	author = {Molinari, Alessio and Esuli, Andrea},
	urldate = {2024-06-27},
	date = {2024-03},
	langid = {english},
	keywords = {Active learning, e-Discovery, Systematic review, {TAR}, Technology-assisted review},
}

@misc{yang_contextualization_2024,
	title = {Contextualization with {SPLADE} for High Recall Retrieval},
	url = {http://arxiv.org/abs/2405.03972},
	abstract = {High Recall Retrieval ({HRR}), such as {eDiscovery} and medical systematic review, is a search problem that optimizes the cost of retrieving most relevant documents in a given collection. Iterative approaches, such as iterative relevance feedback and uncertainty sampling, are shown to be effective under various operational scenarios. Despite neural models demonstrating success in other text-related tasks, linear models such as logistic regression, in general, are still more effective and efficient in {HRR} since the model is trained and retrieves documents from the same fixed collection. In this work, we leverage {SPLADE}, an efficient retrieval model that transforms documents into contextualized sparse vectors, for {HRR}. Our approach combines the best of both worlds, leveraging both the contextualization from pretrained language models and the efficiency of linear models. It reduces 10\% and 18\% of the review cost in two {HRR} evaluation collections under a one-phase review workflow with a target recall of 80\%. The experiment is implemented with {TARexp} and is available at https://github.com/eugene-yang/{LSR}-for-{TAR}.},
	author = {Yang, Eugene},
	urldate = {2024-06-27},
	date = {2024-05},
	doi = {10.1145/3626772.3657919},
	keywords = {Computer Science - Information Retrieval},
}

@misc{bin-hezam_rlstop_2024,
	title = {{RLStop}: A Reinforcement Learning Stopping Method for {TAR}},
	url = {http://arxiv.org/abs/2405.02525},
	shorttitle = {{RLStop}},
	abstract = {We present {RLStop}, a novel Technology Assisted Review ({TAR}) stopping rule based on reinforcement learning that helps minimise the number of documents that need to be manually reviewed within {TAR} applications. {RLStop} is trained on example rankings using a reward function to identify the optimal point to stop examining documents. Experiments at a range of target recall levels on multiple benchmark datasets ({CLEF} e-Health, {TREC} Total Recall, and Reuters {RCV}1) demonstrated that {RLStop} substantially reduces the workload required to screen a document collection for relevance. {RLStop} outperforms a wide range of alternative approaches, achieving performance close to the maximum possible for the task under some circumstances.},
	author = {Bin-Hezam, Reem and Stevenson, Mark},
	urldate = {2024-06-27},
	date = {2024-06},
	doi = {10.1145/3626772.3657911},
	keywords = {Computer Science - Information Retrieval},
}

@misc{sutskever_sequence_2014,
	title = {Sequence to Sequence Learning with Neural Networks},
	url = {http://arxiv.org/abs/1409.3215},
	abstract = {Deep Neural Networks ({DNNs}) are powerful models that have achieved excellent performance on difficult learning tasks. Although {DNNs} work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory ({LSTM}) to map the input sequence to a vector of a fixed dimensionality, and then another deep {LSTM} to decode the target sequence from the vector. Our main result is that on an English to French translation task from the {WMT}'14 dataset, the translations produced by the {LSTM} achieve a {BLEU} score of 34.8 on the entire test set, where the {LSTM}'s {BLEU} score was penalized on out-of-vocabulary words. Additionally, the {LSTM} did not have difficulty on long sentences. For comparison, a phrase-based {SMT} system achieves a {BLEU} score of 33.3 on the same dataset. When we used the {LSTM} to rerank the 1000 hypotheses produced by the aforementioned {SMT} system, its {BLEU} score increases to 36.5, which is close to the previous best result on this task. The {LSTM} also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the {LSTM}'s performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
	publisher = {{arXiv}},
	author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
	urldate = {2024-06-27},
	date = {2014-12},
	doi = {10.48550/arXiv.1409.3215},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{cormack_autonomy_2015,
	title = {Autonomy and Reliability of Continuous Active Learning for Technology-Assisted Review},
	url = {http://arxiv.org/abs/1504.06868},
	abstract = {We enhance the autonomy of the continuous active learning method shown by Cormack and Grossman ({SIGIR} 2014) to be effective for technology-assisted review, in which documents from a collection are retrieved and reviewed, using relevance feedback, until substantially all of the relevant documents have been reviewed. Autonomy is enhanced through the elimination of topic-specific and dataset-specific tuning parameters, so that the sole input required by the user is, at the outset, a short query, topic description, or single relevant document; and, throughout the review, ongoing relevance assessments of the retrieved documents. We show that our enhancements consistently yield superior results to Cormack and Grossman's version of continuous active learning, and other methods, not only on average, but on the vast majority of topics from four separate sets of tasks: the legal datasets examined by Cormack and Grossman, the Reuters {RCV}1-v2 subject categories, the {TREC} 6 {AdHoc} task, and the construction of the {TREC} 2002 filtering test collection.},
	publisher = {{arXiv}},
	author = {Cormack, Gordon V. and Grossman, Maura R.},
	urldate = {2024-06-27},
	date = {2015-04},
	doi = {10.48550/arXiv.1504.06868},
	keywords = {Computer Science - Machine Learning, Computer Science - Information Retrieval},
}

@misc{wang_zero-shot_2024,
	title = {Zero-shot Generative Large Language Models for Systematic Review Screening Automation},
	url = {http://arxiv.org/abs/2401.06320},
	abstract = {Systematic reviews are crucial for evidence-based medicine as they comprehensively analyse published research findings on specific questions. Conducting such reviews is often resource- and time-intensive, especially in the screening phase, where abstracts of publications are assessed for inclusion in a review. This study investigates the effectiveness of using zero-shot large language models{\textasciitilde}({LLMs}) for automatic screening. We evaluate the effectiveness of eight different {LLMs} and investigate a calibration technique that uses a predefined recall threshold to determine whether a publication should be included in a systematic review. Our comprehensive evaluation using five standard test collections shows that instruction fine-tuning plays an important role in screening, that calibration renders {LLMs} practical for achieving a targeted recall, and that combining both with an ensemble of zero-shot models saves significant screening time compared to state-of-the-art approaches.},
	publisher = {{arXiv}},
	author = {Wang, Shuai and Scells, Harrisen and Zhuang, Shengyao and Potthast, Martin and Koopman, Bevan and Zuccon, Guido},
	urldate = {2024-06-27},
	date = {2024-01},
	doi = {10.48550/arXiv.2401.06320},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
}

@misc{yang_goldilocks_2022,
	title = {Goldilocks: Just-Right Tuning of {BERT} for Technology-Assisted Review},
	url = {http://arxiv.org/abs/2105.01044},
	shorttitle = {Goldilocks},
	abstract = {Technology-assisted review ({TAR}) refers to iterative active learning workflows for document review in high recall retrieval ({HRR}) tasks. {TAR} research and most commercial {TAR} software have applied linear models such as logistic regression to lexical features. Transformer-based models with supervised tuning are known to improve effectiveness on many text classification tasks, suggesting their use in {TAR}. We indeed find that the pre-trained {BERT} model reduces review cost by 10\% to 15\% in {TAR} workflows simulated on the {RCV}1-v2 newswire collection. In contrast, we likewise determined that linear models outperform {BERT} for simulated legal discovery topics on the Jeb Bush e-mail collection. This suggests the match between transformer pre-training corpora and the task domain is of greater significance than generally appreciated. Additionally, we show that just-right language model fine-tuning on the task collection before starting active learning is critical. Too little or too much fine-tuning hinders performance, worse than that of linear models, even for a favorable corpus such as {RCV}1-v2.},
	publisher = {{arXiv}},
	author = {Yang, Eugene and {MacAvaney}, Sean and Lewis, David D. and Frieder, Ophir},
	urldate = {2024-06-27},
	date = {2022-01},
	doi = {10.48550/arXiv.2105.01044},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
}

@misc{vladika_scientific_2023,
	title = {Scientific Fact-Checking: A Survey of Resources and Approaches},
	url = {http://arxiv.org/abs/2305.16859},
	shorttitle = {Scientific Fact-Checking},
	abstract = {The task of fact-checking deals with assessing the veracity of factual claims based on credible evidence and background knowledge. In particular, scientific fact-checking is the variation of the task concerned with verifying claims rooted in scientific knowledge. This task has received significant attention due to the growing importance of scientific and health discussions on online platforms. Automated scientific fact-checking methods based on {NLP} can help combat the spread of misinformation, assist researchers in knowledge discovery, and help individuals understand new scientific breakthroughs. In this paper, we present a comprehensive survey of existing research in this emerging field and its related tasks. We provide a task description, discuss the construction process of existing datasets, and analyze proposed models and approaches. Based on our findings, we identify intriguing challenges and outline potential future directions to advance the field.},
	publisher = {{arXiv}},
	author = {Vladika, Juraj and Matthes, Florian},
	urldate = {2024-06-27},
	date = {2023-05},
	doi = {10.48550/arXiv.2305.16859},
	keywords = {Computer Science - Computation and Language},
}

@misc{sadri_continuous_2022,
	title = {Continuous Active Learning Using Pretrained Transformers},
	url = {http://arxiv.org/abs/2208.06955},
	abstract = {Pre-trained and fine-tuned transformer models like {BERT} and T5 have improved the state of the art in ad-hoc retrieval and question-answering, but not as yet in high-recall information retrieval, where the objective is to retrieve substantially all relevant documents. We investigate whether the use of transformer-based models for reranking and/or featurization can improve the Baseline Model Implementation of the {TREC} Total Recall Track, which represents the current state of the art for high-recall information retrieval. We also introduce {CALBERT}, a model that can be used to continuously fine-tune a {BERT}-based model based on relevance feedback.},
	publisher = {{arXiv}},
	author = {Sadri, Nima and Cormack, Gordon V.},
	urldate = {2024-06-27},
	date = {2022-08},
	doi = {10.48550/arXiv.2208.06955},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Information Retrieval},
}

@article{briscoeConductReportingCitation2019,
	title = {Conduct and reporting of citation searching in Cochrane systematic reviews: A cross‐sectional study},
	volume = {11},
	url = {https://pmc.ncbi.nlm.nih.gov/articles/PMC7079050/},
	doi = {10.1002/jrsm.1355},
	shorttitle = {Conduct and reporting of citation searching in Cochrane systematic reviews},
	abstract = {The search for studies for a systematic review should be conducted systematically and reported transparently to facilitate reproduction. This study aimed to report on the conduct and reporting of backward citation searching (ie, checking reference ...},
	pages = {169},
	number = {2},
	journaltitle = {Research Synthesis Methods},
	author = {Briscoe, Simon and Bethel, Alison and Rogers, Morwenna},
	urldate = {2024-11-13},
	date = {2019-07-04},
	langid = {english},
	pmid = {31127978},
	file = {Full Text PDF:/home/aaronfletcher/snap/zotero-snap/common/Zotero/storage/VARWCXEA/Briscoe et al. - 2019 - Conduct and reporting of citation searching in Cochrane systematic reviews A cross‐sectional study.pdf:application/pdf},
}

@online{MECIRManualCochrane,
	title = {{MECIR} Manual {\textbar} Cochrane Community},
	url = {https://community.cochrane.org/mecir-manual},
	abstract = {Methodological Expectations of Cochrane Intervention Reviews ({MECIR}) Version August 2023},
	urldate = {2024-11-13},
	langid = {english},
	file = {Snapshot:/home/aaronfletcher/snap/zotero-snap/common/Zotero/storage/VL4W5MUM/mecir-manual.html:text/html},
}
