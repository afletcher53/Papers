\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{soul}
\usepackage[colorinlistoftodos]{todonotes}

\usetikzlibrary{shapes,arrows,positioning,calc}

% New packages for algorithms
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{makecell}
\title{Goldilocks V3}
\author{Aaron HA Fletcher}
\date{September 2024}

\begin{document}

\maketitle

\section{Introduction}

High-recall retrieval (HRR) tasks involve identifying all or most documents of interest within a large collection. HRR tasks are suitable where the need for high recall outweighs the need for precision, such as in cases of systematic review in medicine, electronic document discovery in Law or online content moderation. Technology-assisted reviews (TAR) are automated methods that reduce the total number of documents reviewed in HRR tasks. 

\begin{itemize}
\item SR in medicine, outline the process.
\item Motivations for using TAR for HRR tasks.
\item Motivation for using LLMs in TAR.

This paper adds to the current research on CAL TAR process by exploring if the ``Goldilocks problem'' exists within BiolinkBERT model.
\end{itemize}
\todo{Flesh out with information regarding SRs}
\section{Background}

A successful approach to TAR is a continuous active learning approach (CAL) in which an oracle, such as a human, is iteratively queried with documents from an unlabelled total pool (starting with a very small number). At each iteration, this labelled pool is then used to incrementally train a classifier to rank documents, and further documents are selected for Oracle labelling.  Once the stopping criteria have been met, the aim is to have most of the relevant documents labelled or ranked highly. 

There have been multiple approaches to using CAL for how, however, work within CAL for TAR can be delineated by the type of models used within the CAL process and how the document is represented within this process. Early work in CAL followed the AUTOTAR approach, in which a feature-based classifier, such as SVM or linear regression, which was paired with a saturated TF-IDF document representation, which is broadly outlined in Algorithm \ref{alg:autotar}. Despite being computationally inexpensive and achieving moderate sucess in recall, these approaches suffered primarily from limitations within the document representation, which was unable to capture the semantic meaning between terms within the document, ignoring long range dependencies and positional information.
With the development of transformer architecture, large language pre-trained models (LLMs) also found success within the CAL process. 


\begin{algorithm}
    \caption{The AUTOTAR CAL approach}
    \label{alg:autotar}
    \begin{algorithmic}[1]
    \State Find a relevant ``seed'' document using ad hoc search, or construct a synthetic relevant document from the topic description
    \State Initialize training set with the seed document from step 1, labeled ``relevant''
    \State Set initial batch size $B \gets 1$
    \Repeat
        \State Temporarily add 100 random documents from the collection to the training set, labeled ``not relevant''
        \State Train an SVM classifier using the training set
        \State Remove the temporary random documents added in step 5
        \State Select the $B$ highest-scoring documents for review
        \State Review the documents, coding each as ``relevant'' or ``not relevant''
        \State Add the reviewed documents to the training set
        \State $B \gets B + B/10$
    \Until{sufficient number of relevant documents have been reviewed}
    \end{algorithmic}
    \end{algorithm}

Adapting LLMs on downstream tasks utilises transfer learning. This is where a model is pretrained using a pretraining objective, such as masked-language modelling (MLM) and then that model is adapted by being fine-tuned on a downstream task. The process of pre-training a large language model can be succinctly defined as training a model using a corpora and minimising loss generated from a pre-training objective (i.e. MLM or next sentence prediction). This pre-trained model is typically adapted through using more domain adaptation (further pretraining (FPT) on a task-specific corpora) and/or through changing the pre-training objective, such as in the CAL process to binary classification.

Early encoder-only architectures, such as BERT, offered an incremental improvement for the CAL process compared with the AUTOTAR approach, albeit with a trade-off in computational cost. In this approach, the feature-based classifier was replaced with the BERT model, which was FPT on a task-specific corpora, and fine-tuned for classification. This approach represents documents through a self-attention mechanism, capturing semantic meaning between terms and long-range dependencies. The general approach is outlined in Algorithm \ref{alg:bert-tar}.

\begin{algorithm}
    \caption{The BERT-based CAL approach}
    \label{alg:bert-tar}
    \begin{algorithmic}[1]
    \State Fine-tune BERT language model on the full document collection using MLM
    \State Find a single random relevant document as the seed
    \State Initialize training set with the seed document from step 2, labeled "relevant"
    \Repeat
    \State Fine-tune BERT for classification using all labeled documents
    \State Use fine-tuned BERT to score all unlabeled documents
    \State Select 25 documents using active learning strategy (relevance feedback or uncertainty sampling)
    \State Review the selected documents, labeling each as "relevant" or "not relevant"
    \State Add the newly labeled documents to the training set
    \Until{20 iterations completed or other stopping criterion met}
    \State Use final fine-tuned BERT model to rank remaining unlabeled documents
    \end{algorithmic}
    \end{algorithm}

Initially, adapting BERT to the TAR process was difficult, often with it performing on par with the AUTOTAR approach \hl{Continuous active learning using Pretrained transformers}. It was found that BERT-based TAR performed better being FPT on the unlabelled collection \hl{effectively adapting pretrained lm for AL} and with a specific number of FPT epochs, with the optimal number being dependent on the dataset, aka the ``Goldilocks problem''. It was termed Goldilocks because FPT was necessary, yet excessive FPT can occur. Interestingly, this ``Goldilocks'' stopping further pretraining epoch was not the same for all datasets, and not even consistent across reproducibility studies within the same dataset. In some datasets, such as CLEF, it didn't exist. Explanations for why this might be the case was not explored, however, it could be suggested that FPT instability could be a contributing factor.

Consistent findings across research using BERT for CAL was that the pretraining domain corpora aligning with the downstream task for the recall task was important to producing better classifiers. Indeed, the Reproducability paper found that using the pretrained BioLinkBERT over BioBERT resulted in better CAL performance on the CLEF dataset. BioBERT's is initialised from existing BERT weights (which copora contained English Wikipedia and BookCorpus), and then further pre-trained and Pubmed Abstracts and PMC full-text articles. BiolinkBERT in contrast is initialised from PubMeBert existing weights (which pretraining corpora contained only PubMed abstracts and PMC full-text articles) and leverages hyperlink information to improve the semantic meaning of the model. A salient difference between the two approaches (FPT from BERT existing weights vs FPT from domain-specific corpora weights) is that the vocabulary of FPT from domain-specific corpora matches more closely with the downstream task vocabulary, which is not the case for FPT from BERT existing weights.

The Reproducability paper's main relevant findings to this work were:
\begin{itemize}
    \item Goldilocks problem wasn't apparent within BioBERT model on the CLEF dataset. 
    \item Using a more domained aligned pretrained model (BioLinkBERT) resulted in superior performance to a less aligned pretrained model (BioBERT) with domain-specific FPT.
\end{itemize}

The work did not explore if the ``Goldilocks problem'' exists within these more domain aligned pretrained models, or to put it another way, if superior performance could be achieved within these domain aligned models through FPT. This work aims to address this gap in research. \todo{This is the research gap from the previous work}

LLMs with different pretraining objectives have been found to perform better in classification tasks. So far, work has utilised MLM, where the typically 15\% of tokens are masked during pre-training, with the model being trained to predict these masked tokens. Recent work, ELECTRA, has explored using a corrupted token objective, where the model is trained to predict if tokens within a sequence are corrupted or not. Corrupted token prediction (CTP) pre-training objective outperforms MLM in classification tasks. To date, no work has utilised CTP in the CAL process. \todo{This is the section half of the paper, and the most novel part of the work}.
\todo{Mention about features which make BERT CAL performance better}

\section{Datasets}

This research focuses on CLEF datasets and Synergy Dataset. \todo{No one has used the Synergy Dataset for TAR before}. 

\section{Research Questions}

\begin{enumerate}
    \item \textit{Is the "Goldilocks problem" apparent on the BioLinkBERT model using the CLEF dataset?}
    \item \textit{Does using a model trained using a corrupted token pretraining objective result in superior CAL performance than the masked-language modelling objected on the CLEF dataset?}
    \item \textit{Are there features within the CLEF dataset that correlate with performance?}
    
\end{enumerate}

\section{Experimental Setup}

\section{Hardware and Hyperparameters}

All experiments were conducted on the University of Sheffield's High Performance Computing cluster. A100 GPUs with 80GB of memory were used for all experiments using pretrined LLM models, and baseline experiments were conducted on a single CPU. Hyperparameters
were taken from the Reproducability paper

\begin{itemize}
    \item Batch size: 25
    \item Further pretraining on unlabelled collection.
    \item Total of 20 iterations of active learning loop (501 total oracle reveals)
    \item Total of 20 fine-tune epochs per active learning loop.
\end{itemize}

\section{Evaluation Metrics}

Recall-precision (R-precision) was reported for all experiments. R-precision determines the percentage of relevant documents that are ranked in the top $k$ documents, where $k$ is the total number of relevant documents in the collection. R-precision reported is the R-precision as calculated after the final active learning iteration (iteration 20). Within systematic reviews, a R-precision of 0.95 (95\%) is considered sufficient.

Unlike the reproducibility paper which used paired t-tests with bonferroni correction per dataset and selection policy, statistical significance of the differences between the FPT epoch and R-precision was calculated using Friedman test over all datasets grouped by selection policy. Since the goal is to find differences between treatment groups (i.e. FPT epochs), the Friedman test is more appropriate for comparing multiple groups simultaneously, and bonferroni correction was not required.

\section{Results}
\subsection{RQ1}

Using the BiolinkBERT large model surpassed the performance of the BiolinkBERT base model 7 out of 12 times. The Friedman test for individual datasets found significant differences between the FPT epochs 4 out of 12 times, however, when considering all datasets together, there was no significant difference between the FPT epochs and R-precision for relancy selection policy or uncertainty selection policy. This indicates that the ``Goldilocks problem'' is not apparent within the large BiolinkBERT model for the CLEF dataset, and that further pretraining does not produce an statistically significant improvement in R-precision. The average R-precision of each FPT epoch is reported in Table \ref{tab:results-average}, with the highest R-precision for relevancy selection policy being 0.847 at FPT ep2 and the highest R-precision for uncertainty being  0.832 at FPT ep1. 

\begin{table}[htbp]
    \centering
    \footnotesize
    \setlength{\tabcolsep}{4pt}
    \begin{tabular}{l>{\raggedright\arraybackslash}p{1.2cm}ccccc}
    \hline
    \textbf{Collection} & \textbf{Dataset size} & \textbf{Model} & \multicolumn{2}{c}{\textbf{R-Precision (↑)}} & \multicolumn{2}{c}{\textbf{Friedman (p)}} \\
    \cline{4-7}
    & & & \textbf{Rel.} & \textbf{Unc.} & \textbf{Rel.} & \textbf{Unc.} \\
    \hline
    \multirow{6}{*}{\makecell[l]{Clef 2019\\dta test}} & 
    \multirow{6}{*}{8} & BiolinkBert-Base-ep0 & \textbf{0.909} & \textbf{0.857} & \multicolumn{2}{c}{---} \\
    & & BiolinkBert-Large-ep0 & 0.897 & 0.803 & \multirow{5}{*}{0.914} & \multirow{5}{*}{0.632} \\
    & & BiolinkBert-Large-ep1 & 0.827 & 0.832 & & \\
    & & BiolinkBert-Large-ep2 & 0.812 & 0.774 & & \\
    & & BiolinkBert-Large-ep5 & 0.841 & 0.814 & & \\
    & & BiolinkBert-Large-ep10 & 0.881 & 0.846 & & \\
    \hline
    \multirow{6}{*}{\makecell[l]{Clef 2017\\test}} & 
    \multirow{6}{*}{30} & BiolinkBert-Base-ep0 & 0.812 & 0.794 & \multicolumn{2}{c}{---} \\
    & & BiolinkBert-Large-ep0 & 0.828 & 0.797 & \multirow{5}{*}{\textbf{\textless0.05}} & \multirow{5}{*}{\textbf{\textless0.05}} \\
    & & BiolinkBert-Large-ep1 & 0.826 & \textbf{0.827} & & \\
    & & BiolinkBert-Large-ep2 & \textbf{0.858} & 0.804 & & \\
    & & BiolinkBert-Large-ep5 & 0.827 & 0.777 & & \\
    & & BiolinkBert-Large-ep10 & 0.799 & 0.757 & & \\
    \hline
    \multirow{6}{*}{\makecell[l]{Clef 2017\\train}} & 
    \multirow{6}{*}{20} & BiolinkBert-Base-ep0 & \textbf{0.838} & 0.761 & \multicolumn{2}{c}{---} \\
    & & BiolinkBert-Large-ep0 & 0.778 & 0.765 & \multirow{5}{*}{\textbf{\textless0.05}} & \multirow{5}{*}{0.28} \\
    & & BiolinkBert-Large-ep1 & 0.808 & 0.789 & & \\
    & & BiolinkBert-Large-ep2 & 0.767 & 0.701 & & \\
    & & BiolinkBert-Large-ep5 & 0.816 & 0.786 & & \\
    & & BiolinkBert-Large-ep10 & 0.827 & \textbf{0.796} & & \\
    \hline
    \multirow{6}{*}{\makecell[l]{Clef 2018\\test}} & 
    \multirow{6}{*}{30} & BiolinkBert-Base-ep0 & 0.794 & 0.780 & \multicolumn{2}{c}{---} \\
    & & BiolinkBert-Large-ep0 & 0.789 & 0.774 & \multirow{5}{*}{0.52} & \multirow{5}{*}{0.50} \\
    & & BiolinkBert-Large-ep1 & \textbf{0.812} & 0.790 & & \\
    & & BiolinkBert-Large-ep2 & 0.797 & \textbf{0.791} & & \\
    & & BiolinkBert-Large-ep5 & 0.763 & 0.773 & & \\
    & & BiolinkBert-Large-ep10 & 0.763 & 0.769 & & \\
    \hline
    \multirow{6}{*}{\makecell[l]{Clef 2019\\DTA int.\\train}} & 
    \multirow{6}{*}{20} & BiolinkBert-Base-ep0 & 0.939 & 0.923 & \multicolumn{2}{c}{---} \\
    & & BiolinkBert-Large-ep0 & 0.939 & 0.902 & \multirow{5}{*}{0.78} & \multirow{5}{*}{0.50} \\
    & & BiolinkBert-Large-ep1 & 0.941 & 0.935 & & \\
    & & BiolinkBert-Large-ep2 & 0.948 & 0.921 & & \\
    & & BiolinkBert-Large-ep5 & 0.952 & 0.945 & & \\
    & & BiolinkBert-Large-ep10 & \textbf{0.945} & \textbf{0.947} & & \\
    \hline
    \multirow{6}{*}{\makecell[l]{Clef 2019\\DTA int.\\test}} & 
    \multirow{6}{*}{20} & BiolinkBert-Base-ep0 & \textbf{0.934} & \textbf{0.900} & \multicolumn{2}{c}{---} \\
    & & BiolinkBert-Large-ep0 & 0.899 & 0.856 & \multirow{5}{*}{0.87} & \multirow{5}{*}{\textbf{\textless0.05}} \\
    & & BiolinkBert-Large-ep1 & 0.904 & 0.840 & & \\
    & & BiolinkBert-Large-ep2 & 0.909 & 0.878 & & \\
    & & BiolinkBert-Large-ep5 & 0.882 & 0.835 & & \\
    & & BiolinkBert-Large-ep10 & 0.865 & 0.841 & & \\
    \hline
   
    \end{tabular}
    \caption{Performance comparison across different collections and models}
    \label{tab:results}
\end{table}

\begin{table}[htbp]
    \centering
    \caption{Average R-precision of each FPT epoch for CLEF dataset}
    \begin{tabular}{l>{\raggedright\arraybackslash}p{1.2cm}ccccc}
        \textbf{Policy} & \textbf{ep0} & \textbf{ep1} & \textbf{ep2} & \textbf{ep5} & \textbf{ep10} \\
        \hline
        Uncertainty & 0.813 & 0.832 & 0.813 & 0.815 & 0.814 \\
        Relevancy & 0.840 & 0.845 & 0.847 & 0.842 & 0.835 \\
    \hline

    \end{tabular}
    \label{tab:results-average}
\end{table}
\end{document}
