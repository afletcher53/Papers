\documentclass[10pt,oneside]{book}

%%%%%%%%%%%%%%%%%
% import packages %
%%%%%%%%%%%%%%%%%
\usepackage{lipsum}
\usepackage{lmodern}
\usepackage{ragged2e}
\usepackage{adjustbox}
\usepackage{array}
\usepackage{rotating}
\usepackage{array}
\usepackage{setspace}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{tcolorbox}
\usepackage{graphicx} 
\usepackage{fancybox} 
\usepackage{soul,color}
\usepackage{amsmath}
\usepackage{pdfpages}
\usepackage{tocloft}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{titlesec}
\usepackage{titletoc}
\usepackage{tabularx}
\usepackage{hyperref}
\usepackage{epigraph}
\usepackage{pdfpages}
\usepackage{pdflscape}
\usepackage{afterpage}
\usepackage{tablefootnote}
\usepackage{rotating}
\usepackage{pgfgantt}
\usepackage[acronyms]{glossaries}
\usepackage[
layout=letterpaper, 
paper=letterpaper, 
portrait, 
head=0.5in,
foot=0.5in,
top=.75in, 
bottom=.75in, 
left=0.75in, 
right=0.75in, 
margin=1in
]{geometry}
\usepackage[style=numeric-comp,backend=biber,backref=true,sorting=none]{biblatex}
\usepackage{cleveref}
\usepackage[toc,page]{appendix}
\usepackage{subfiles} % best loaded last in the preamble

% Custom column type for ragged-right with a fixed width
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}


%Acronyms
\newacronym{tar}{TAR}{Technology-Assisted Review}
\newacronym{cal}{CAL}{Continuous Active Learning}
\newacronym{llm}{LLM}{Large Language Model}
\newacronym{gnn}{GNN}{Graph Neural Network}
\newacronym{gcn}{GCN}{Graph Convolutional Network}
\newacronym{gats}{GATs}{Graph Attention Networks}
\newacronym{bcs}{BCS}{Backwards Citation Searching}
\newacronym{fcs}{FCS}{Forwards Citation Searching}
\glsaddall
\makeglossaries

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % Other commands
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\bibliographyLSR}[1]{\bibliography{#1}}
\newcommand{\ts}{\textsuperscript} %sobrescrito

\onehalfspacing

\AtBeginBibliography{\small}

\usetikzlibrary{shapes, arrows, positioning, fit,backgrounds}
\usetikzlibrary{arrows.meta}

\hypersetup{
colorlinks=true,
urlcolor=blue,
linkcolor=blue,
citecolor=blue,
pdftitle={@title - @author},
pdfsubject={Systematic Review},
pdfauthor={@author},
pdfkeywords={list; here; your; keywords (key words)}
}

\addbibresource{zot_references.bib}
\setlength{\cftsecnumwidth}{2.5em}
\setcounter{tocdepth}{1}
\begin{document}

\pagenumbering{arabic}

\begin{titlepage}


% -------------------------------------------------------------------
% You need to edit the details here
% -------------------------------------------------------------------

\begin{center}
{\LARGE University of Sheffield}\\[1.5cm]
\linespread{1.2}\huge {\bfseries Efficient Screening in Medical Systematic Reviews}\\[1.5cm]
\linespread{1}
\includegraphics[width=5cm]{images/tuoslogo.png}\\[1cm]
{\Large Dr. Aaron HA Fletcher}\\[1cm]
% {\large \emph{Supervisor:} Dr. Mark Stevenson}\\[1cm] % if applicable
\large A report submitted in partial fulfilment of the requirements\\ for the degree of PhD in Computer Science\\[0.3cm] 
\textit{in the}\\[0.3cm]
Department of Computer Science\\[2cm]
\today
\end{center}

\end{titlepage}

% -------------------------------------------------------------------
% Declaration
% -------------------------------------------------------------------

\newpage
\section*{\Large Declaration}

All sentences or passages quoted in this document from other people's work have been specifically acknowledged by clear cross-referencing to author, work and page(s).  Any illustrations that are not the work of the author of this report have been used with the explicit permission of the originator and are specifically acknowledged.  I understand that failure to do this amounts to plagiarism and will be considered grounds for failure.\\[1cm]

\noindent Name: Aaron HA Fletcher\\[1mm]
\rule[1em]{25em}{0.5pt}

\noindent Signature:\\[1mm]
\rule[1em]{25em}{0.5pt}

\noindent Date:\\[1mm]
\rule[1em]{25em}{0.5pt}



%%%%%%%%%%%%
% Abstract %
%%%%%%%%%%%%
%% remove spacing from abstract

\chapter*{\Large \center Abstract}

% Guidance of how to write an abstract/summary provided by Nature: https://cbs.umn.edu/sites/cbs.umn.edu/files/public/downloads/Annotated_Nature_abstract.pdf


Systematic review screening is costly and repetitive and is a major roadblock to the timely creation of evidence. Combining this approach with rapidly growing medical literature is not scaleable. \gls*{tar} and \gls*{cal} have promised to reduce screening costs. However, \gls*{cal} approaches emulate a human-centric approach, which was designed to address a need for near-total document recall. The project addresses the need for higher efficiency and near-total recall by approaching screening differently. First, it proposes combining citation network analysis with \gls*{cal} to prioritise relevant documents early. This includes exploring backwards and forward citation searches for seed document augmentation. Second, the review outlines the incorporation of document metadata — such as publication year and authorship — into \gls*{gnn} to capture richer inter-document relationships. By augmenting standard text-based methods with structured metadata and adjacency information, the approach aims to identify relevant documents sooner. Third, it explores novel stopping criteria aligned with user-defined utility metrics, moving beyond fixed recall thresholds to measure how newly discovered evidence alters clinical conclusions. The research will use datasets like CLEF-TAR and Synergy, employing widely adopted metrics including R-Precision, Work Saved over Sampling (WSS@k), recall@95\% and Percentage Relevant Documents. Ultimately, this research aims to reduce screening costs without jeopardising completeness, offering robust methods to systematically identify the most critical evidence in large medical corpora. It aims to enable more timely, resource-effective, and reliable systematic reviews.
    \smallskip
    \smallskip
    
    \textbf{Keywords} -  Systematic Reviews, Stopping Algorithms, Continuous Active Learning, Technology-Assisted Review, Utility, Medical Literature Screening, Evidence-based medicine, Metadata analysis
\newpage
\begingroup % Localize font size change
\small % Or \scriptsize or \tiny
\vspace*{-3cm}
\tableofcontents

\endgroup
% \listoffigures
% \listoftables
\newpage
\newcommand{\lightshadowbox}[1]{%
  \setlength{\fboxsep}{6pt}%
  \setlength{\shadowsize}{1pt}%
  \shadowbox{#1}%
}


\newpage
%%%%%%%%%%%
% Article %
%%%%%%%%%%%
\chapter{Executive Overview}

\epigraph{A man is known by the company he keeps}{Aesop}

\section{Background}
Systematic reviews are a cornerstone for evidence-based medicine, providing rigorous, high-level research syntheses to guide clinical practice and policy. Yet this rigour carries a steep cost: systematic reviews are labour-intensive, often requiring researchers to screen thousands of documents manually. The problem this research aims to improve is efficiency in this process (i.e., the amount of documents screened). \gls*{tar} methods, particularly \gls*{cal}, promise to alleviate some of this burden; however, many existing approaches still treat documents as isolated entities, overlooking inter-document relationships or disregarding previously recalled documents' contribution to a search goal.

Better document representation can solve this problem. Just as an individual's quality might be inferred from their associations, the true value of a research document often lies not only in its isolated content but also in its connections to the broader scholarly landscape. Manual screening, and even many \gls*{cal} approaches, shallowly represent documents. That is to say, they use only information sourced from the title and abstract despite many more signals being available (inter-document connections), such as citation networks or metadata (e.g., source of publication, authorship, date of publication). These signals, used instinctively by human operators, could steer the selection for screening towards high-value studies. Moreover, the conventional practice of stopping screening relies on a simplistic, binary categorisation of documents as relevant or not, triggering the stop after a fixed ``target recall" threshold is met. This shallow document representation fails to recognise that documents contribute variably to a review's conclusions. While it provides a seemingly objective benchmark, target recall fails to consider the information's utility. Whether the accumulated evidence sufficiently addresses (i.e., how the information contained within a document relates to other documents) the research question or whether further screening will likely produce diminishing returns remains unexamined. 

This PhD, therefore, seeks more efficient and information-centric strategies to automate systematic review screening. First, this research will investigate relationship-aware \gls*{cal} methods that harness inter-document connections to rank and prioritise more intelligently for screening. Second, this research will examine the value of intra-document information, creating utility-driven stopping criteria that shift the focus from static recall targets to the actual value of newly discovered evidence. Rather than screen indiscriminately to meet a blanket recall requirement, the process stops once the reviewed studies collectively offer a robust answer to the research question. By utilising existing inter-document information or aligning the screening endpoint with genuine evidence needs, the resource demands of systematic reviews can be reduced while maintaining quality.

% \begin{enumerate}
% \item \textbf{Citation-Aware Prioritisation:} 
%     Incorporate backward and forward citation searches into CAL, accelerating the identification of key articles and enriching early training signals.

% \item \textbf{Graph Neural Networks with Metadata aware prioritisation:} 
%     Investigate how Graph Neural Networks can exploit structured metadata—publication year, co-authorship, journal, or citation counts — to screen relevant documents more effectively.

% \item \textbf{Utility-Driven Stopping Criteria:}
%     Move beyond blanket recall thresholds toward methods that measure newly discovered evidence's utility. Rather than screening to an arbitrary recall target, the process can pause once the resulting evidence set adequately addresses the review's objectives.
% \end{enumerate}

\section{Objectives and research questions}

The overarching goal is to reduce the manual screening workload (e.g., reduced Work Saved over Sampling, Percentage of Relevant Documents) while maintaining key performance measures (e.g., high Recall@k, R-Precision).

To address this goal, both document screening prioritisation and stopping methods must be considered (see Sections~\ref{sec:screening Priorisation}, \ref{sec:metadata}, and \ref{sec:Stopping_algorithms}). Specifically, this PhD proposes improving the representation of documents in \gls*{cal} and aligning stopping methods with user information needs (see Section~\ref{sec:Project_Overview}). Three core research questions guide the work:

\begin{enumerate} 

\item \textbf{RQ 1}: \emph{What is the impact of integrating backwards and forward citation network analysis on the performance of \gls*{cal} models for medical systematic review screening?}
\item \textbf{RQ 2}: \emph{How does incorporating \gls*{gnn} architectures that utilize article metadata (e.g., publication date, author information, citation counts) affect the efficiency and accuracy of document prioritisation in \gls*{cal}, compared to traditional text-only representations?}
\item \textbf{RQ 3}: \emph{How do stopping criteria based on the utility of retrieved information (aligned with user information needs) influence the total manual screening workload in \gls*{cal}, and how do these criteria compare with conventional recall-based stopping rules?}

\end{enumerate}

These questions will be addressed using publicly available datasets such as CLEF-TAR and Synergy, enriched with metadata from resources like OpenAlex (see Section~\ref{sec:datasets}). Experiments will employ existing approaches (e.g., Encoder-CAL, Baseline Model implementations) and introduce novel methods (e.g., utility-based stopping algorithms that factor in the implications of recalled information; see Section~\ref{sec:research_proposals}).

Ultimately, this PhD aims to develop methods that reduce the screening demands of systematic reviews. By combining better document representations through metadata or citation-aware insights and flexible stopping rules, the research seeks to make evidence synthesis more resource-efficient and timely in an era of exploding medical literature.

\chapter{Literature Review}

\section{Systematic reviews}

Systematic reviews provide a rigorous and transparent approach to synthesising research evidence on a specific topic, forming a cornerstone of evidence-based practice. Unlike traditional narrative reviews, systematic reviews adhere to a pre-defined, systematic methodology that minimises bias and ensures reproducibility. This methodical approach encompasses comprehensive literature searches, critical appraisal of studies, and careful synthesis of findings, ultimately providing a reliable and comprehensive overview of the current state of knowledge.

The term \emph{systematic} underscores the methodical and structured approach used throughout the review process. While the precise execution varies depending on the type of review and research question, several core characteristics define this methodology:

\begin{itemize}
    \item {\bf{Explicit, Step-by-Step Methodology:}} systematic reviews follow a clearly defined, step-by-step methodology, often documented in comprehensive procedural manuals. In medicine, for instance, the Cochrane Collaboration's Review Manual \cite{lefebvre_cochrane_2011} provides a detailed guide to conducting Cochrane Reviews. Similarly, in qualitative research, Noblit and Hare's book on meta-ethnography \cite{noblit_meta-ethnography_1988} outlines procedures for synthesising qualitative studies. These manuals provide a framework for standardised, transparent reviews.
    \item {\bf{Pre-Established Review Protocol:}} systematic reviews require the development of a detailed protocol {\it{before}} the review commences. This protocol acts as a blueprint, outlining the review's objectives, search strategy, inclusion and exclusion criteria, data extraction methods, and analysis plan. The pre-defined protocol prevents arbitrary changes in the review's scope during the process and minimises duplication of effort among reviewers. Furthermore, many reviewers register their protocols in publicly accessible registries like PROSPERO\footnote{https://www.crd.york.ac.uk/prospero/}, or open-access platforms, enhancing transparency and accountability \cite{tawfik_protocol_2020}.
    \item {\bf{Reporting Guidelines:}} systematic reviews adhere to established reporting guidelines to ensure clarity and consistency. These guidelines specify the essential elements to include in the final review report \cite{moher_preferred_2010}. Numerous reporting guidelines are available, tailored to various review types and research designs. Resources like the EQUATOR Network\footnote{https://www.equator-network.org/} compile and offer these guidelines, promoting standardised reporting across different fields.
    
    \item {\bf{Quality Assessment Checklist:}} The checklist provides an assessment of the methodological quality (risk of bias) of included studies, aiding the reader in interpreting the overall credibility of the review's findings. This allows the reader to answer the question, ``Is this a good or bad example of a review?". This process typically uses validated quality assessment checklists or tools appropriate for the study designs under review \cite{whiting_proposed_2017}.
    \item {\bf{Methodological Surveys or Audits:}} Methodological surveys or audits evaluate how emerging review types (e.g., scoping or umbrella reviews) are conducted, often signalling the need for refined reporting standards. They can occur before formal guidelines are established or continuously e.g., \cite{dalton_potential_2017, france_methodological_2014}. By aggregating practices from published reviews, they identify methodological inconsistencies (e.g., misuse of meta-analysis for heterogeneous data) or innovations. Such audits signal when a review type gains traction and inform consensus on best practices, often catalysing reporting standards (e.g., PRISMA extensions) or warnings against flawed approaches \cite{tricco_prisma_2018, sarkis-onofre_how_2021,rethlefsen_prisma-s_2021, rethlefsen_prisma-s_2021-1}
\end{itemize}

While this document explores the methodologies and challenges of systematic reviews, its literature review, constrained by the typical limitations of a PhD project, cannot be a full systematic review because systematic reviews are resource-intensive, often requiring dedicated funding, a multi-year timeframe, and a multidisciplinary team.
\subsection{In medicine}

% systematic review use in medicine.
Central to medical research and practice, systematic reviews synthesise existing literature on clearly framed clinical questions \cite{kranke_evidence-based_2010, higgins_cochrane_2019}. They consolidate extensive evidence into a cohesive, critically appraised summary that offers clear decision support for clinicians and policymakers. systematic reviews are created across virtually every medical subspecialty. Recent publications illustrate their scope, exploring cardiovascular risks in endometriosis \cite{parsa_endometriosis_2025}, clarifying links between antidepressants and insomnia in adolescents \cite{turkmen_systematic_2025}, and examining anaemia in pregnancy \cite{azzam_anemia_2025}. The exponential growth of new studies necessitates reliance on systematic syntheses for reliable, up-to-date guidance.


% How systematic reviews support decision making
systematic reviews provide the highest-ranked evidence for many clinical questions in evidence-based medicine (EBM) underpinning clinical decision-making. As shown in Table \ref{tab:evidence_levels}, EBM organises evidence types hierarchically, ranking expert opinion as the weakest (level 5) and systematic reviews of randomised controlled trials (RCTs) as the strongest (level 1a). To minimise bias, systematic reviews, unlike traditional narrative reviews, follow strict methodological guidelines such as comprehensive literature searches and critical appraisal of each included study. This rigorous approach ensures a transparent and reproducible evidence synthesis, enabling practitioners to make more informed decisions tailored to patient needs.

\input{tables/oxford_hier}

% Growth of systematic reviews
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{images/increase_in_publications.jpg}
    \caption{Increasing publications over that past two decades \cite{ghasemi_scientific_2022}.}
    \label{fig:increasing_publications_over_time}
\end{figure}

The publication rate of systematic reviews in medicine has surged exponentially in recent decades, a trend reflecting their growing centrality to evidence-based practice. Between 1986 and 2015, PubMed indexed a 2,728\% increase in articles labelled as systematic reviews, far outpacing the 153\% rise in other publications during the same period \cite{ioannidis_mass_2016}. This expansion underscores a growing need for consolidated, high-quality evidence to address diverse clinical questions, especially as the volume of primary research increases. For instance, the number of clinical trials—a key source of evidence—doubled between 2001 and 2020, while peer-reviewed journals nearly tripled \cite{ghasemi_scientific_2022}; see Figure \ref{fig:increasing_publications_over_time}). Methodological advances, such as PRISMA guidelines and GRADE frameworks, have strengthened systematic review rigour, enabling them to shape clinical guidelines, inform policy, and even synthesise overlapping evidence through umbrella reviews. Cochrane exemplifies this impact: its 9,000+ systematic reviews, accessed over 17.5 million times by 2023, underscore its global reach and influence, reflected in a journal impact factor of 8.8\footnote{https://www.cochrane.org/about-us/scientific-strategy}.


\subsection{The process}

% Brief overview of the Cochrane Review process and its characteristics.
Although no single universally correct method exists for conducting a medical systematic review, with various approaches available (e.g., Joanna Briggs Institute review \cite{santos_joanna_2018}, Network Meta-Analysis \cite{bafeta_reporting_2014}, or meta-analysis\cite{moher_improving_1999}) \cite{munn_what_2018}, the Cochrane Review is a commonly used process \cite{cipriani_what_2011}. The Cochrane Review framework is particularly relevant to this PhD because each step is meticulously documented, enabling potential automation.


\begin{itemize}
    \item {\bf{Standardised Methodology:}} systematic reviews often use a PICO format (Patient, Intervention, Comparison, Outcome). All reviews follow strict guidelines in the Cochrane Handbook, ensuring consistency. This process minimises bias through methods like dual study selection and data extraction by multiple authors.
    \item {\bf{Mandatory Protocol Registration:}} Cochrane Reviews require protocol registration before the review commences, reducing bias from post-hoc changes \cite{cumpston_chapter_2024}.
    \item {\bf{Regular Updates:}} Cochrane systematic reviews are updated periodically to reflect new evidence.
    \item {\bf{GRADE framework for evidence certainty:}} Uses a system (high, moderate, low, very low) to rate the certainty of evidence in the summary of findings table \cite{guyatt_grade_2008}.
\end{itemize}


% Focus on the search process within Cochrane reviews.

Information Retrieval (IR) — often led by medical and healthcare librarians or information specialists (IR specialists) — plays a vital role in producing high-quality Cochrane Reviews. Their IR expertise ensures the comprehensiveness and rigour of the systematic review process. Extensive research confirms the positive impact of IR specialists' involvement in systematic reviews, highlighting their contributions to both the search process \cite{le_benchmarking_2023, brunskill_case_2022, schvaneveldt_assessing_2021} and overall search quality \cite{giroudon_qualite_2023, pawliuk_librarian_2024, ramirez_adherence_2022}. Collaboration between IR specialists and review authors spans the early planning stages to the final write-up and subsequent updates.

IR specialists perform a wide range of tasks, including:

\begin{itemize}
\item {\bf{Source Selection:}} Advising authors on selecting appropriate databases and other sources tailored to the specific review topic.
\item {\bf{Search Strategy Development:}} Designing or guiding the development of comprehensive and sensitive search strategies for major bibliographic databases and trial registers.
\item {\bf{Search Execution:}} Running searches in databases and trial registers accessible to the IR specialist.
\item {\bf{Results Management:}} Saving, collating, and sharing search results with authors in suitable formats, including de-duplicating records.
\item {\bf{Protocol and Review Development:}} Drafting or assisting in drafting the search methods sections of Cochrane protocols, reviews, and updates.
\item {\bf{Methodological Compliance:}} Ensuring that Cochrane protocols, reviews, and updates adhere to the Methodological Expectations of Cochrane Intervention Reviews (MECIR) related to searching activities.
\item {\bf{Translation Organisation:}} Facilitating the translation or data extraction of studies reported in languages other than English, enabling assessment of these reports for inclusion or exclusion.
\item {\bf{Document Retrieval:}} Obtaining copies of trial reports for review teams when needed.
\item {\bf{Formatting and Style:}} Checking and formatting the references to included and excluded studies according to the Cochrane Style Manual.
\end{itemize}

Because IR specialists undertake a wide range of tasks, understanding the structure of the systematic review process is crucial. The systematic review process consists of five phases, as shown in Table \ref{tab:stages_of_systematic review}. Because IR specialists' expertise is essential for Stage 2 (Identifying relevant work), this PhD research focuses on optimising processes within this stage. This stage can be further granularised into several substages, many of which directly involve the tasks outlined above:

\begin{itemize}
\item {\bf{Inclusion/exclusion criteria generation:}} Defining the precise criteria for including or excluding studies, supported by IR specialist guidance on source selection and search strategy development.
\item {\bf{Search Strategy Development:}} Creating the detailed search algorithms used to query databases (a core IR specialist responsibility).
\item {\bf{Database Searching:}} Implementing the search strategy across various databases and trial registers (conducted by IR specialists).
\item {\bf{Protocol Writing:}} Documenting the search methodology in detail, with IR specialists contributing the search methods section.
\item {\bf{Title and Abstract Screening:}} Initially assessing study relevance based on titles and abstracts, facilitated by IR specialist results management and de-duplication.
\item {\bf{Full-Text Download and Screening:}} Obtaining and reviewing the full text of potentially relevant studies supported by IR specialist document retrieval.
\item {\bf{Manual Search:}} Supplementing database searches with manual searching of relevant journals, conference proceedings, and other sources, often guided by IR specialists.
\end{itemize}



\begin{table*}[t]
\centering
\small
\begin{tabular}{|c|p{0.85\textwidth}|}
\hline
\textbf{Stage} & \textbf{Purpose} \\
\hline
1 & \textbf{Framing questions for a review:} The research question is structured and explicitly formulated. \\\hline
2 & \textbf{Identifying relevant work:} A wide range of databases are searched to identify research to be included. Potential research is first identified, screened, eligibility checked, and then a decision is made on the inclusion of that research \cite{tawfik_step_2019}. \\\hline
3 & \textbf{Assessing the quality of studies:} Research is tested for quality, such as minimum research design, and subjected to higher quality assessment checks, including tests for research heterogeneity. \\\hline
4 & \textbf{Summarizing the evidence:} Data synthesis occurs with tabulation of study characteristics and quality. Statistical testing is performed at this stage. \\\hline
5 & \textbf{Interpreting the findings:} Any issues highlighted in the previous steps should be addressed. Generate recommendations guided by reference to the strength of the evidence. \\
\hline
\end{tabular}
\caption{Stages of a Systematic Review.}
\label{tab:stages_of_systematic review}
\end{table*}



% Focus on the title and abstract screening substage.
This PhD research focuses on the Title and Abstract screening substage of the `Identifying Relevant Work' phase, a critical process often visualised using the PRISMA flow diagram (Figure \ref{fig:prisma_flow}). This stage alone is estimated  to account for 10-20\% of the time it takes to plan and conduct a systematic review \cite{haddaway_predicting_2019}.

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{Confirmation Review//images/prisma_flow.png}
    \caption{PRISMA flow diagram for a simulated Cochrane systematic review \cite{tawfik_protocol_2020}. Note the different substages of the identification of relevant work in blue.}
    \label{fig:prisma_flow}
\end{figure}

The identification stage compiles all potential studies from searched databases into a large dataset $T$ of studies that may or may not be relevant to the systematic review. Screening further refines this pool by removing duplicates and assessing titles and abstracts to reduce the number of full-text articles requiring assessment in the eligibility substage. Traditionally, 2-3 reviewers manually evaluate titles and abstracts to determine whether each study should be included or excluded based on predetermined criteria. At this stage, studies are typically categorised as relevant (related to the systematic review) or not relevant (unrelated to the systematic review).

% Research within the title and abstract screening substage

Best practices have been outlined to increase consistency and reduce bias in the title and abstract screening phase \cite{polanin_best_2019}. At the start of this stage, using an abstract screening tool with clear, concise questions and conducting training sessions where screeners learn and pilot test the screening process using 20-30 abstracts is recommended to ensure intercoder agreement. During the screening phase, teams should hold weekly meetings, avoid changes to the screening tool, use text-mining, employ independent double-screening of each work, and immediately reconcile disputes. After the screening phase, analysing the process itself for validity is recommended.

Research on title and abstract screening reveals crucial elements for successful outcomes and strongly supports using both the title \emph{and} abstract. For instance, recent work comparing title-only (Ti/O) versus title-plus-abstract (Ti+Ab) screening found that Ti+Ab achieved significantly higher sensitivity (94.7\% vs. 57.9\%) by capturing studies with ambiguous titles but relevant abstract content. At the same time, Ti/O risked excluding 42.1\% of eligible studies due to insufficient title clarity \cite{teo_title-plus-abstract_2023}. Another study showed that adopting a Title-First screening approach (rejecting a study based on the title, followed by abstract assessment for inclusion) reduced workload compared to a title-and-abstract approach, with no reduction in recall of relevant research \cite{mateen_titles_2013}. The use of non-experts (medical students) as a feasible alternative to experts when undertaking citation screening was demonstrated in the TASER study \cite{ng_title_2014}.


\subsection{Challenges}

% Resource usage
The main challenges fall into three categories: resource use, consistency, and bias. The title and abstract screening stage often involves hundreds or thousands of potentially relevant studies, each requiring assessment by at least one, ideally two, trained reviewers. This scale is not trivial. Estimates suggest that a proficient information specialist can screen between 0.133 and 2.88 abstracts per minute \cite{shemilt_use_2016,giummarra_evaluation_2020,felizardo_visual_2013}; consequently, screening a large volume of abstracts is a significant undertaking. This task's sheer scale demands considerable time from expert reviewers, whose expertise is costly and scarce, thus increasing the financial burden of conducting systematic reviews

Furthermore, attempts to mitigate costs by reducing screening requirements, such as employing a single reviewer, substantially compromise the recall of relevant studies. A randomised controlled trial using the Cochrane Crowd platform demonstrated that single-reviewer approaches could miss up to 13\% of relevant studies, whereas a dual-reviewer approach missed only 3\% \cite{gartlehner_single-reviewer_2020}. Similar findings have been reported in other studies, indicating that even experienced reviewers, when working in isolation, consistently missed more relevant references \cite{waffenschmidt_single_2019}. Because minimising the risk of excluding pertinent evidence is crucial, dual screening is considered indispensable despite the associated resource implications.

The temporal demands of systematic reviews are also substantial, with an average completion time of 67.3 weeks \cite{borah_analysis_2017}. The screening phase alone can prolong the timeline by several months. While the precise number of abstracts screened per review is not uniformly documented, surveys of established systematic reviews report a median of 555 abstracts, spanning from 232 to 9,648 \cite{nama_successful_2021}. In one analysis of 14 systematic reviews, 19,334 abstracts were screened, yielding only 3.1\% (605) relevant studies \cite{nama_successful_2021}. At such scales, reviewer fatigue poses a legitimate concern, as prolonged engagement in tedious screening tasks can diminish vigilance and compromise the accuracy of the review process.


% Consistency 
Consistently applying inclusion and exclusion criteria across all studies poses a persistent challenge, with inter-coder discrepancies common, even when two reviewers evaluate the same abstracts. This is particularly true for studies in a ``grey area" that require nuanced judgment for inclusion or exclusion. Resolving these discrepancies typically requires discussion between reviewers or consultation with a third, adding roughly 5 minutes per conflict \cite{shemilt_use_2016}. With thousands of abstracts, this resolution process significantly increases resource and time demands. Moreover, inherent variability within review teams—differing expertise, subjective interpretations, and individual biases—fosters inconsistencies. Although rigorous pilot testing and ongoing calibration exercises help, consistently applying criteria remains challenging, especially over extended screening periods with numerous abstracts.

% Bias
Bias in title and abstract screening occurs when certain studies are systematically favoured or excluded. Several factors contribute to this risk: 
\begin{itemize} 
\item \textbf{Single-reviewer bias:} As previously discussed, single-reviewer approaches miss more eligible studies, potentially reducing the comprehensiveness of the final evidence base.
\item \textbf{User fatigue:} Extended screening periods can cause ``attention drift", leading reviewers to exclude relevant studies inadvertently.
\item \textbf{Prior knowledge and expectations:} Reviewers with pre-existing knowledge may unconsciously favour studies aligning with their beliefs or published in well-regarded journals. Conversely, they may dismiss studies they deem less relevant, introducing selection bias.
\end{itemize}

% Bias Ctd
Another source of bias is assuming humans always label documents correctly. Wang et al. report error rates as high as 10.76\% during abstract screening \cite{wang_error_2020}. While strategies like dual-review and reporting inter-coder agreements help, they do not guarantee error elimination.

\section{Screening prioritisation}\label{sec:screening Priorisation}

%What is screening prioritisation
Screening determines whether a document is relevant to the research question. Reviewers performing systematic reviews screen the title and abstract substage by assessing basic document information (i.e, title and abstract) against predefined eligibility criteria. Existing practice usually treats each document as an isolated entity, neglecting supplementary metadata such as citations, co-authors, or citation frequency.  A second, more resource-intensive full-text review follows to confirm inclusion. Because full-text review is time-consuming, many methods have been proposed to automate or semi-automate the title and abstract screening step—often referred to as prioritisation—to reduce the screening burden.

Prioritisation is the process of ranking documents by their likelihood of relevance. Prioritisation puts the documents at the top of the list that should address the systematic review's research question, enabling reviewers to find relevant literature more quickly. In principle, prioritisation can dramatically reduce screening efforts by allowing a screener to stop once they have located most or all of the relevant articles. For example, consider a hypothetical dataset of 20 documents, with 5 of those being relevant. All 20 must be screened without any prioritisation to find the five that matter. If the documents are ranked perfectly in descending order of relevance, the screener could theoretically locate all five relevant documents by examining only the first 5 in the list, reducing total document reviews. 

\gls*{tar}, also known as Computer-Assisted Review or Predictive Coding, is a process that uses machine learning to assist in document screening. Approaches to \gls*{tar} fall into two categories: (1) those ranking directly with queries (e.g. the query being the title \cite{alharbi_ranking_2017, alharbi_retrieving_2018}, boolean search terms  \cite{alharbi_ranking_2017, alharbi_retrieving_2018, alharbi_ranking_2019}, or review objectives \cite{ferro_qut_2017, scells_integrating_2017}), and (2) those using indirect methods like relevance feedback \cite{alharbi_ranking_2019} or active learning \cite{cormack_technology-assisted_2017, cormack_systems_2019, grossman_technology-assisted_2010, grossman_automatic_2017}. This research focuses on using active learning, which has successfully been used in many areas of information retrieval \cite{cormack_autonomy_2015, cormack_engineering_2016, yu_fast2_2019, yu_finding_2018, miwa_reducing_2014}.

\subsection{Active learning}

% What is active learning
Deep learning models and other classifiers often require large labelled datasets to perform well. In systematic review screening, gathering labels can be slow and expensive, especially at the outset when little is known about relevance. Active learning aims to mitigate this challenge by selecting the most \emph{informative} unlabelled documents for human labelling in iterative cycles.

This iterative process aims to substantially improve model performance while minimising the time and effort required for labelling. A typical active learning workflow selects a batch of unlabelled samples based on their potential to improve the model. These selected samples are then labelled and incorporated into the growing training dataset. The model is subsequently retrained using this expanded dataset, leading to improved model classification and ranking accuracy. The core principle behind active learning is to balance exploring uncertain or ambiguous documents (exploration) and exploiting documents that are likely to be highly relevant (exploitation). This approach enhances the model's ability to rank documents accurately, making the systematic review process more efficient and effective.

Variations of active learning exist, such as membership query synthesis \cite{angluin_queries_1988}, stream-based selective \cite{akinseloyin_novel_2024} and pool-based sampling \cite{lewis_sequential_1994}. 
Pool-based sampling best suits title and abstract screening because the unlabelled document set is known in advance. Figure \ref{fig:pool_based_query} illustrates this workflow. Mainstream prioritisation approaches largely model documents based on their text alone, such as bag-of-words or embeddings of titles and abstracts \cite{diao_lexical_2021}. Conventional active learning algorithms do not incorporate inter-document relationships; they rely on textual features or basic metadata to measure uncertainty and relevance. This can overlook important ``network-level" cues (e.g., shared references, reciprocal citations, co-authorship clusters) that could guide the active learning process toward the most pertinent articles faster.


\begin{figure}
\centering
\includegraphics[width=0.5\linewidth]{images/pool_based_strategy.png}
\caption{Overview of a pool-based query strategy for active learning, replicated \cite{ren_survey_2020}}.
\label{fig:pool_based_query}
\end{figure}

% Active Learning Notation 
\begin{tcolorbox}[title=The General Active Learning Process]  
\small
The active learning process can be formally described as follows:

\begin{enumerate}
    \item A sampling policy ($\pi$) intelligently selects the most informative samples ($\mathbf{T}_{C,i}$) from an unlabelled dataset ($\mathbf{T}_{U,i}$).
    \item These samples are passed to an oracle ($O$) for labelling.
    \item The labelled samples are added to a growing known dataset ($\mathbf{T}_{K,i}$).
    \item A classifier is trained on this labelled dataset.
    \item The classifier ranks the remaining unlabelled samples in ($\mathbf{T}_{U,i}$), often based on the classifier's uncertainty or expected model improvement.
    \item This iterative process repeats, intending to improve the classifier's performance with each labelling round until a predefined stopping criterion is met.
\end{enumerate}
\end{tcolorbox}
The oracle within systematic reviews denotes a human-verified label resulting from screening potential research for inclusion within the systematic review within the title and abstract screening stage. More concretely, $O$ can be considered a function $O(x) = y$ where $x$ is a document representation (i.e. if a human was reviewing a title and abstract or a representation of the title and abstract for automated approaches), and $y$ is the assigned category (included or excluded). In active learning research, in contrast to research in systematic reviews, it is assumed that for each $x$, $O$ provides a single judgement ($y$), which is always correct.

\input{tables/mathmatical_notation_active_learning}

% How AL doesn't address systematic review issues

While active learning offers advantages, its direct application to systematic reviews has limitations:

\begin{itemize}
    \item \textbf{Delayed Incorporation of Early Information:} Waiting until a minimum number of documents has been assessed before training the model means valuable information gathered in the early phases is not incorporated into the ranking process.
    \item \textbf{Delayed Search Phase:} The actual search and screening process is delayed until sufficient relevant studies are present in the document collection.
    \item \textbf{Minimum Number of Relevant Studies Required:} A minimum number of relevant studies is often required in the initial document collection. This is usually impractical in the early stages of a systematic review.
\end{itemize}

To truly minimise screening effort, a scenario is needed where minimal expert screening is performed on documents returned from the identification phase, and the model can safely extrapolate that screening to a larger pool.

These disadvantages lead to diversifying \gls*{tar} approaches. The different \gls*{tar} approaches have been outlined in Table \ref{tab:differences_in_tar} and can be categorised as follows: Simple passive learning (SPL) \cite{cohen_reducing_2006}, where a ``seed set" of human-labelled documents from $T$ creates the labelled set $T_{K,i}$ used to train a classifier. This classifier then labels $T_{U, i}$ without further learning. Simple Active Learning (SAL), where a ``seed set" of human-labelled documents from $T$ creates the labelled set $T_{K,i}$ is used to train a classifier. This classifier then labels $T_{U, i}$, which is ordered by relevance, and then the most uncertain document labels are revealed. The model is retrained after each batch of newly labelled documents, improving its accuracy. The \gls*{cal} process is similar to SAL; the model is updated continuously, in real-time or near real-time, as human reviewers label documents. There are no discrete rounds; learning is ongoing. The system may still prioritise documents for review based on uncertainty or other strategies, but it can also adapt to reviewer behaviour and potentially re-rank documents on the fly. Cormack et al. \cite{cormack_autonomy_2015} argue that \gls*{cal} better aligns with systematic review workflows because the goal is to unearth all relevant documents quickly (i.e. exploitation) rather than just optimising model accuracy per see (i.e. balancing exploitation \emph{and} exploration). Nonetheless, as the subsequent research will show, \gls*{cal} approaches have traditionally treated each document individually rather than utilising the relational structure in the corpus.


\begin{table*}[t]

    \centering
    \footnotesize
    \begin{tabular}{|c|c|c|c|}
        \hline
        \textbf{Feature} & \textbf{Simple Passive Learning} & \textbf{Simple Active Learning} & \textbf{Continuous Active Learning} \\
        \hline
        Learning & One-time, from initial set  & Iterative, in batches  & Continous, real-time \\
        \hline
        Model Updates & None after initial training & After each batch & After each labelled document \\
        \hline
        Reviewer Interaction & None after initial set & Periodic (batch-wise) & Continuous \\
        \hline
        Adaptability & Static  & Moderate & High \\
        \hline
        Prioritisation & Based on initial model & Adapts per batch & Adapts continuously \\
        \hline
    \end{tabular}
    \caption{The key differences between the different forms of active learning within TAR.}
    \label{tab:differences_in_tar}
\end{table*}



\subsection{Metadata usage in screening}

% Clarify what is meant by metadata, and where we can find it!
\gls*{cal}, in principle, benefits greatly from additional signals that can quickly surface the most relevant documents. Citation networks and metadata can do exactly that. Within this research, metadata refers to the structured or semi-structured information accompanying a publication beyond its raw text—such as author affiliations, publication dates, co-author relationships, journal or conference venues, keywords (including MeSH terms), and other bibliographic details. This metadata is abundant, freely available, and already computed. OpenAlex\footnote{https://www.openalex.org}, for example, is an open-sourced API that covers a large proportion of the literature, providing extensive metadata.

Inter-document relationship refers to citation network analysis, which can be performed using this metadata. So, for example, if a paper cites another, this document is said to be related. Analysis of inter-document relationships and metadata is something that searchers instinctively do when screening documents. To illustrate, if a searcher finds a foundational relevant document, such as Cormack's work on Auto \gls*{tar}, their other work is likely related to that topic. If a searcher is searching for research on COVID-19, publications before 2019 are not likely to be related. 

The manual approach to title and abstract screening reviews all documents within the total document pool, so inter-document relationships offer little benefit. Interestingly, \gls*{bcs} and \gls*{fcs} are used in the Cochrane approach in the preceding stage, ``identification" that forms the pool~\cite{briscoe_conduct_2020, noauthor_mecir_nodate}. Once the pool of candidate articles is set, the screening phase—where documents are prioritised or ranked—almost always relies solely on each document’s title/abstract, ignoring citation or co-authorship signals that might refine relevance scores. This standard approach—extracting only an article’s abstract and minimal bibliographic fields—overlooks valuable signals embedded in established citation networks, co-author clusters, and publication venues.

Despite the substantial research on automating systematic review screening, most approaches rely almost exclusively on a document’s title and abstract (rarely supplemented with keywords or MeSH terms). A recent scoping review of PubMed-indexed studies on systematic review automation (covering 123 articles) demonstrates this gap clearly: Tóth et al. found that the great majority of record-screening automation tools (broadly utilising SPL, \gls*{cal} or SAL approaches) used either bag-of-words or TF-IDF representations of the title and abstract text, with only a small proportion incorporating additional metadata such as publication date or author information \cite{toth_automation_2024}. Indeed, a typical pipeline in these tools converts the abstract into a vector of token counts or embeddings and then applies a machine learning classifier or an active learning framework to predict relevance. While a handful of studies appended ``bibliographic details” or ``MeSH terms,” Tóth et al. note that these uses of metadata are inconsistent, rarely extend beyond indexing keywords and do not systematically exploit richer inter-document relationships (e.g., citation networks, co-authorship, or shared references). The author has not identified studies that incorporate metadata into \gls*{cal} in a structured way. 

Addressing this gap represents both a challenge and a major opportunity. Incorporating structured metadata into \gls*{cal} prioritisation pipelines could improve recall by surfacing relevant clusters of connected documents earlier. Simultaneously, it may reduce screening workloads by deprioritising documents in low-yield regions of the citation or co-authorship graph. Thus, research into \gls*{cal} here is viewed through the lens of how they represent documents and if they consider inter-document relationships.  

\subsection{Continuous active learning for systematic reviews}

Recall targets in systematic reviews differ from other domains (e.g.\ e-discovery in law or social-media sentiment analysis) by demanding near-complete recall to ensure no critical study is missed. In medical research, overlooking even a single relevant article can critically affect the interpretation of evidence. Despite this demanding requirement, \gls*{cal} has shown promise for reducing systematic review screening workloads by up to 77\%~\cite{van_der_vet_propagation_2016}. Nonetheless, current \gls*{cal} approaches for systematic reviews still largely treat documents as stand-alone entities, relying on shallow representations (e.g.\ bag-of-words or simple neural embeddings) and giving limited attention to inter-document relationships (e.g.\ shared citations or authorship).

To understand the evolution of \gls*{cal} methods and identify opportunities for incorporating relational signals, the author categorises existing approaches based on how they represent documents (i.e., feature-based vs \ encoder-based vs \ decoder-based). A Feature-based approach is where the title and abstract are transformed into numerical vectors representing features like word presence or importance (e.g., TF-IDF); an Encoder-based approach, where models like BERT capture semantic relationships between words in the title and abstract using a masked self-attention mechanism; and a Decoder-based approach is where generative models like GPT predict the probability distribution of the next word in a sequence, leveraging unidirectional context. Such a taxonomy helps illustrate the progression from methods relying on simple lexical features to those capturing increasingly complex semantic relationships and ultimately highlights the potential for incorporating relational information.

\paragraph{Feature-based approaches: }

These methods, while foundational, represent documents as isolated entities. They typically transform document text (title and abstract) into numerical vectors, where each dimension represents a specific feature, such as the frequency of a term (e.g., TF-IDF). A common characteristic of these methods is their treatment of documents as relatively independent entities, focusing primarily on word-level importance without explicitly modelling the inter-document relationship.

Cormack et al.~\cite{cormack_evaluation_2014} introduced a foundational feature-based \gls*{cal} approach using a Support Vector Machine (SVM) classifier. Their method trained a Support Vector Machine (SVM) classifier on an initial set of documents identified through keyword seeding. While this work demonstrated the superiority of \gls*{cal} over traditional screening methods in an e-discovery context, it presented limitations when applied to the specific requirements of systematic reviews. Notably, the use of a large, fixed batch size (1,000 documents) and a focus on achieving recall@75\% are not well-aligned with the need for near-total recall and adaptive learning in medical literature reviews. Moreover, no leveraging of relational structure inherent in medical literature was used.

While subsequent research, like Auto~TAR~\cite{cormack_autonomy_2015}, explored refinements such as alternative term weighting (e.g., Cornell ltc) and batch size optimisation, these methods still primarily focus on word-level importance within individual documents; Auto TAR approach is outlined in Figure~\ref{fig:autotar_process}. Auto~TAR differed from the foundational system by using a single seed document (instead of 1,000) and incrementally increasing by approximately 10\% each iteration, rounded up, demonstrating improved performance and better alignment with systematic review screening. It used Cornell ltc term weighting \cite{salton_smart_1965} to represent documents. Auto~TAR has remained a de facto baseline in many \gls*{cal} studies, sometimes called the ``base model implementation" (BMI).

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{images/autotar.jpg}
    \caption{Auto~TAR outline, as documented within the patient filing by Cormack et al. \cite{cormack_systems_2016}}.
    \label{fig:autotar_process}
\end{figure}

More recent work has examined hyperparameter choices within feature-based \gls*{cal} pipelines. Ferdinands et al.~\cite{ferdinands_performance_2023} simulated human reviewers screening six labelled systematic reviews, comparing four classifiers (Naive Bayes, Logistic Regression, SVM, Random Forest) under two document representations (TF-IDF vs.\ doc2vec). The results indicated that Naive Bayes with TF-IDF features reduced the average time to discovery. However, none of these systems integrated metadata or inter-document relationships.

% Approach looking at sentence-level classification vs document-level
Incorporating more information in the screening stage is a step towards richer representations, but they fail to leverage inter-document relationships fully. Although standard feature-based \gls*{cal} often relies on the title and abstract alone, Zhang et al.~\cite{zhang_evaluating_2020} showed that using isolated representative sentences (via a TF-IDF-based approach) could achieve comparable accuracy and higher efficiency than the BMI. These experiments did not use medical domain datasets and utilised full-text documents, but the study hints at the potential gains from more nuanced feature engineering. Other work has examined MeSH keywords \cite{miwa_reducing_2014} and Latent Dirichlet Allocation (LDA) \cite{hashimoto_topic_2016, miwa_reducing_2014} to enrich feature sets and showed promising performance improvements. This work demonstrates the potential for exploring alternative, superior representations beyond titles and abstracts.

% Incorporating semi-supervision via label propagation
Through incorporating citation proximity, Kontonatsios et al.'s work on label propagation highlights the potential of relational information ~\cite{kontonatsios_semi-supervised_2017}. It introduced a label-propagation method in which a small batch of human-labelled abstracts assign provisional labels (in TF-IDF or spectral-embedded space) to nearby unlabelled citations. These pseudo-labelled documents augment the training pool, accelerating the active learning process. The approach surfaced relevant papers significantly earlier than purely supervised strategies — reinforcing the notion of leveraging unlabelled data and relational signals (e.g.\ local neighbourhoods in feature space) to increase recall. Kontonatsios et al.’s results enhanced recall without imposing an additional annotation burden. This motivates further exploration of semi-supervised paradigms in future \gls*{cal} systems, especially with advanced document embeddings and network-based relationships.

% Summary of feature-based CAL
In summary, feature-based \gls*{cal} has proven robust and is a strong baseline.  However, these methods view documents as isolated or rely only on limited textual features (TF, TF-IDF, or keywords). \emph{They do not exploit deeper semantic embeddings or intricate inter-document relationships}, leaving a clear research gap. Though these feature-based techniques paved the way for \gls*{cal} in systematic reviews, they typically ignore nuanced contextual signals among documents. This omission impedes refined strategies for prioritising and terminating screening. The next section explores whether encoder-based methods offer a deeper representation — and possibly an avenue to more context-aware \gls*{cal}.

\paragraph{Encoder-based approaches: }
% Encoder based approaches

Encoder-based models like BERT capture deeper semantic relationships within documents but typically do not explicitly model relationships between documents. Self-attention mechanisms have revolutionised the field of Natural Language Processing, significantly improving performance across a wide range of tasks \cite{vaswani_attention_2023}. However, \gls*{cal} processes did not fully leverage these advancements initially. The major contribution of self-attention lies in its ability to generate context-dependent text representations, allowing for a more nuanced understanding of text compared to earlier feature-based methods. This capability enables self-attention models to capture subtle semantic relationships and long-range dependencies often missed by traditional approaches like TF-IDF, which were previously dominant in \gls*{tar}.

Encoder-based models, such as BERT, PubMedBERT and BiolinkBERT, achieve high performance on classification tasks through a two-step process of pre-training (where a model has initially trained large corpora of text to develop a general understanding of language and the specific domain) and fine-tuning (where a pre-trained model is further trained on small,task-specific datasets to adapt it for the particular task). 

Initial work like CALBERT explored incorporating encoder-based models but did not demonstrate superiority \cite{sadri_continuous_2022}. CALBERT ranks a pool of documents using a logistic-regression-based–based \gls*{cal} model, then refines the top $k$ documents with a BERT-based reranker. Each selected document (title + abstract) is truncated to 512 tokens, converted into an embedding via BERT's CLS token, and passed through a fully connected layer to produce a relevance score. After presenting the highest-scoring document for user feedback, CALBERT is fine-tuned on this new relevance information, and the underlying \gls*{cal} model is also retrained. However, despite the appeal of using transformers for feature-rich document representations, CALBERT often underperforms the simpler BMI. Variations such as appending the query text (monoBERT) or continuously fine-tuning BERT did not yield consistent gains in a high-recall setting. 

Further research explored pre-training strategies that hindered encoder-based \gls*{cal}. Yang et al. \cite{yang_goldilocks_2022} fine-tuned a pre-trained BERT on an unlabelled corpus (testing 0-10 epochs) before applying it to \gls*{cal}. They iteratively sampled 200 documents, labelled them, and fine-tuned BERT for 20 epochs, using the previous iteration's model as the starting point. They found that five epochs of initial fine-tuning yielded performance comparable to TF-IDF and logistic regression on an in-domain dataset but significantly worse on an out-of-domain dataset. This could be attributed to ``catastrophic forgetting" \cite{xu_forget_2020}, where excessive fine-tuning on the task corpus causes the model to lose general knowledge from pre-training. However, Mao's subsequent research suggested that the optimal pre-training epoch (coined ``goldilocks" epoch) \gls*{cal} was not universal, especially in medical datasets \cite{mao_reproducibility_2024}. Mao's most performant model was not fine-tuned, but rather a model that uses a graph-based approach to pretrain, ${\text{BiolinkBERT}}_{\text{base}}$.

BioLinkBERT incorporates citation links during pre-training and represents a significant step towards leveraging relational information. It is the most performant encoder model on the CLEF dataset in a \gls*{cal} setting to date. The LinkBERT approach to pre-training models was to view a pertaining corpus as a graph of documents, with each document being a vertex and hyperlinks forming edges between documents \cite{yasunaga_linkbert_2022} and compared to BERT approach in Figure \ref{fig:linkbert-comparison}. These linked documents were then placed within the same context, different from that of BERT random document allocation, in which no linkage between documents within a context window is required. LinkBERT differs from curriculum learning, where a model is provided with examples of increasing difficulty, as the context windows' documents are not ordered by difficulty. A domain-specific variant of LinkBERT, BioLinkBERT\footnote{https://huggingface.co/michiyasunaga/BioLinkBERT-base}, was created, which was pre-trained only on PubMed articles, with linkage of documents being determined through citations of that research. Models were then trained using standard masked language modelling and next-sentence prediction. The performance of a base model (100M parameters) and a large model (340M parameters) was compared to PubMedBERT\footnote{https://huggingface.co/microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext} in BLURB~\cite{gu_domain-specific_2021}, MedQA-USMLE~\cite{jin_what_2021}, and MMLU-professional medicine (medical-specific downstream benchmark tasks)~\cite{hendrycks_measuring_2021}. ${\text{BiolinkBERT}}_{\text{large}}$ achieved state-of-the-art on all reported benchmarks, with an improvement in the BLURB score of 3.2\% above PubMedBERT.

\begin{figure}[t]
    \centering
    \begin{tikzpicture}[
        doc/.style={rectangle, draw, fill=blue!20, minimum width=2.5cm, minimum height=1cm},
        window/.style={rectangle, draw, fill=purple!20, minimum width=4cm, minimum height=1cm},
        mlm/.style={rectangle, draw, fill=orange!20, minimum width=4cm, minimum height=0.8cm},
        node distance=2cm,
        scale=0.9
    ]
    % Traditional BERT (top)
    \node[align=center] at (-3,5) {\Large Traditional BERT};
    \node[doc] (a1) at (-6,3.5) {Cancer Paper};
    \node[doc] (a2) at (0,3.5) {Climate Paper};
    \node[window] (b1) at (-3,2) {Random Context};
    \node[mlm] (m1) at (-3,0.5) {MLM: The [MASK] study};
    
    \draw[->] (a1) -- (b1);
    \draw[->] (a2) -- (b1);
    \draw[->] (b1) -- (m1);
    
    % LinkBERT (bottom) - with increased horizontal spacing
    \node[align=center] at (-3,-1) {\Large LinkBERT};
    \node[doc] (c1) at (-8,-2.5) {Cancer Study};
    \node[doc] (c2) at (-3,-2.5) {Biomarkers};
    \node[doc] (c3) at (2,-2.5) {Treatment};
    
    \node[window] (d1) at (-6,-4) {Linked Context};
    \node[mlm] (n1) at (-6,-5.5) {MLM: Cancer [MASK]};
    
    \node[window] (d2) at (0,-4) {Linked Context};
    \node[mlm] (n2) at (0,-5.5) {MLM: Treatment [MASK]};
    
    % Citations with curved arrows
    \draw[->, bend angle=30] (c1) to[bend left] node[midway,above] {\small cites} (c2);
    \draw[->, bend angle=30] (c2) to[bend left] node[midway,above] {\small cites} (c3);
    
    % Context connections with slight bends to avoid overlap
    \draw[->] (c1) -- (d1);
    \draw[->] (c2) to[bend right=15] (d1);
    \draw[->] (c2) to[bend left=15] (d2);
    \draw[->] (c3) -- (d2);
    \draw[->] (d1) -- (n1);
    \draw[->] (d2) -- (n2);
    
    \end{tikzpicture}
    \caption{Comparison of document processing in traditional BERT versus LinkBERT. Traditional BERT (top) randomly groups documents into context windows, while LinkBERT (bottom) uses citation relationships to create meaningful document groupings for pretraining. The citation-based grouping ensures that semantically related documents are processed together during masked language modelling tasks.}
    \label{fig:linkbert-comparison}
\end{figure}

Sparse encoder approaches like SPLADE for \gls*{cal} have recently been explored outside the medical domain. It leverages a masked language modelling head to expand each token in a document based on contextual cues from the surrounding text. Rather than extracting a single [CLS] embedding, SPLADE predicts a full vocabulary distribution for each token and aggregates these token-level distributions (via max pooling and sparsity regularisation) to form a sparse document representation. This technique preserves many advantages of bag-of-words approaches (e.g., interpretability, efficiency) while injecting knowledge gleaned from large-scale language model pre-training.

Yang et al. found that replacing TF–IDF or BM25 features with SPLADE-based sparse vectors yields a 10–20\% cost reduction \cite{yang_contextualization_2024}. Crucially, these gains arise because the model can capture synonyms, paraphrases, or other subtle relationships that purely lexical methods miss—accelerating the discovery of relevant studies. This highlights how richer, context-aware document representations enhance the effectiveness of \gls*{cal}, especially under the demanding requirement of near-total recall. The efficacy of medical pre-trained models using SPLADE on medical \gls*{tar} datasets has not been explored.

Research most closely aligned with the central motivation is presented in SciMine \cite{guo_scimine_2023}. This work addresses screening prioritisation within an active learning framework by leveraging inter-document relationships derived solely from the title and abstract. SciMine employs document-level representations generated by SPECTER—a pre-trained transformer model originally developed on citation networks—and enhances these with phrase-level features. The phrase-level module extracts key phrases using a mining tool and obtains contextual embeddings (e.g., via SciBERT), which are subsequently clustered to capture semantic nuances. The approach performs superior to other methods, notably improving early recall and work-saved metrics. However, the technique remains limited in scope as it focuses exclusively on textual and indirect citation-based semantics without fusing additional metadata (such as authorship, publication venue, etc.) that could further improve screening prioritisation.

% https://arxiv.org/pdf/2407.00635 - dense retrieval - is query based

\paragraph{Decoder-based approaches: }
Decoder-based approaches, such as LLM+\gls*{cal}, have been outlined and included in this literature review for completeness, as decoder-based work is highly topical. In the typical decoder-based approach, a prompt (with or without explanatory examples) is sent to a model, which generates text/tokens in response. Where BERT encodes a passage as a hidden representation, GPT-based models generate text from a unidirectional context.

Decoder-based \gls*{cal} approaches used prompt engineering with a chain-of-thought approach. Bron et al. proposed a novel method, LLM+\gls*{cal}, that leverages the capabilities of  a decoder-based \gls*{llm} to enhance the screening process in systematic literature reviews~\cite{bron_combining_2024}. This contrasts significantly with the encoder-based methods discussed previously, which primarily focus on generating contextualised representations for ranking. Bron et al.'s approach focuses on developing more fine-grained and explainable classifications by prompting the \gls*{llm} to evaluate each document against individual inclusion criteria specified in the review protocol. Instead of a single binary inclusion/exclusion decision, the \gls*{llm} provides a structured response with reasoning and cited evidence (rationales) for each criterion. This study was, however, only run on a single dataset (Post-traumatic stress disorder systematic review), and the generalisability of this approach is yet established. This criterion-level classification is a unique aspect of their work. It offers potential advantages in integrating \gls*{llm}s to generate 'synthetic' metadata about a document, which can be combined with a document representation. While this approach demonstrates the power of \gls*{llm}s, they still largely focus on individual documents.

Work on prompt selection in decoder-based approaches has demonstrated that active learning can be used to select better representative examples for shot prompting.  Baseline, zero-shot prompting not in an active learning process showed good results on a medical dataset, comparable to decoder BioBERT approaches~\cite{wang_zero-shot_2024}. Few-shot prompting also resulted in better performance \cite{margatina_importance_2022} in combination with active learning. It showed that using active learning to identify and choose the most representative examples significantly improved few-shot prompt performance on downstream tasks. This suggests that strategically combining active learning with prompt engineering could enhance the effectiveness of decoder-based models in \gls*{tar}, even with limited labelled data. While this approach demonstrates the power of \gls*{llm}s, they still largely focus on individual documents, analysing them against inclusion criteria. Even active learning for prompt selection, while improving inter-document representation for prompting, does not yet address the crucial need to model inter-document relationships.  This suggests that while decoder-based methods hold promise, further research is needed to explicitly incorporate relational information into their frameworks. 

While decoder-based approaches might produce rationales or explanations, their ability to handle large-corpus level inter-document relationships is limited by their context window size, which restricts the number of documents and relationships they can represent in a single context.

\subsection{Conclusion}
The reviewed literature consistently demonstrates the benefit of moving beyond shallow text representations. However, even the most advanced encoder- and decoder-based methods do not explicitly utilise the rich inter-document relationships present in medical research.  Incorporating these relationships is crucial for improving CAL in systematic reviews. Tools, such as citation mining and \gls*{gnn}, offer a natural way to represent and reason about these relationships, making them a promising avenue for future research.


% https://ceur-ws.org/Vol-3770/paper8.pdf - A true LLM + CAL Approach
% https://arxiv.org/html/2401.06320v2 - Showing that LLMs in a zero shot setting have been used to screen documents
% https://openreview.net/forum?id=i0RfSS9CUU - using AL to choose best example for the few shot approach
% https://pmc.ncbi.nlm.nih.gov/articles/PMC11504244/ - general overview of software
% https://www.researchgate.net/publication/385410245_Accelerating_Systematic_Reviews_with_Large_Language_Models_Current_Practices_and_Recommendations
% https://systematicreviewsjournal.biomedcentral.com/articles/10.1186/s13643-024-02609-x#Sec2 - NO priorisation so not AL

\section{Metadata and citation networks}\label{sec:metadata}

As outlined in the preceding sections, this PhD research aims to enhance the efficiency and accuracy of systematic review screening through \gls*{cal}. A key limitation of current\gls*{cal} approaches is their shallow document representation, often neglecting valuable metadata and inter-document relationships within citation networks. This section explores how incorporating these elements can significantly improve document screening prioritisation, addressing the PhD's core objective of minimising expert screening effort while maintaining high recall. Firstly, traditional methods like \gls*{bcs} and \gls*{fcs} are considered. Secondly, \gls*{gnn}s are assessed. Finally, the potential of integrating \gls*{llm}s into the screening process is reviewed.

In scientific research, understanding relationships between documents is critical for identifying relevant literature, tracing ideas' evolution, and assessing individual studies' impact. To situate their findings within the broader literature, authors must reference prior works in their research papers, substantiating claims and demonstrating connections to existing knowledge. This practice relies on standardised formatting conventions and peer-reviewed publication systems, which ensure consistency and credibility in research communication \cite{noauthor_bmc_nodate}. These references form an explicit and analysable indicator of document relationships: the citation network. Specifically, citations documented in a paper's reference section create a structured graph that maps connections between studies. By analysing this network, researchers can infer relationships between papers, assuming that cited or cited works share meaningful relevance to the original research. This signal can be exploited for screening prioritisation as if a researcher has published relevant work towards a search; their other work is likely to be related.

\subsection{Citation network mining}

Performant, simple, and robust citation network mining approaches exist in medical research, i.e., \gls*{bcs} and \gls*{fcs}. The two methods can be understood using the graph network approach outlined below.

\begin{tcolorbox}[title=Citation Network Definitions] 
\small
Let $G = (D, E)$ be a directed citation graph, where each vertex $D_i \in D$ represents a research article, and a directed edge ($D_i, D_j) \in E$ means that $D_i$ cites $D_j$. $D$ represents the universe of all documents in $G$. Let $D_i \in D$ be a document of interest. 

\begin{itemize}
    \item $D_{ip} = {D_j \in D | (D_i, D_j) \in E}$ as the set of articles referenced by $D_i$ (i.e., articles $D_i$ cites). 
    \item $D_{if} = {D_j \in D | (D_j, D_i) \in E}$ as the set of articles that reference $D_i$ (i.e, articles that cite $D_i$).

\end{itemize}

In most citation graphs (which are usually directed acyclic graphs, or at least lack reciprocal citations), the set of articles cited by a given paper and the set of articles that cite it are disjoint: $D_{ip} \cap D_{if} = \emptyset$. This is because a single edge represents only a citation relationship in a temporal direction. Searching both sets will, therefore, provide different relevant articles.
\newline
\newline
Two primary citation network mining approaches are defined, utilising the sets of related articles:

\begin{itemize}
    \item \gls*{bcs}:  This approach examines the set of articles \emph{cited by} the paper of interest, $D_{ip}$.  In practice, this involves analysing the relevancy of each article in $D_{ip}$ to the research question. \cite{lefebvre_cochrane_2011, akers2009systematic}
    \item \gls*{fcs}: This approach examines the set of articles that \emph{cite} the paper of interest, $D_{if}$.  Similarly, the relevancy of each article in $D_{if}$ is assessed\footnote{\gls*{fcs} involves using a citation index to identify studies that cite a source study. A citation index is a database of scholarly articles and their citations, such as PubMed, Google Scholar, Scopus or OpenAlex.}.
\end{itemize}

\end{tcolorbox}


% Research on using BCS and FCS in screening:
\gls*{bcs} and FCS are valuable tools within the identification phase of systematic reviews, helping researchers discover relevant studies by leveraging citation relationships to form the total document pool. They are not yet, however, utilised within the screening process that this research will focus on (as traditional approaches look at all documents within the pool). Even the use of \gls*{bcs} in the \emph{identification phase} (as mandated by the Cochrane Handbook, criterion C30~\cite{noauthor_mecir_nodate}) is not ubiquitous, with 87\% of sampled systematic reviews reportedly using \gls*{bcs} and 9\% \gls*{fcs} \cite{briscoe_conduct_2019}. Within this context, success is often measured by the number of relevant articles retrieved, with studies demonstrating the benefits of \gls*{bcs}/\gls*{fcs} in increasing recall. For example, research consistently shows that utilising \gls*{bcs} and \gls*{fcs} improves the retrieval of eligible articles, with the vast majority of studies (over 96\%) reporting some level of benefit \cite{hirt_citation_2023}. This research highlights the important role these methods play in comprehensive literature searching, yet they are not utilised within \gls*{cal}, which effectively searches for relevant documents for humans to screen.   

\begin{figure}[t]
\begin{center}
\begin{tikzpicture}[
    >={Stealth[scale=1.2]}, % Use a standard arrow tip like Stealth
    vertex/.style = {circle, draw, minimum size=1.2cm, inner sep=1pt},
    ref_edge/.style = {->, thick, blue},
    label_style/.style = {font=\small}
]
% Main document Di
\node[vertex, fill=yellow!20] (Di) at (0,0) {$D_i$};

% Past references (Dip) - More fanned out
\node[vertex] (Dip1) at (-3,2.5) {$D_{ip1}$};
\node[vertex] (Dip2) at (0,3) {$D_{ip2}$};
\node[vertex] (Dip3) at (3,2.5) {$D_{ip3}$};

% Future references (Dif) - More fanned out
\node[vertex] (Dif1) at (-3,-2.5) {$D_{if1}$};
\node[vertex] (Dif2) at (0,-3) {$D_{if2}$};
\node[vertex] (Dif3) at (3,-2.5) {$D_{if3}$};

% Edges for past references
\draw[ref_edge] (Di) -- (Dip1);
\draw[ref_edge] (Di) -- (Dip2);
\draw[ref_edge] (Di) -- (Dip3);

% Edges for future references
\draw[ref_edge] (Dif1) -- (Di);
\draw[ref_edge] (Dif2) -- (Di);
\draw[ref_edge] (Dif3) -- (Di);

% Labels for sets
% \node[label_style] at (-4,2) {$D_{ip}$ (Referenced by $D_i$)};
% \node[label_style] at (-4,-2) {$D_{if}$ (References to $D_i$)};

% Relevancy function examples
\node[label_style] at (5,2.5) {$O(D_{ip1}) = 1$};
\node[label_style] at (5,2) {$O(D_{ip2}) = 1$};
\node[label_style] at (5,1.5) {$O(D_{ip3}) = 0$};
\node[label_style] at (5,-1.5) {$O(D_{if1}) = 1$};
\node[label_style] at (5,-2) {$O(D_{if2}) = 0$};
\node[label_style] at (5,-2.5) {$O(D_{if3}) = 0$};

% Time arrow (vertical)
\draw[->, thick] (7,3.5) -- (7,-3.5); % Stealth is already set as default
\node[label_style] at (7,3.8) {Past};
\node[label_style] at (7,-3.8) {Future};

\end{tikzpicture}
\label{fig:bcs-fcs-explained}
\caption{Visual comparison of \gls*{bcs} and \gls*{fcs} approaches. Note that citation mining effectively operates on a temporal axis.}
\end{center}
\end{figure}

% How this approach will benefit CAL
The logical and simple augmentation of the encoder \gls*{cal} approach would be to exhaust both the \gls*{bcs} and \gls*{fcs} networks of a seed document before initiating the encoder \gls*{cal} process. A simple and performant approach could be achieved by first identifying all documents related to the seed document through \gls*{bcs} or FCS searches, assessing their relevance, and then adding the labelled documents to the training data used in the first epoch of the encoder \gls*{cal} process (see Figure 2.6). The theoretical benefits of citation network mining are that it can be used to augment the \gls*{cal} process in ways that overcome some of the limitations of this process. Firstly, \gls*{cal} requires labelled data to train a classifier model, which is assumed to perform better with more data points. Encoder \gls*{cal} approaches suffer disproportionately from feature-based \gls*{cal} approaches due to their need for larger training data to effectively learn meaningful representations, as they need to learn complex contextual relationships between words and concepts. Feature-based models, in contrast, can rely on simpler statistical patterns. When working with limited labelled data in the early screening stages, encoder models may struggle to generalise well, potentially leading to suboptimal performance in identifying relevant documents. Citation mining can also neatly fit into the existing performant encoder \gls*{cal} process, as a single sample seed document is often used during the first epoch for fine-tuning. A better approach would be to exhaust the \gls*{bcs}/\gls*{fcs} citation network of that seed document for labelling before using revealed relevant documents to fine-tune the model, potentially resulting in a more performant model at the earlier screening stages with less Oracle cost.

While \gls*{bcs} and \gls*{fcs} are effective methods for initial document discovery, they are inherently limited by their reliance on direct citation links. This reliance prevents them from identifying thematic connections between uncited yet related documents. This limitation is particularly relevant because the initial document pool is vastly smaller than the total scholarly literature, increasing the likelihood of important non-direct citations. Furthermore, \gls*{bcs} and \gls*{fcs} do not consider the semantic content of the papers beyond the presence or absence of citation links. This potential citation disconnection could mean relevant documents that do not share direct citations or use different terminology to describe similar concepts may be missed. Additionally, constraining document representations to the title, abstract, and citation analysis miss other relevant features, such as author information, publication date, or journal. Previous work by this author has demonstrated that additional features are important for downstream document classification tasks - see Appendix \ref{app:Predicting_article_retractions}. Publication year was found to be the most important feature when using standard feature-based and encoder- and decoder-based \gls*{llm}s (more so than the title and abstract). It might be argued that the importance of this feature might be specific to that task (i.e., retraction classification); yet, logically, it follows that when searching through a pool of documents, publication year could be an important signal for relevance, as knowledge of that search area improves, so does the definition, experimental design and even disease classification process improves.

These limitations highlight the need for a more sophisticated approach to capturing the full complexity of structural and semantic inter-document relationships. \gls*{gnn}s excel precisely in this area: They can capture multi-hop relationships (e.g., documents that share a chain of citations), leverage co-author networks, or incorporate publication venues.

\subsection{Advanced graph approaches}

\gls*{gnn}s can model the citation network in a way that goes beyond direct citation links, allowing for identifying relevant documents based on structural and semantic similarity. By incorporating document metadata and leveraging advanced techniques like message passing, \gls*{gnn}s can capture complex relationships that are not apparent when using \gls*{bcs}/\gls*{fcs} alone. This approach allows us to represent documents not only by their textual content (title and abstract) but also by their \gls*{bcs} or \gls*{fcs} links and associated metadata. \gls*{gnn}s are a family of deep learning models specifically designed to analyse data structured as graphs \cite{lee_attention_2018, wu_comprehensive_2021, bronstein_geometric_2017, khemani_review_2024}. Unlike traditional neural networks, such as Convolutional Neural Networks and Recurrent Neural Networks, which excel at processing grid-like or sequential data, respectively, \gls*{gnn}s are adept at leveraging the inherent relationships within graph data. These relationships can be represented as nodes (documents) and edges (connections) – see Figure \ref{fig:GNNs}. \gls*{gnn}s find applications in diverse areas, such as modelling interactions between users and products in e-commerce for recommendation systems or categorising research papers within citation networks. 

The fundamental mechanism behind \gls*{gnn}s is ``message passing". This iterative process involves each node aggregating information from its neighbouring nodes and subsequently updating its own representation based on this aggregated data. Message passing allows \gls*{gnn}s to capture complex dependencies and patterns that are not easily discernible in traditional Euclidean space. This process can be thought of as passing ``relevance signals" through the \gls*{gnn}, potentially allowing early identification of relevant clusters, enabling faster stopping and, thus, less screening.

Within the broader \gls*{gnn} landscape, various architectures exist, each employing a unique approach to message passing. A \gls*{gcn} \cite{berg_graph_2017} utilises a form of weighted averaging, combining the feature vectors of neighbouring nodes with weights derived from the graph structure. They then apply transformations to these combined features. gls*{gcn}s have demonstrated success in applications such as recommendation systems \cite{fan_graph_2019}, image classification \cite{monti_geometric_2016}, and traffic forecasting \cite{cui_traffic_2020}. \gls*{gats} enhance this process by incorporating an attention mechanism. This mechanism dynamically weighs the importance of each neighbour's features during aggregation, allowing the model to focus on the most relevant connections. GraphSAGE offers a different approach, focusing on inductive representation learning. It learns aggregation functions that sample and combine features from a node's neighbourhood, enabling generalisation to unseen nodes. 

While a comprehensive exploration of each \gls*{gnn} variant is beyond the scope of this review, it is important to note that these architectures show significant promise for document classification due to their ability to model complex relationships between documents. The next section considers how \gls*{gnn}s can be integrated into the document classification.
\begin{figure}[h!]
    \centering
    \begin{tikzpicture}
        % Euclidean Space (Left)
        \begin{scope}
            \node[circle, fill=blue!20, draw, minimum size=0.5cm] (A) at (0,0) {};
            \node[circle, fill=orange!20, draw, minimum size=0.5cm] (B) at (2,0) {};
            \node[circle, fill=cyan!20, draw, minimum size=0.5cm] (C) at (4,0) {};
            \node[circle, fill=orange!20, draw, minimum size=0.5cm] (D) at (0,2) {};
            \node[circle, fill=blue!20, draw, minimum size=0.5cm] (E) at (2,2) {};
            \node[circle, fill=red!20, draw, minimum size=0.5cm] (F) at (4,2) {};
            \node[circle, fill=purple!20, draw, minimum size=0.5cm] (G) at (0,4) {};
            \node[circle, fill=red!20, draw, minimum size=0.5cm] (H) at (2,4) {};
            \node[circle, fill=purple!20, draw, minimum size=0.5cm] (I) at (4,4) {};

            \draw (A) -- (B);
            \draw (B) -- (C);
            \draw (D) -- (E);
            \draw (E) -- (F);
            \draw (G) -- (H);
            \draw (H) -- (I);
            \draw (A) -- (D);
            \draw (B) -- (E);
            \draw (C) -- (F);
            \draw (D) -- (G);
            \draw (E) -- (H);
            \draw (F) -- (I);
        \end{scope}
        
        % GNN In Euclidean Space (Right)
        \begin{scope}[xshift=8cm]
            \node[circle, fill=blue!20, draw, minimum size=0.5cm] (A) at (0,2) {};
            \node[circle, fill=orange!20, draw, minimum size=0.5cm] (B) at (2,0) {};
            \node[circle, fill=yellow!80, draw, minimum size=0.5cm] (C) at (2,2) {};
            \node[circle, fill=yellow!40, draw, minimum size=0.5cm] (D) at (4,2) {};
            \node[circle, fill=purple!20, draw, minimum size=0.5cm] (E) at (4,3) {};
            \node[circle, fill=red!20, draw, minimum size=0.5cm] (F) at (2,4) {};
            \node[circle, fill=green!20, draw, minimum size=0.5cm] (G) at (5.5,2) {};
            \node[circle, fill=orange!20, draw, minimum size=0.5cm] (H) at (0,1) {};

            \draw (A) -- (B);
            \draw (A) -- (C);
            \draw (B) -- (C);
            \draw (C) -- (D);
            \draw (C) -- (H);
            \draw (D) -- (E);
            \draw (D) -- (G);
            \draw (E) -- (F);
            \draw (G) -- (B);
        \end{scope}
    \end{tikzpicture}
    \caption{Grid Graph (Euclidean Structure) (Left), Arbitrary Graph (Non-Euclidean Structure) (Right)}
    \label{fig:GNNs}
\end{figure}

\begin{tcolorbox}[title=Conceptualising the integrating \gls*{gnn}s for document classification]  
\small


  A node is represented by a feature matrix containing information about the document. This \textbf{Node feature matrix}, $X$, which has the dimensions of $m$ (the number of nodes) and $n$ (the number of features). $X \in \mathbb{R}^{m \times n}$. X does not have to be a square matrix and does not encode any information about the graph's structure. 

  Consider three research papers as nodes, with features: [Author, Title Length, Abstract Length, Citation Count]
  $X = \begin{bmatrix}
  \text{"Smith"} & 82 & 500 & 45 \\
  \text{"Johnson"} & 95 & 475 & 23 \\
  \text{"Zhang"} & 67 & 612 & 89
  \end{bmatrix}$
  Where $X \in \mathbb{R}^{3 \times 4}$ represents:
  
  3 papers (rows)
  4 features per paper (columns)
  Mixed data types (categorical and numerical)

  Structural information is encoded in the \textbf{adjacency matrix}, $A$, which has the dimensions of $m$ (the number of nodes) and $m$ (the number of nodes). $A \in \mathbb{R}^{m \times m}$. A encodes information about the graph's structure and is used to determine relationships between nodes. Conventionally, the source nodes are the rows, and the destination nodes are the columns of the matrix. 1 indicates an edge between the source node $u$ and destination node $v$. Note that there is a choice here, with the matrix diagonal being 0 or 1. This choice is based on whether you consider the source node connected to itself. In cases where the representation of the node depends on itself and adjacent nodes, the diagonal should be set to 1. In the scenario of citation networks, the diagonal should be set to 1, as a paper is likely to reference and build upon its own findings throughout. Setting the diagonal to 0 is akin to attempting to predict the representation of the node based only on its adjacent nodes, which is not the case in citation networks. If an adjacency matrix is symmetric around it's diagonal, then the graph is undirected, otherwise it is directed (i.e. $U$ is connected to $V$ and $V$ is connected to $U$). In citation networks, this is not the case, as because paper A cites paper B, it does not mean the reverse is true.    

  Consider the same three research papers with the following adjacency matrix:
  $A = \begin{bmatrix}
  0 & 1 & 0 \\
  0 & 0 & 1 \\
  1 & 0 & 0
  \end{bmatrix}$
  Which represents the following graph:

  \begin{center}
    \begin{tikzpicture}[
        > = {Stealth[scale=1.2]},
        vertex/.style = {circle, draw, minimum size=1.2cm, inner sep=1pt},
        ref_edge/.style = {->, thick, blue},
        label_style/.style = {font=\small}
    ]

    \node[vertex] (Smith) at (0,0) {Smith};
    \node[vertex] (Johnson) at (2,0) {Johnson};
    \node[vertex] (Zhang) at (1,2) {Zhang};

    \draw[ref_edge] (Smith) -- (Johnson);
    \draw[ref_edge] (Zhang) -- (Johnson);
    \draw[ref_edge] (Smith) -- (Zhang);
    \end{tikzpicture}
    \end{center}

    With both $X$ and $A$ defined, we can numerically represent the graph. The node feature matrix $X$ is the initial/input node features, with our goal for learning on graphs to learn node embeddings $H \in \mathbb{R}^{N\times D}$ where $D$ is a chosen hidden dimension size. 


   
\end{tcolorbox}

\begin{tcolorbox}[title=Message Passing Neural Networks in \gls*{gnn}s]  
\small
    CNNs, designed for grid-structured data like images, rely on fixed-size convolutional filters to learn spatial invariance. However, graphs present a challenge. Their variable size and non-grid structure mean these filters, effective for image grids, are not directly applicable. Therefore, Message Passing Neural Networks (MPNNs) were developed. MPNNs address this limitation by using message passing, a graph-native approach, to learn representations from the irregular structure of graphs, effectively achieving a form of spatial (or rather, structural) invariance suitable for graph data.

    The process is defined as follows:
    \begin{itemize}
        \item {\bf{Message:}} every node decides how to send information to neighbouring nodes it is connected to by edges
        \item {\bf{Aggregate:}} nodes receive messages from all their neighbours, who also pass messages and decide how to combine the information from all of their neighbours. 
        \item {\bf{Update:}} each node decides how to combine neighbourhood information with its own information and updates its embedding for the next time step. 
    \end{itemize}

    By doing this, nodes pass each other information and disseminate information around the graph, allowing the network to learn spatial invariance.  This can be repeated for a fixed number of iterations ($k$), with the larger the value of $k$, the more the more diffuse the information around the graph becomes.

    The MPNN process can be further clarified:

    \begin{itemize}
        \item {\bf{Message:}} The source node $U$ will pass a message $m_{uv}$ to the destination node $V$. The message depends on the \gls*{gnn} architecture, with the easiest example message being passed being $U$ node's feature $h_u$ vector to $V$.
        \item {\bf{Aggregate: }} The destination node $V$ will receive messages from all its neighbouring nodes and needs to decide how to combine the information from all of its neighbours. This is typically done using a sum, average or max pooling of the messages from all neighbouring nodes. It is important that the aggregation function has to be a permutation invariant function, as the order of the messages should not affect the output. This gives us a combined neighbourhood node embedding, denoted as $h_{N(V)}$, where $N(V)$ is the set of all neighbouring nodes to $V$, meaning all nodes connected to $V$ by an edge. $h_{N(v)}^{k+1} = AGGREGATE({h_u^k, \forall u \in N(v)})$
        \item {\bf{Update: }} Each node updates its own embedding based on the combined neighbourhood embedding and its own embedding from the previous timestep. $h_v^{k+1} = \sigma(W \cdot CONCAT(h_v^k, h_{N(v)}^{k+1}))$
    \end{itemize}

\end{tcolorbox}

\subsection{Graph neural networks for document classification}

Modelling document relationships as citations allows us to leverage a homogeneous graph structure. Each node represents a research paper, and edges depict relationships between these papers, such as citations or co-authorship. This structure results in a uniform type for both nodes and edges.

\gls*{gnn}s offer two primary learning paradigms for node prediction: transductive and inductive learning. Transductive learning leverages the entire graph structure during training to infer labels for unlabelled nodes within that same graph. Conversely, inductive learning trains on a subset of the graph, enabling it to generalise and predict labels for previously unseen documents. Transductive \gls*{gnn}s can be particularly useful for systematic review screening because they leverage the relationships between documents within a fixed set to enhance the screening process. Similarly, inductive \gls*{gnn}s facilitate automated screening of new papers, improving efficiency and scalability and providing a strategy to integrate these new documents into the graph, potentially lending themselves more to a systematic review update. Given the temporal nature of research publications, where papers cannot cite each other bidirectionally, a directed (i.e., edges explicitly flow from one node to another) graph models these relationships appropriately. Furthermore, since the larger pool of documents remains constant during the screening phase, the graph is static (i.e., not evolving).


\paragraph{\gls*{gnn} document representation approaches: }

Researchers have achieved \gls*{gnn} document representation via different approaches. For example, Yao et al. proposed a transductive approach called Text gls*{gcn} \cite{yao_graph_2018}, which constructs a single, global graph representing the entire corpus. In this global graph, nodes represent both words and documents, and edges reflect word co-occurrence across the corpus and document-word relationships using methods like TF-IDF. While this approach effectively captures global word co-occurrence, it struggles with unseen documents and prioritises corpus-level relationships over document-level contextual nuances. Later, Zhang et al. built individual graphs for each document and then used a GNN to learn the fine-grained word representations based on their logical structures \cite{zhang_evaluating_2020}. They construct these graphs using a sliding window inside each document, allowing them to capture contextual word relationships. Creating individual graphs for each document also enables inductive learning of new words, overcoming another limitation of previous approaches. GNN document representation was advanced further with the conTextING approach by combining the per-document graph with BERT's contextual embeddings \cite{huang_contexting_2022}. More recently, Piao et al. proposed a novel method called Sparse Structure Learning \cite{piao_sparse_2022}, which builds upon these prior works by creating trainable, individual, and sparse graphs for each document. This method starts with sentence-level subgraphs and then learns to connect them sparsely, capturing both local syntactic and global semantic information while dynamically adjusting to the specific structure of each document. These four papers highlight a progression in \gls*{gnn}-based text classification: from leveraging global, corpus-wide information via a single, large graph (Text gls*{gcn}) to focusing on local, document-specific information through individual graphs and inductive learning (TextING/conTextING) and finally to learning dynamic, sparse graph structures tailored to each document (Sparse Structure Learning). However, these approaches primarily focus on textual features. This research, in contrast, will investigate integrating structural features (citations) and document metadata to enhance \gls*{gnn}-based document classification further.

\paragraph{Node Classification in \gls*{gnn}s: }

\gls*{gnn}s can operate at the node level to classify documents \cite{kipf_semi-supervised_2017, yao_graph_2018, wang_graph_2024, rong_dropedge_2020}. This means they can classify individual documents (nodes) based on their content and connections within the graph, potentially enhancing document representation and information integration. Node-level classification is particularly relevant when dealing with unlabelled data, which aligns with the active learning premise of this PhD.

% leveraging citation and metadata
Research specifically on document classification using \gls*{gnn}s demonstrates their performance. Ly et al. classified research output into context-specific categories by enriching a simple \gls*{gnn} with representations that encode references, co-authorships, shared publication sources, and subject headings as distinct edge types \cite{ly_article_2024}. They demonstrated that using multi-graphs (i.e. more than one edge allowed between nodes, each denoting a different type of relationship) to encode this data consistently improved the performance of various \gls*{gnn} models on a PubMed diabetes dataset. Zhang et al.'s work used on medical bibliographic classification task demonstrated increased performance when utilising a novel Transformer-based heterogeneous \gls*{gnn}, namely Text Graph Transformer (TG-Transformer), which captures structure and heterogeneity from the text graph and is scalable to large-sized corpora due to a mini-batch text graph sampling method \cite{zhang_evaluating_2020}. This method significantly reduces computing and memory costs, making TG-Transformer outperform state-of-the-art approaches on text classification tasks. The authors also found that modelling heterogeneity information and including structural encodings and pre-trained word embeddings further improved performance.

All this research supports the premise that representing a document less shallowly (i.e., with a \gls*{gnn} architecture) results in improved classification performance. Better representation is not an end in itself but a means to reduce screening loads and ensure near-total recall in a fraction of the time.

\paragraph{Active learning with \gls*{gnn}s: }

Work on using \gls*{gnn} node classification in an active learning setting has been reviewed \cite{madhawa_active_2020}. Katsimpras proposed a framework that first uses active learning to select highly uncertain unlabelled nodes to be labelled and included in the training dataset \cite{katsimpras_improving_2024}. While their approach is not exactly a \gls*{cal} approach (they created pseudo labels to augment training with \emph{uncertain} data points), it demonstrates that active learning can be used. Building upon this, the recent FICOM (Feature Influence Diversified COverage Maximization) paper by Zhang et al. introduces a novel active learning framework specifically designed for \gls*{gnn}s in semi-supervised node classification tasks \cite{zhang_ficom_2024}. FICOM addresses key challenges in this area, such as scalability to large graphs and the need for effective node selection criteria. The authors proposed a unique approach combining Personalized PageRank coverage to capture node importance in the graph structure with embedding diversity to account for feature similarity. Both of these researchers did not use a \gls*{cal} approach.

Another approach to integrating active learning with \gls*{gnn}s was generating a node representation of a document through a gls*{gcn}, passing messages, and selecting the most informative nodes for labelling \cite{wu_active_2021}. This representation was then used for k-medoids clustering, with a cluster's label centre being queried. This allows for choosing the most representative nodes within a cluster, reflecting the diversity of the document collection. The intuition behind this method is that nodes closest to the cluster centre are likely to be more informative and contribute significantly to the learning process of the \gls*{gnn}. By selecting these central nodes, less annotation cost was achieved for better performance. This research did not use a \gls*{cal} approach (as it set $k$ for message passing to the budget), so it did not align strongly with the systematic review process. This research shows two things: 1) document representations through a \gls*{gnn} architecture can be performant, and 2) \gls*{gnn} use in active learning shows potential for reducing annotation cost.

\paragraph{Prompt engineering: }

\gls*{llm}s can be strategically integrated as enhancers within this framework to enrich document representations further and enhance the capabilities of \gls*{gnn}s. As outlined, citation network graph analysis occurs above the document level by utilising extracted features about documents. \gls*{llm}s are a natural replacement for feature extraction, as they possess the increased ability to understand the semantic meaning of documents. The ultimate goal of using \gls*{llm}s and graph networks is to complement and enhance issues with each other. 

Research has been conducted into using \gls*{llm}s within the \gls*{gnn}s and has developed a robust taxonomy for categorising \gls*{llm}s \cite{ren_survey_2024}. A promising application of \gls*{llm}s within graph networks is to use \gls*{llm} as an enhancer. \gls*{llm}s can encode text into nodes using more complex features, such as semantic meaning, which can be used within the \gls*{gnn}s~\cite{zolnai-lucas_stage_2024}.
    
Explanation-based enhancers query an \gls*{llm} using prompting to capture higher-level features about documents, which is used to enrich node representations before processing with a \gls*{gnn}, with the process being abstracted in Figure \ref{fig:llm4g}. He et al. used the approach that prompted GPT 3.5 with a document's abstract and text and questions about that document using a zero-shot approach~\cite{he_harnessing_2024}. The \gls*{llm} response then forms features that are amended to the original node representations. This approach requires domain-specific knowledge, as features deemed important (and hence prompt used) depend on the research domain. It was performant in the PubMed domain, scoring greater node classification accuracy using this approach. These results suggest that \gls*{llm}-based feature enrichment can significantly improve the performance of \gls*{gnn}s on document classification tasks.

\begin{figure}[h]
    \centering
    \begin{tikzpicture}[node distance=3cm,auto]
        
        % Define styles
    \tikzstyle{node} = [circle, minimum width=2.5cm, minimum height=2.5cm, draw=black!50, thick]
    \tikzstyle{process} = [rectangle, rounded corners, minimum width=2cm, minimum height=1cm, draw=black!50, thick]
    \tikzstyle{features} = [text width=2cm, align=left]
        
        % Initial node
    \node[node] (initial) {
            \begin{tabular}{c}
                Initial Node\\[2pt]
                h\textsubscript{orig}\\[2pt]
                \scriptsize{[Title \& Abstract]}
            \end{tabular}
        };
        
        % Process box
    \node[process, right of=initial, xshift=3cm] (process) {
            \begin{tabular}{c}
               \gls*{llm} \\Prompting
            \end{tabular}
        };
        
        % Enriched node
    \node[node, right of=process, xshift=4cm] (enriched) {
            \begin{tabular}{c}
                Enriched Node\\[2pt]
                [h\textsubscript{orig}, h\textsubscript{pred}, h\textsubscript{expl}]\\[2pt]
                \scriptsize{[combined features]}
            \end{tabular}
        };
        
        % Arrows
    \draw[->, thick] (initial) -- (process) node[midway, above] {text input};
    \draw[->, thick] (process) -- (enriched) node[midway, above] {enriched features};
        

        
    \end{tikzpicture}
    \caption{Node feature enrichment process using an \gls*{llm}.}
    \label{fig:llm4g}
    \end{figure}

\paragraph{Screening prioritisation with active Learning using \gls*{gnn}s:} While research explores \gls*{gnn}s in active learning settings, a significant gap in the literature remains: the application of \gls*{gnn}s within a \gls*{cal} framework, specifically designed for the unique demands of systematic review document screening and leveraging the rich information available in metadata and citation networks. This PhD research will directly address this gap. 


\subsection{Conclusion}

A critical shortcoming in current screening approaches for systematic reviews is their tendency to treat each document as an isolated text. Commonly, only the title and abstract are used to prioritise documents for manual screening, neglecting the rich metadata available in bibliographic databases (e.g., publication year, venue, or author information) and the structures inherent in citation networks. Traditional methods like \gls*{bcs} and \gls*{fcs} help identify relevant studies by tracing references across papers, but they remain limited to direct citations. Consequently, they overlook deeper or indirect relationships—such as shared co‐authors, research topics, or citation ``chains"—that could reveal additional high‐value documents.

\gls*{gnn} models offer a way to exploit these richer inter‐document relationships. \gls*{gnn}s can treat each document as a node enriched with metadata and connected by edges (citations or co‐authorship links), enabling ``message‐passing" that propagates relevance signals across the network. Incorporating metadata and citation information in this way has shown promise for improving accuracy and reducing screening efforts. Although GNN‐based classification has succeeded in analogous tasks, few—if any—published approaches integrate \gls*{gnn}s into a \gls*{cal} workflow specifically for medical systematic reviews. Two methods could be explored: 1) How using \gls*{gnn} document representations (along with metadata) can improve the encoder \gls*{cal} process, and 2) How to utilise a \gls*{gnn} to recommend articles for screening based on a dynamic update of the \gls*{gnn}'s understanding of document relevance based on an oracle updating documents within it. 

The PhD is driven by the need to minimise expert screening time while safeguarding near‐total recall in systematic reviews. Existing \gls*{cal} pipelines already reduce effort relative to manual screening but still rely on shallow document representations. By systematically integrating metadata (e.g., publication date, journal status) and citation networks (e.g., references, co‐authors) into \gls*{cal}, this PhD aims to rank and prioritise relevant documents more intelligently. Doing so could surface key studies earlier—allowing reviewers to find the needed evidence faster.

\section{Stopping algorithms}\label{sec:Stopping_algorithms}

Once a prioritisation strategy effectively ranks potentially relevant studies, knowing when to stop remains essential for reducing screening costs in systematic reviews. Even the best prioritisation scheme is of limited use if the entire pool is screened, so these stopping approaches strive to balance cost against retrieval. In an idealised scenario, if the total number of relevant documents is known ahead of time and used to guide stopping with a target recall of 100\%, system efficiency depends solely on how well documents are ranked. By contrast, setting the target recall below 100\% now allows efficiency to be dictated by the ranking \emph{and} the stopping method. In practice, 100\% recall is not required to answer the search question; therefore, efficiency improvements can come from improving screening prioritisation \emph{and} stopping algorithms. Many systematic review approaches in \gls*{tar} adopt near-total recall thresholds (e.g., 95\%). Traditional approaches to stopping assume the goal is total coverage of relevant items. Yet in reality, humans often stop earlier, guided by factors such as time constraints, perceived sufficiency of information, or intuitive notions of diminishing returns \cite{ilani_analysis_2024, browne_cognitive_2007, wu_online_2014}.

User studies in broader IR contexts confirm that stopping decisions are multifaceted, shaped by both cognitive and environmental factors \cite{ilani_analysis_2024}. Cognitive factors include the searcher’s sense of ``information sufficiency", personal experience, and tolerance for ``bad" or irrelevant information. Environmental factors include the nature of the task (e.g., urgency or complexity), time or resource constraints, and the retrieval system's ability to yield relevant evidence \cite{prabha_what_2007}. Although systematic reviews involve multiple screeners and formal protocols, these human tendencies still manifest in practice: team members inevitably weigh whether additional screening is likely to yield new insights or simply re-confirm existing conclusions.

Multiple approaches to stopping methods exist and span various domains (e.g., eDiscovery \cite{yang_heuristic_2021, yang_minimizing_2021}, medical systematic reviewing \cite{shemilt_pinpointing_2014}, software engineering systematic reviewing \cite{yu_fast2_2019}, and environmental health \cite{howard_swift-active_2020}). Below, stopping approaches are categorised into two broad groups—those that estimate the total number of relevant studies $R$ and those that do not — and then highlight how recent research on stopping heuristics and ``information sufficiency" can refine or replace conventional recall-based rules, as outlined by ~\cite{stevenson_stopping_2023}.

\paragraph{Approaches that estimate $R$:} A traditional way to automate stopping methods is to calculate the total number of relevant documents, $R$, in the unlabelled pool and halt once a chosen fraction (e.g., 95\%) has been identified. This can be done via:

\textbf{Sampling methods} estimate $R$ by taking a random sample at the start of the process and stopping when this number or a proportion of it has been reached. Shemilt et al. used sample size calculations to estimate relevant documents within a defined confidence level~\cite{shemilt_pinpointing_2014}, while Howard et al. modelled the remaining relevant documents using the Negative Binomial distribution~\cite{howard_swift-active_2020}. Callaghan and Muller-Hansen improved this by using hypergeometric distribution and statistical testing to create a statistically grounded stopping rule~\cite{callaghan_statistical_2020}. Other methods, such as S-CAL and Autostop, leverage nonuniform sampling to minimise document examination. S-CAL uses stratified sampling with decreasing inclusion probability, using a classifier for initial ranking and batch-wise relevance estimation~\cite{cormack_scalability_2016}. Autostop employs Horovitz-Thompson and Hansen-Huruwitz estimators, accounting for decreasing relevance probability, to produce unbiased R estimates and use these and their variance to determine when to stop the review process~\cite{li_when_2020}. These estimators rely on appropriate sampling probability distributions, with the AP-Prior distribution identified as a strong performer.

\textbf{Classification-based methods} estimate $R$ on unlabelled data using a trained classifier model from labelled data. These approaches are commonly applied in an active learning framework. Yu and Menzies added `temporary labels` to the unexamined documents, trained a classifier and used that classifier to estimate $R$ in the unlabelled pool~\cite{yu_fast2_2019}. Yang et al. similarly trained a classifier to predict $R$~\cite{yang_heuristic_2021}. These approaches are not ideal, as the prevalence of documents in the labelled pool might not match the prevalence of the documents in the unlabelled pool~\cite{del_coz_learning_2021}. Indeed, this difference is preferable in a \gls*{cal} approach: relevant documents are first labelled in an ideal screening prioritisation system under the \gls*{cal} approach. 

Methods for estimating $R$ can also leverage the rankings produced by algorithms.  One approach models the distribution of relevant documents as a Gaussian random variable, calculating the probability of a document's relevance to determine if a recall target has been met \cite{hollmann_ranking_2017}. Another similar technique models the scores as a normal distribution, using the area under the curve to estimate $R$ \cite{cormack_machine_2009}.

Recent work by Stevenson et al.~\cite{stevenson_stopping_2023} used \textbf{Point Processes} — specifically using Inhomogeneous Poisson processes to model the occurrence of relevant documents in a ranked list as a counting process, allowing a more dynamic estimation of the remaining relevant documents. 

\paragraph{Approaches that do not estimate $R$: }  Not all stopping criteria attempt to model the entire pool. Several practical alternatives exist:

A simple yet effective method for determining when to stop document screening involves halting the process after \textbf{ observing consecutive irrelevant documents.}  This strategy capitalises on the fact that relevant documents are often clustered within the screening order rather than uniformly distributed.  For example, Ros et al.\cite{ros_machine_2017} stopped screening after encountering 50 consecutive irrelevant documents.  Other variations on this approach include stopping after a predetermined number of documents have been examined, after a fixed number of relevant or irrelevant documents have been seen, or after a specific sequence of non-relevant documents is observed \cite{losada_when_2019}. Empirically, humans do something similar, sometimes calling it ``frustration" or ``disgust" stopping, in which encountering too many low-yield items signals a reduced chance of future gains \cite{cooper_selecting_1973, kraft_stopping_1979, ilani_analysis_2024}.

The \textbf{knee method} exploits the point at which the curve of a graph visibly bends from a high to a low slope, indicating where diminishing returns between the trade-off between two variables begin. Again, this is only possible because of the non-uniform distribution of relevant documents within the pool. This approach requires the calculation of $P$, which is the current gradient divided by the previous gradient. A hyperparameter is subsequently needed to determine when to stop retrieval (i.e., stop when $P < X$). Multiple researchers have used this approach \cite{cormack_engineering_2016, li_when_2020}, yet there is no consensus on a suitable hyperparameter.  

\textbf{Target methods} aim to ensure a specific recall level is achieved with a given confidence.  These methods begin by randomly selecting a small set of documents (the "target set") from the larger pool and returning them to the pool.  Screening continues until this target set is identified, and then the process stops.  The size of the initial random sample is determined statistically; for instance, Cormack et al. \cite{cormack_engineering_2016} suggest that a sample of 10 documents is sufficient to guarantee a 0.7 recall with 95\% confidence.  A variant, the Quantile Binomial Coefficient Bound, addresses the limitations of sequential testing  \cite{lewis_certifying_2021} but requires a suitable control set.

An emerging line of research uses \textbf{Reinforcement Learning} to decide when to stop. RLStop frames the stopping decision as a sequential decision-making process, where a learning agent processes a ranked list of documents in batches and receives feedback on their choices~\cite{bin-hezam_rlstop_2024}. RLStop utilises Proximal Policy Optimisation to train a neural network that approximates the optimal policy. By avoiding an explicit estimation of $R$, RLStop circumvents challenges associated with modelling the non-uniform distribution of relevant documents in ranked lists. Instead, it dynamically adapts its stopping decision based on the observed pattern of relevance, making it a robust alternative in diverse \gls*{tar} settings.

While these approaches may not attempt to estimate $R$, they still do not consider the utility of the information within the document, such as how it might contribute to answering the reason behind the search. Moreover, all stopping methods (those which estimate $R$ or not) cannot be considered if adding more relevant documents genuinely alters a conclusion. 

Importantly, all such approaches assume a binary notion of relevance and aim for a ``coverage" metric—mirroring the classical impetus in systematic reviews for total recall. However, evidence from user-behaviour research underscores that real searchers also weigh whether additional studies are qualitatively altering conclusions or simply repeating known points \cite{ilani_analysis_2024, browne_stopping_2005}.

Utility refers to the value or usefulness of a document or piece of information to an information seeker in satisfying their specific information need. It goes beyond relevance by considering how the information helps the user achieve their goal or complete a task. Utility is subjective and context-dependent, meaning that the same document can have different utility for different users or even for the same user in different situations. As utility is subjective, understanding the behaviour, needs and requirements of the information seeker is needed to realise any stopping method based on utility. 

Each of these above stopping methods can interpreted as a form of ``satisfaction" or ``frustration" rule \cite{ilani_analysis_2024, cooper_selecting_1973}: do we have enough relevant items (satisfaction) or have we encountered too many misses in a row (frustration)?

\paragraph{Incorporating User and Utility Factors: }

While the approaches above suit many retrieval tasks, a significant body of research—both within information science and in broader domains of user behaviour—shows that people use complex heuristics to judge the utility or sufficiency of additional information \cite{nickles_judgment-based_1995, browne_stopping_2005, ilani_analysis_2024}. Research on the analysis of user behaviour during search tasks, including when they decide to stop search sessions, suggests that users are motivated by the following cognitive and environmental factors when deciding when to stop searching \cite{ilani_analysis_2024}:

\begin{itemize}
    \item \emph{Information Sufficiency}. Information sufficiency is a cognitive factor of the feeling that the information being obtained so far is ``good enough" to motivate the decision to stop searching \cite{cooper_selecting_1973,prabha_what_2007,zach_when_2005,dostert_users_2009}. This notion is used by Historians when researching, reporting that ``when they feel they have sufficient information to write research, they stop (halt to a research study), even if other references promise additional information"~\cite{dalton_historians_2004}.
    Early examples of such rules were the \emph{satisfaction} rule, which stopped when a fixed number of relevant documents have been encountered, and the frustration rule, where the search is stopped after a fixed number of irrelevant documents have been seen~\cite{cooper_selecting_1973}. Kraft et al. extended this by adding a combined satisfaction and frustration rule~\cite{kraft_stopping_1979}. Specifically, Kraft and Lee termed these the ``satiation" (akin to satisfaction) and ``disgust" (akin to frustration) rules. Other research looks at satisficing (e.g., achieving an adequate or satisfactory result rather than an optimal one)~\cite{simon_behavioral_1955}. Sufficiency around stopping rules can be divided into two categories: judgement and reason \cite{nickles_judgment-based_1995, maxwell_modelling_2021}. A mental threshold is set in judgment-based stopping rules; a running total is kept for that threshold. When this running total exceeds the threshold, retrieval is stopped. In reason-based stopping rules, evidence is gathered, and arguments are made against the available evidence. In studies on stopping behaviour, one’s sense of information sufficiency can be due to collecting good information, tolerating bad information, and/or both~\cite{wu_how_2013}. Many stopping rules or heuristics were created based on the above two categories, with some examples listed below:

   \begin{itemize}
    \item \textbf{Satisfaction, judgement-based rule:} e.g., search until a certain number of relevant documents have been encountered.
    \item \textbf{Single criterion rule, judgement-based rule:} e.g., search until sufficient information about a particular criterion has been satisfied.
    \item \textbf{Representational stability rule, reason-based:} e.g., search until a mental representation of the problem does not change.
    \item \textbf{Mental list rule, reason-based:} e.g., stop when a redefined list of items is verified from the search process.
   \end{itemize}

    These stopping rules and the broader concept of information sufficiency have been observed across various professions and contexts. Studies have examined stopping behaviours in students~\cite{browne_stopping_2005, browne_cognitive_2007, dedema_examination_2019, nickles_judgment-based_1995, creighton_university_cognitive_2017, gerhart_generalizing_2020}, public sector policymakers~\cite{berryman_what_2006}, system analysts~\cite{pitts_stopping_2004}, auditors~\cite{poziemski_when_2019} and investors~\cite{pennington_how_2016}. They have, however, not been applied to systematic reviews.
 
    \item \emph{Experiences.} Having experience has been demonstrated to affect stopping behaviour. Experienced analysts used magnitude threshold and mental list rules, while inexperienced analysts used a difference threshold~\cite{pitts_stopping_2004}. With investors, having more experience resulted in more use of absolute stopping rules (i.e., stop after obtaining a certain amount of information)~\cite{pennington_how_2016}. Inexperienced investors tended to use a single criterion rule.

    \item \emph{Feelings.} Belief in finding satisfactory documents to answer an information need affects stopping behaviour~\cite{wu_online_2014}. Kantor believed that the belief that there were enough documents to answer the question resulted in them screening more documents and stopping later, regardless of whether there were enough documents~\cite{kantor_model_1987}. External pressure, such as time deadlines, tended to cause people to retrieve fewer documents.

\end{itemize}

\paragraph{Utility-Based Approaches: } 
Therefore, broader research into stopping methods indicates other more intuitive approaches that closely match humans' approach outside of fixed judgement-based rules traditionally employed. Rather than forcing a fixed recall threshold, these methods capture information utility: they watch how newly included studies shift an aggregated result (such as effect size) or a summary conclusion (such as whether a new risk factor emerges). Some approaches draw on the concept of ``representational stability" \cite{nickles_judgment-based_1995}—if adding more studies does not alter the meta-analysis outcome or the qualitative synthesis, the process can stop \cite{ilani_analysis_2024}.

Utility-based stopping is highly relevant in medical systematic reviews, where not all relevant studies have the same clinical impact. Indeed, an entire cluster of small or repetitive RCTs may add negligible value once multiple large trials are already included. Stopping methods for systematic review screening have historically assumed a rigid requirement of near-total recall. While viable, these approaches can prolong screening even after securing the most relevant information. Novel approaches—whether user-focused, cognitively informed, or utility-based—offer more nuanced trade-offs, echoing real user behaviours studied in the broader IR literature \cite{browne_stopping_2005, ilani_analysis_2024}. Addressing when to stop is as crucial as how to rank: improvements in either dimension can significantly reduce the human burden of systematic reviews while retaining confidence that key evidence is not overlooked.

\section{Datasets}\label{sec:datasets}

Numerous data sets relate to screening and stopping methods, all of which have been used in the reviewed literature. Datasets were chosen based on whether they were related to medical research in some way, if relevancy judgements were available for each piece of research and if a meaningful structural relationship could be formed between pieces of research using the OpenAlex API\footnote{https://openalex.org/}. Notably excluded datasets include TREC 2015/2016 Total Recall~\cite{roegiest_trec_2015, grossman_trec_2016}, Jeb Bush Email\footnote{https://ab21www.s3.amazonaws.com/JebBushEmails-Text.7z} and RCV1-v2~\cite{lewis_rcv1_2004}, all of which did not contain medical research.

\subsection{CLEF-TAR (2017, 2018, 2019)}

CLEF-TAR is a dataset that was released as part of CLEF eTASK 2 and is available on github\footnote{https://github.com/CLEF-TAR/tar} \cite{kanoulas_clef_2017, kanoulas_clef_2018, kanoulas_clef_2019}. Originally designed with document ranking as the primary focus, the information contained within the data set allows for the subprocess simulation of the title and abstract selection of the systematic review procedure, using published real-world Cochrane systematic reviews. Each year, this data set was incrementally updated. Table \ref{tab:training_dataset_clef} outlines the ``needle in a haystack" issue with these datasets and succinctly highlights the presence of a large imbalance of the TIR class. Diagnostic test accuracy systematic reviews summarise a test accuracy, while intervention reviews assess the effectiveness/safety of a treatment, vaccine, device, preventative measure, procedure, or policy.

\begin{table*}
    \centering
    \begin{tabular}{|c|c|c|c|c|c|}
    \hline
        Dataset & Total systematic reviews & Type(s) of systematic review & T & TR & TR/T\\   \hline
        CLEF 2017 & 50 & DTA & 269628 & 4661  & 0.017 \\   \hline
        CLEF 2018 & 30 & DTA & 266657 & 4351 & 0.016\\   \hline
        CLEF   2019 & 8 & DTA & 485153 & 8315 & 0.017\\   \hline
        CLEF  2019 & 40 & Intervention & 31644 &  448 & 0.014 \\   \hline
        Synergy & 26 & Mixed & 169288 &  2834 & 0.017 \\   \hline
    \end{tabular}
    \caption{Training Dataset sizes for the TAR datasets}
    \label{tab:training_dataset_clef}
\end{table*}

Presumably, due to copyright laws, the CLEF dataset did not provide the titles or abstracts for each research found in the Identification Phase; rather, it relied on the users to download them for experimentation. This is an important oversight of the data set as titles and abstracts \emph{can} be updated or retracted post-publication, meaning fair comparison across time might become increasingly challenging. To combat this effect and allow more meaningful comparison to their work, this PhD will use a recently collected set \(2024\) of titles/abstracts created by Mao et al. for the CLEF dataset, \cite{mao_reproducibility_2024}\footnote{https://github.com/ielab/goldilocks-reproduce}.

\subsection{Synergy dataset}
The Synergy dataset \cite{de_bruin_synergy_2023}, while less frequently used in the literature, offers a more contemporary collection of systematic reviews\footnote{https://github.com/asystematic revieweview/synergy-dataset/tree/master}. This data set comprises 26 systematic reviews spanning multiple domains, focusing predominantly on the medical field (20 out of 26 reviews). Reviews included in this data set range from 2002 to 2022, providing more recent information than the CLEF data set.
The Synergy data set features diverse domains, allowing cross-domain analysis despite its primary focus on medical reviews. It also includes an expanded variable set. In addition to the basic information found in the CLEF dataset, Synergy incorporates authorship details, referenced works, and publication years, all sourced from the OpenAlex API.
Due to its more recent compilation, limited use in existing research, and cross-domain nature, this dataset will externally validate approaches. Including systematic reviews from non-medical domains, such as computer science, allows evaluations of the transferability of TAR approaches across different fields. Synergy's TR/T ratio of 0.017 is consistent with the class imbalance observed in the CLEF datasets, making it suitable for comparative studies and model evaluation in the context of title and abstract selection tasks.

% \subsubsection{TREC Total Recall Track Dataset (2015, 2016)}
% The TREC Total Recall Track produced data sets specifically designed for high-recall retrieval tasks, similar to those encountered in systematic reviews\footnote{https://trec.nist.gov/data/total-recall/}\cite{roegiest_trec_2015, grossman_trec_2016}. This data set simulates scenarios where the goal is to find all or nearly all relevant documents in a collection, which aligns closely with the objectives of the title and abstract screening phase in systematic reviews. The data set includes a corpus of documents, topics (which can be seen as analogous to research questions in systematic reviews), and relevance judgments.

% \subsubsection{Jeb Bush Emails Dataset}
% The Jeb Bush Emails dataset is an unconventional choice for TAR research, originally consisting of emails released by former Florida Governor Jeb Bush\footnote{https://ab21www.s3.amazonaws.com/JebBushEmails-Text.7z}. This data set is suitable for TAR experiments because of its large size and the presence of both relevant and irrelevant documents. Although not directly related to systematic reviews, it provides a real-world corpus that can be used to simulate document classification tasks inherent in the systematic review process.

% \subsubsection{RCV1-v2 Dataset}
% The RCV1-v2 (Reuters Corpus Volume 1, Version 2) is a large, manually categorised newswire data set \cite{lewis_rcv1_2004} that was published by Reuters between August 20, 1996, and August 19, 1997\footnote{https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/datasets/\_rcv1.py}. The dataset features 804,414 documents with multi-label classification across 103 topic categories, organised in a hierarchy. The documents are provided in XML format with rich metadata and the content is primarily English news stories covering a wide range of topics.
% Although not originally designed for systematic reviews, RCV1-v2 has been used in various text classification and information retrieval tasks.  In the context of systematic reviews and TAR, the use of the RCV1-v2 data set lies in the simulation of approaches on a large-scale dataset to test the scalability and efficiency of screening algorithms and to evaluate any potential transferability of the approaches.

% RCV1-v2 dataset is adapted for use in AL by denoting all documents as \textbf{$T$}, \textbf{$T_R$} as all documents having a specific label, and those without it, as by treating the entire corpus as \textbf{$T$}, \textbf{$T_{IR}$}, we can approximate the binary classification challenge of title and abstract Screening within systematic reviews.

\section{Evaluation metrics}
Evaluation metrics for systematic review \gls*{tar} process can be categorised between assessing how well a classifier minimised the relevant documents excluded by the classifier with a set work budget (i.e. effectiveness) or the reduction in the reviewer's workload by excluding the maximum number of irrelevant documents while maintaining recall (efficiency).  Most of the research produced within this will focus on improving the effectiveness of AL models within the medical \gls*{tar} domain. The author chooses not to optimise the computational efficiency between approaches but rather to improve the final result. This is for numerous reasons; however, the main two are that as computer processing increases, these practical limitation concerns become less and improvement in effectiveness will have a greater impact on systematic review usefulness than maximising efficiency.  The author aims to report the time taken to run the algorithms, time complexity, and hardware that ran upon them so that a comparison to the time taken by humans can occur.

\subsection{Recall@k}
In the context of systematic reviews, achieving high recall is more critical than high precision. \emph{Recall} represents the proportion of relevant documents correctly identified among all truly relevant documents \cite{omara-eves_using_2015}. This focus on recall may seem counterintuitive, but it is crucial for two reasons. First, each missed document could potentially contain significant information for the systematic review. Second, the initial screening is followed by a more precise full-text review (as outlined in the PRISMA workflow, Figure \ref{fig:prisma_flow}), where precision is emphasised.
Although maximising recall is important, it is not practical to aim for 100\% due to diminishing returns. As recall approaches higher levels, the computational cost of screening additional documents increases substantially, often yielding minimal benefit. To balance effectiveness and efficiency, researchers of \gls*{tar} for systematic review commonly use and consider recall @ 95\% as useful (that is, k = 0.95). This measure indicates the recovery achieved when 95\% relevant documents are recovered, striking a pragmatic balance between comprehensive coverage and resource use. A higher recall@k is considered a more effective approach.
Recall is outlined in Equation \ref{eq:recall}.

\begin{equation}
\text{recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}
\label{eq:recall}
\end{equation}
Recall@95\% is shown in Equation \ref{eq:recall_at_k}.
\begin{equation}
\text{recall@95\%} = \frac{\text{TP}}{\text{TP} + \text{FN}}
\label{eq:recall_at_k}
\end{equation}

Where recall is calculated once an AL classifier achieves 95\% TP.

A small point on nomenclature: Historically, recall has been referred to in the medical literature as sensitivity. These are two different terms for the same metric, and the use of either term depends on the domain. Additionally, in the legal domain, there might be references to recall@75\% which is not as useful for the medical domain. Legal domains often prioritise based on cost-effectiveness, time-constraints and proportionability, which medical reviews require as close to absolute information as possible \cite{tsafnat_systematic_2014}.

\subsection{R-Precision}
Given the \textbf{$T_R$}, this effectiveness metric determines what proportion of documents returned by the approach within the total number of relevant documents were actually relevant \cite{manning_introduction_2008}. The best score for R-precision is 1 (i.e., all relevant documents were returned in the top \textbf{$T_R$} position). It allows for an adaptive cutoff for \textbf{$T_R$}, which adapts to the systematic review, and also considers precision. Note that this evaluation metric can only be used when the \textbf{$T_R$} for a query is known and is outlined in Equation \ref{eq:r-precision} :

\begin{equation}
\text{R-Precision} = \frac{\text{Relevant Documents in top } T_R}{T_R}
\label{eq:r-precision}
\end{equation}
\subsection{Work saved over sampling (WSS@k)}

\emph{WSS@k} is an efficiency metric that would be valuable to report on to enable other researchers in the field to compare their approaches to mine and, if appropriate, improve upon \cite{kusa_analysis_2023, cohen_reducing_2006}. Again, k (recall) is typically set to 0.95. This metric evaluates the work saved over random sampling, with a higher WSS@k being more efficient, being described in the original paper as, \emph{``the percentage of papers that meet the original search criteria that the reviewers do not have to read (because they have been screened out by the classifier)."}

WSS@k is outlined in Equation~\ref{eq:wss}:


\begin{equation}
\text{WSS} = \frac{\text{TN} + \text{FN}}{\text{T} - (1 - \text{Recall})}
\label{eq:wss}
\end{equation}

This can also be expressed as in Equation~\ref{eq:wss_variant}:
\begin{equation}
\text{WSS} = \frac{\text{TN} + \text{FN}}{\text{T} - 1 + \frac{\text{TP}}{\text{TP} + \text{FN}}}
\label{eq:wss_variant}
\end{equation}

\subsection{Percentage of Relevant Documents (PRD)}


\emph{Percentage of Relevant Documents (PRD)} is an efficiency metric designed to complement (or sometimes replace) more conventional measures of ``work saved" (e.g., Document Retrieval Savings). It quantifies how many \emph{relevant} documents—out of the total relevant set—were actually screened in reaching the final stopping decision. PRD is outlined formally in Equation in \ref{eq:prd}.

\begin{equation}
\text{PRD} = \frac{\text{Number of relevant documents screened}}{\text{Total number of relevant documents}} * 100\%
\label{eq:prd}
\end{equation}

Typical ``work saved" metrics (e.g., WSS@\textit{k}) focus on the overall proportion of the \emph{entire} document pool that was \emph{not} examined before stopping. While these can be informative, they sometimes penalise scenarios where the best decision (from a utility perspective) is to examine most or all of the collection — if, for instance, the system cannot be certain about its conclusion without reviewing nearly every document. In such cases, ``work saved" metrics may misleadingly report ``poor" savings when compared to recall-based approaches, even though reviewing all (or most) of the corpus \emph{was} the correct decision.

PRD focuses specifically on the \emph{relevant} documents that had to be examined (rather than all documents, relevant or not). This metric addresses two practical issues when comparing utility-based metrics to recall-based ones:

\begin{itemize}
    \item \textbf{Overly Large Pools.} If the total document pool is huge, a conventional ``work saved" metric might look unfavourable the moment the system decides to screen a large fraction of those documents. PRD counter-balances this by highlighting the fraction of \emph{relevant} items actually looked at. 
    \item \textbf{Reliance on screening prioritisation approach.} Stopping methods, in particular, are affected by how performant screening prioritisation is. Abstracting the irrelevant documents allows stopping methods to be considered without tangentially measuring the effectiveness of screening prioritisation.
\end{itemize}


\section{Conclusions}

\hl{TODO}
% Common themes running through literature, what at the gaps etc etc

\chapter{Project Overview}\label{sec:Project_Overview}

\section{Aim and hypothesis}

\begin{itemize}
    \item \textbf{Hypothesis 1:} Representing documents more richly during the \gls*{cal} process results in better performance.
    \item \textbf{Hypothesis 2:} Aligning stopping methods with information needs reduces document retrieval.
\end{itemize}

Hypothesis 1 is addressed via research questions 1 and 2, while hypothesis 2 is addressed by research question 3.

\begin{enumerate}
    \item \textbf{Research Question 1}: What is the impact of integrating backwards and forward citation network analysis on the performance of continuous active learning models for medical systematic review screening?
    \item \textbf{Research Question 2}: How does the use of \gls*{gnn} architectures that integrate article metadata (e.g., publication date, author information, citation counts) affect the efficiency and accuracy of document prioritisation in continuous active learning for systematic reviews, relative to traditional text-only representations?
    \item \textbf{Research Question 3}: How do stopping criteria based on the utility of retrieved information (aligned with user information needs) influence the total manual screening workload in continuous active learning for systematic reviews, and how do these criteria compare with conventional recall-based stopping rules?
\end{enumerate}

\section{Research proposals}\label{sec:research_proposals}

\subsection{Question 1: Leverage citation networks for continuous active learning}

\emph{What is the impact of integrating backwards and forward citation network analysis on the performance of continuous active learning models for medical systematic review screening?}

The aim is to determine whether enriching the initial training set with documents identified through citation networks leads to higher recall rates, improved R-Precision, and reduced overall human screening workload. A search of relevant literature (see Figure \ref{fig:search-results}) demonstrates that neither \gls*{bcs} nor \gls*{fcs} has been integrated into \gls*{cal} processes, underscoring the novelty and potential impact of the proposed approaches.


\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}[
        node distance = 0.5cm,
        box/.style={rectangle, draw, text width=4cm, minimum height=1cm, align=center},
        query/.style={rectangle, draw, fill=gray!10, text width=4cm, minimum height=2.5cm, align=center}
    ]
        % Search queries side by side
        \node[query] (q1) {Search Query 1:\\
            (``backward citation'' OR\\``citation analysis'')\\
            AND ``active learning''};
        \node[query] (q2) [right=0.5cm of q1] {Search Query 2:\\
            (``citation graph'' OR\\``citation network'') AND\\(``active learning'' OR\\``interactive learning'')};
        \node[query] (q3) [right=0.5cm of q2] {Search Query 3:\\
            (``bibliometric analysis'' OR\\``reference mining'') AND\\(``selective sampling'' OR\\``query learning'')};
            
        % Results box below
        \node[box] (results) [below=1cm of q2] {No Related Documents Found};
            
        % Arrows
        \draw[-stealth] (q1) -- (results);
        \draw[-stealth] (q2) -- (results);
        \draw[-stealth] (q3) -- (results);
        
    \end{tikzpicture}
    \caption{Results from search on arxiv and PubMed demonstrating the absence of related works, ran on 13th November 2024.}
    \label{fig:search-results}
\end{figure}


\paragraph{Experimental approach: } A comparative study involving two primary approaches will be conducted to address this gap. The first, termed Citation-Augmented CAL (CA-CAL), employs citation network analysis to expand the initial seed set used to train the \gls*{cal} model. In contrast, the Baseline \gls*{cal} (BE-CAL) approach relies on conventional methods, using only title and abstract features for the encoder-based CAL model; the authors have provided a full working code that this author has run on Sheffield's HPC successfully. Other comparators would be the Auto~TAR (B-CAL), and pure \gls*{fcs}/\gls*{bcs} search (FCS/BCS-SCREEN). The performance of these approaches will be evaluated on established datasets, including the CLEF-TAR (2017, 2018, 2019) and Synergy datasets.

The experimental design can incorporate several key variations to understand the impact of different parameters on \gls*{cal} performance:

\begin{itemize}
    \item \textbf{Seed Size:} Compare scenarios starting with a single seed document (randomly selected from known relevant documents) against those beginning with multiple seed documents (e.g., 3–5 relevant documents).
    \item \textbf{Citation Search Type:} Three variations will be assessed: using only \gls*{bcs} (CA-CAL-BCS), using only \gls*{fcs} (CA-CAL-FCS), and combining both (CA-CAL-BCS+FCS).
    \item \textbf{Citation Network Depth:} What is the effect of network expansion by including only direct citations (one hop away) versus extending to include citations of citations (two hops away).
\end{itemize}

The experimental procedure will begin with constructing citation networks using the OpenAlex API. For each known relevant document, both its cited (backwards) and citing (forward) documents will be retrieved. These documents will augment the initial seed set depending on the experimental condition. The expanded seed set will then be used to train an encoder-based \gls*{cal} model (leveraging BioLinkBERT), after which the \gls*{cal} process will be initiated. The model will rank the remaining unlabelled documents in each iteration, and a fixed batch (e.g., the top 25 documents) will be selected for manual labelling. This training, ranking, and labelling cycle will continue until a stopping criterion is met (such as achieving 95\% recall or labelling a predetermined number of documents, consistent with preliminary experiments). Existing hyperparameter choices (e.g., batch size, learning rates, fine-tuning epochs or active learning iterations) can be taken from the existing implementation or form part of the experimental research.

\paragraph{Evaluation: } Success will be measured through several performance metrics, including Recall@95\%, R-Precision, WSS@95\%. Statistical analyses (e.g., paired t-tests) will assess whether the observed differences between CA-CAL variations and the baseline approach are significant. To ensure a fair comparison of the total human screening burden, the number of documents examined during the initial citation-based pre-screening stage will be combined with the number of documents labelled by the active learning process when assessing the overall cost associated with each CA-CAL approach.

\paragraph{Expected outcome: } It is expected that CA-CAL variations will outperform the B-CAL approach because of the richer and more informative initial training sets. It is hypothesised that CA-CAL-BCS+FCS will be the most effective, and beginning with multiple seed documents will yield better results than starting with a single seed. Furthermore, denser citation networks are anticipated to correlate with improved performance, though the optimal network depth may reveal a trade-off between computational cost and screening benefits.


\paragraph{Potential limitations: } Potential limitations of this design lie in the data quality. The accuracy of the citation networks depends on the completeness and accuracy of data retrieved from OpenAlex. Additionally, while API access is available, utilising the available fixed database release to ensure consistency across experiments might be prudent. The challenge of identifying isolated nodes must be addressed (see Work Completed).

\paragraph{Work completed towards RQ1:}

\paragraph{Isolated nodes:}

A major limitation of the \gls*{bcs} and \gls*{fcs} citation network mining approach is its inability to identify indirect citation relationships. An indirect citation occurs when research papers are connected through intermediate references, forming a chain of citations rather than a direct reference. For instance, when document $D_i$ cites document $D_{ip1}$, which in turn cites document $D_{ip2}$, a relationship exists between $D_i$ and $D_{ip2}$ despite the absence of a direct citation. 

This relationship represents an indirect citation, which is shown in \ref{fig:indirect-citation}. This causes issues if $D_{ip1}$ is not included in the document pool, as $D_i$ and $D_{ip1}$ will no longer have an edge. Researchers have proposed several modifications to the citation network mining process to
address this limitation:

\begin{center}
    \begin{tikzpicture}[
        > = {Stealth[scale=1.2]},
        vertex/.style = {circle, draw, minimum size=1.2cm, inner sep=1pt},
        ref_edge/.style = {->, thick, blue},
        indirect_edge/.style = {->, thick, red, dashed},
        label_style/.style = {font=\small}
    ]
    
    % Main document Di
    \node[vertex, fill=yellow!20] (Di) at (0,0) {$D_i$};
    
    % Direct reference
    \node[vertex] (Dip1) at (-2,2) {$D_{ip1}$};
    
    % Indirect reference
    \node[vertex] (Dip2) at (2,4) {$D_{ip2}$};
    
    % Add edges
    \draw[ref_edge] (Di) -- (Dip1) node[midway, left] {Direct};
    \draw[ref_edge] (Dip1) -- (Dip2) node[midway, right] {Direct};
    \draw[indirect_edge] (Di) to[bend right] node[midway, right] {Indirect} (Dip2);
    
    % Time arrow (vertical)
    \draw[{Stealth[scale=1.5]}->, thick] (4,4.5) -- (4,-1);
    \node[label_style] at (4,4.8) {Past};
    \node[label_style] at (4,-1.3) {Future};
    
    % Legend
    \node[label_style] at (6,2) {Legend:};
    \draw[ref_edge] (5,1.5) -- (7,1.5) node[right] {Direct citation};
    \draw[indirect_edge] (5,1) -- (7,1) node[right] {Indirect citation};

    \end{tikzpicture}

    \label{fig:indirect-citation}
    \end{center}


\begin{itemize}
    \item \textbf{Matching isolated nodes based on similarity metric of their embeddings}: If $N$ is all the documents in the total pool, and $N_{isolated}$ is the set of documents that are not cited by any other document in $N$, then for each document $D_{ip} \in N_{isolated}$, find the document $D_i \in N$ with the highest similarity metric (i.e. cosine similarity) to $D_{ip}$. Add an artificial edge between $D_i$ and $D_{ip}$.
    \item \textbf{Matching isolated components on similarity metric of their embeddings}: Some small groups of documents (called isolated components) may be disconnected from the main cluster when analysing document clusters. These isolated components have fewer connections to other documents, which can reduce classification accuracy. To fix this:
    \begin{itemize}
        \item Identify isolated components $C_{isolated}$ that have fewer or equal nodes than the main cluster
        \item For each node in these isolated components
        \item Calculate a similarity metric (i.e. cosine similarity) to nodes in larger clusters $C_i$
        \item Connect it to the most similar large cluster by adding an artificial edge
    \end{itemize}

\end{itemize}

This constraint, however, doesn't make it unsuitable as a partial improvement to the encoder \gls*{cal} process for identifying relevant documents based on the initial seed document. Even without considering indirect citations, assessing the citation network of the seed document is potentially more appropriate than that of the entire document collection in the early screening stages.

\paragraph{Quantifying the \gls*{bcs} citation mining signal: }

A pilot experiment was designed to assess this claim. The relevance of cited works within a defined document pool by leveraging the OpenAlex API. For each document in the pool of the CLEF 2019 DTA dataset, the API was used to retrieve its references, and these references were filtered to include only those documents that were in the total pool. Then, three groups of ``seed” documents were sampled for comparative analysis: a positive group (up to 100 documents labelled as relevant), a negative group (up to 100 documents labelled as irrelevant), and a random group (100 documents selected at random from the total pool). The total count of its one-hop cited references marked as relevant or not relevant was calculated for each seed document. The counts of relevant (true) and irrelevant (false) citations were aggregated across the eight systematic reviews. Z-tests for proportions were performed to compare the three groups, and the differences were visualised using stacked bar charts (with 95\% confidence intervals computed via a normal approximation). The results show that the positive seed group exhibited a higher proportion of one-hop relevant citations compared to both the negative and random groups - See Figure \ref{fig:bcs_experiement}, with statistical tests (e.g., z-statistics over 11 and p-values $\leq$ 0.001) confirming that these differences were highly significant.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{Confirmation Review//images/bcs_experiment.png}
    \caption{Ratio of one-hop relevance ratio from starting seeds}
    \label{fig:bcs_experiement}
\end{figure}


\subsection{Question 2: Leveraging metadata for continuous active learning}


\emph{How does incorporating structured article metadata (e.g., publication date, author information, citation counts) into document representations using \gls*{gnn} architectures affect the efficiency and accuracy of document prioritisation in continuous active learning for medical systematic reviews, compared to traditional approaches?}

\textbf{Aim and Rationale:}
Traditional \gls*{cal} models for systematic review screening primarily rely on text extracted from the title and abstract. Although effective, these models may overlook crucial contextual signals that reside in structured metadata. By incorporating metadata—information readily available from sources such as the OpenAlex API—into document representations via \gls*{gnn} architectures, the aim is to: \begin{itemize} \item Improve the accuracy of document prioritisation (e.g., higher Recall@95\% and R-Precision)
\item Reduce the manual screening workload (i.e., wss@k). \end{itemize}

\gls*{gnn}s are tools to improve document representation, and as such, this PhD does not look to advance \gls*{gnn} architecture. 

\paragraph{Experimental Approach: } 

To evaluate the benefits of metadata integration, a comparative study involving three groups is proposed:

\begin{enumerate} \item \textbf{Baseline \gls*{cal} (BE-CAL):}
This approach uses a standard encoder model (e.g., BioLinkBERT) that encodes each document’s title and abstract into a fixed-size (e.g., 768-dimensional) vector derived from the [CLS] token. This representation is then used within the active learning loop for ranking and selection.

\item \textbf{\gls*{gnn}-based Document Representation without Metadata (\gls*{gnn}-DR):}
In this model, a document graph is constructed based solely on text features (titles and abstracts) and inter-document connectivity (e.g., citation links). A \gls*{gnn} (using an architecture such as gls*{gcn}s, \gls*{gats}, or GraphSAGE) is then applied to derive node embeddings (for example, 128-dimensional vectors) that capture textual content and connectivity.

\item \textbf{\gls*{gnn}-based Document Representation with Metadata (\gls*{gnn}-DR+Meta):}
Here, node features are enriched by concatenating the text-based embedding (e.g., from a BERT-based model) with structured metadata. For instance, the initial feature vector for a document might be represented as:


\[
[\underbrace{\mathrm{BERT}_{768}}_{\text {title+abstract }}, \underbrace{\text { Year }}_{\text{Publication Year}}, \underbrace{\text { JournalID }}_{\text{journal identifier}} , \underbrace{\text { Citation Count }}_{\text{citation count}} ,\ldots] 
\]

\vspace{0.5em}

Each of these representations will feed into the \gls*{cal} pipeline. A classifier (e.g., logistic regression) will be trained using these embeddings to rank the unlabelled documents iteratively. Performance will be measured using Recall@95\%, R-Precision, and WSS@k metrics.

The \gls*{gnn} is then applied to this enriched graph to produce improved node embeddings incorporating textual, relational, and metadata signals. \end{enumerate}
The performance of these approaches will be evaluated on established datasets, including the CLEF-TAR (2017, 2018, 2019) and Synergy datasets.
\paragraph{Experimental variations:}

\begin{itemize} \item \textbf{Metadata Feature Selection:}
An ablation study will assess the relative contribution of individual metadata features (e.g., publication year, author names, journal identifiers). This will help determine the most informative features.

\item \textbf{Embedding dimensionality:}
Experiments will be conducted with different dimensions (e.g., 128, 256, 512) for the \gls*{gnn}-derived node embeddings to balance the representational richness and computational efficiency. Overstuffing can potentially occur in lower dimensions.

\item \textbf{Message passing hyperparameter, $k$:}
Experiments will be conducted with different variations for $k$ (e.g., 1, 2, 5, 10) for the \gls*{gnn}-derived node embeddings. Increasing $k$ values will increase the amount of ``relational" data shared between each node (document representation). High values of $k$ will result in a fully receptive field (in other words, every node is updated using information from every other node). This could result in over-smoothing, where node representations become too similar, or over-squashing, where too much information from many nodes is compressed into a fixed embedding dimension. 


\item \textbf{\gls*{gnn} Architecture:}
Different message-passing architectures (e.g., gls*{gcn}s, \gls*{gats} or GraphSAGE) will be compared to determine which best captures the combined signals from text and metadata. \end{itemize}

\paragraph{Expected Outcomes:}


\begin{itemize} 
\item The GNN-DR+Meta approach will achieve higher recall and improved ranking accuracy than the baseline BE-CAL and the GNN-DR (without metadata) approaches. 
\item Incorporating metadata will reduce the number of documents requiring manual screening, thereby increasing overall screening efficiency (i.e., reduced WSS@k.)
\item The experimental results will provide quantitative evidence (via metrics such as Recall@95\%, R-Precision, and WSS@k) that structured metadata offers complementary signals to text. 
\end{itemize}


\paragraph{Potential Limitations:}
\begin{itemize} 

\item \textbf{Quality of Metadata:} The performance of the metadata-enhanced approach is contingent on the completeness and accuracy of the metadata sourced (e.g., from OpenAlex). Noisy or incomplete metadata may reduce the anticipated benefits. 

\item \textbf{Increased Model Complexity:} Incorporating additional metadata increases the dimensionality and complexity of the node representations, which may require additional computational resources and careful hyperparameter tuning - see embedding dimensionality and message passing hyperparameter experimental variations. 
\item \textbf{Generalisability:} While focusing on medical systematic reviews, further experiments will be needed to assess whether the findings generalise to other domains or types of systematic reviews (i.e., synergy dataset. 

\end{itemize}

\textbf{Technology Stack and Resources:}
The proposed study will utilise: \begin{itemize} \item \textbf{\gls*{gnn} Libraries:} Python packages such as PyTorch Geometric\footnote{https://github.com/pyg-team/pytorch\_geometric} or Deep Graph Library\footnote{https://www.dgl.ai/} for implementing and experimenting with various \gls*{gnn} architectures. \item \textbf{Pre-trained Language Models:} Models such as PubmedBERT\footnote{https://huggingface.co/microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext}, BioBERT\footnote{https://huggingface.co/dmis-lab/biobert-v1.1} and BioLinkBERT\footnote{https://huggingface.co/michiyasunaga/BioLinkBERT-base} for obtaining initial text embeddings. \item \textbf{Metadata Retrieval:} The OpenAlex API (or similar bibliographic databases) for obtaining structured document metadata. 
\item \textbf{High-Performance Computing: } Running these models will require the use of HPC, provided by the university.
\end{itemize}

\paragraph{Work Completed towards RQ2: }

\paragraph{Replication of Encoder-CAL approach: }
The previously reported Mao study was recreated, where an Encoder \gls*{cal} approach was used in the \gls*{cal} process, except the larger $\text{BioLinkBERT}_{\text{large}}$ formed the classifier~\cite{mao_reproducibility_2024}. The larger model achieved higher performance in R-Precision in 7 of 12 datasets/policy combinations. The Friedman test for individual datasets found significant differences between the FPT epochs 4 out of 12 times; however, when considering all datasets together, there was no significant difference between the FPT epochs and R-precision for relevant selection policy or uncertainty selection policy. This indicates that the ``Goldilocks problem", which was previously reported in the literature in non-medical domains, was not apparent when using the large BiolinkBERT model for the CLEF dataset within a \gls*{cal} process, indicating that further pre-training of models was unnecessary as it does not produce a statistically significant improvement in R-precision. The average R-precision of each FPT epoch is reported in Table \ref{tab:results}, with the highest R-precision for relevancy selection policy being 0.847 at further pretrain epoch two and the highest R-precision for uncertainty being 0.832 at further pretrain epoch 1.

Key findings from this exploratory research are that an optimal pre-training epoch is unlikely to be found within the CLEF dataset and, hence, not a viable avenue for future research. In terms of experimental design, certain hyperparameters
were chosen without clear reasoning (such as batch size being 25, fine-tuning for 20 epochs and stopping after 501 documents labelled). This limitation is considered a barrier to improving the performance of the encoder \gls*{cal} process within that experimental framework, given that reported R-Precision values are already close to the natural ceiling of 1 (with R-Precision reaching 0.945 in some cases). Furthermore, using a more performance/larger model will likely be fruitful for future research. However, it depends on the availability and development of superior models (the creation of which is unfeasible during a PhD period). This undertaken research did, however, highlight that leveraging citations themselves was valuable to the \gls*{cal} process. 

\begin{table}[htbp]
    \centering
    \footnotesize
    \setlength{\tabcolsep}{4pt}
    \begin{tabular}{l>{\raggedright\arraybackslash}p{1.2cm}ccccc}
    \hline
    \textbf{Collection} & \textbf{Dataset size} & \textbf{Model} & \multicolumn{2}{c}{\textbf{R-Precision (↑)}} & \multicolumn{2}{c}{\textbf{Friedman (p)}} \\
    \cline{4-7}
    & & & \textbf{Rel.} & \textbf{Unc.} & \textbf{Rel.} & \textbf{Unc.} \\
    \hline
    \multirow{6}{*}{\makecell[l]{Clef 2019\\dta test}} & 
    \multirow{6}{*}{8} & BiolinkBert-Base-ep0 & \textbf{0.909} & \textbf{0.857} & \multicolumn{2}{c}{---} \\
    & & BiolinkBert-Large-ep0 & 0.897 & 0.803 & \multirow{5}{*}{0.914} & \multirow{5}{*}{0.632} \\
    & & BiolinkBert-Large-ep1 & 0.827 & 0.832 & & \\
    & & BiolinkBert-Large-ep2 & 0.812 & 0.774 & & \\
    & & BiolinkBert-Large-ep5 & 0.841 & 0.814 & & \\
    & & BiolinkBert-Large-ep10 & 0.881 & 0.846 & & \\
    \hline
    \multirow{6}{*}{\makecell[l]{Clef 2017\\test}} & 
    \multirow{6}{*}{30} & BiolinkBert-Base-ep0 & 0.812 & 0.794 & \multicolumn{2}{c}{---} \\
    & & BiolinkBert-Large-ep0 & 0.828 & 0.797 & \multirow{5}{*}{\textbf{\textless0.05}} & \multirow{5}{*}{\textbf{\textless0.05}} \\
    & & BiolinkBert-Large-ep1 & 0.826 & \textbf{0.827} & & \\
    & & BiolinkBert-Large-ep2 & \textbf{0.858} & 0.804 & & \\
    & & BiolinkBert-Large-ep5 & 0.827 & 0.777 & & \\
    & & BiolinkBert-Large-ep10 & 0.799 & 0.757 & & \\
    \hline
    \multirow{6}{*}{\makecell[l]{Clef 2017\\train}} & 
    \multirow{6}{*}{20} & BiolinkBert-Base-ep0 & \textbf{0.838} & 0.761 & \multicolumn{2}{c}{---} \\
    & & BiolinkBert-Large-ep0 & 0.778 & 0.765 & \multirow{5}{*}{\textbf{\textless0.05}} & \multirow{5}{*}{0.28} \\
    & & BiolinkBert-Large-ep1 & 0.808 & 0.789 & & \\
    & & BiolinkBert-Large-ep2 & 0.767 & 0.701 & & \\
    & & BiolinkBert-Large-ep5 & 0.816 & 0.786 & & \\
    & & BiolinkBert-Large-ep10 & 0.827 & \textbf{0.796} & & \\
    \hline
    \multirow{6}{*}{\makecell[l]{Clef 2018\\test}} & 
    \multirow{6}{*}{30} & BiolinkBert-Base-ep0 & 0.794 & 0.780 & \multicolumn{2}{c}{---} \\
    & & BiolinkBert-Large-ep0 & 0.789 & 0.774 & \multirow{5}{*}{0.52} & \multirow{5}{*}{0.50} \\
    & & BiolinkBert-Large-ep1 & \textbf{0.812} & 0.790 & & \\
    & & BiolinkBert-Large-ep2 & 0.797 & \textbf{0.791} & & \\
    & & BiolinkBert-Large-ep5 & 0.763 & 0.773 & & \\
    & & BiolinkBert-Large-ep10 & 0.763 & 0.769 & & \\
    \hline
    \multirow{6}{*}{\makecell[l]{Clef 2019\\DTA int.\\train}} & 
    \multirow{6}{*}{20} & BiolinkBert-Base-ep0 & 0.939 & 0.923 & \multicolumn{2}{c}{---} \\
    & & BiolinkBert-Large-ep0 & 0.939 & 0.902 & \multirow{5}{*}{0.78} & \multirow{5}{*}{0.50} \\
    & & BiolinkBert-Large-ep1 & 0.941 & 0.935 & & \\
    & & BiolinkBert-Large-ep2 & 0.948 & 0.921 & & \\
    & & BiolinkBert-Large-ep5 & 0.952 & 0.945 & & \\
    & & BiolinkBert-Large-ep10 & \textbf{0.945} & \textbf{0.947} & & \\
    \hline
    \multirow{6}{*}{\makecell[l]{Clef 2019\\DTA int.\\test}} & 
    \multirow{6}{*}{20} & BiolinkBert-Base-ep0 & \textbf{0.934} & \textbf{0.900} & \multicolumn{2}{c}{---} \\
    & & BiolinkBert-Large-ep0 & 0.899 & 0.856 & \multirow{5}{*}{0.87} & \multirow{5}{*}{\textbf{\textless0.05}} \\
    & & BiolinkBert-Large-ep1 & 0.904 & 0.840 & & \\
    & & BiolinkBert-Large-ep2 & 0.909 & 0.878 & & \\
    & & BiolinkBert-Large-ep5 & 0.882 & 0.835 & & \\
    & & BiolinkBert-Large-ep10 & 0.865 & 0.841 & & \\
    \hline
   
    \end{tabular}
    \caption{Performance comparison across different collections and models}
    \label{tab:results}
\end{table}


\subsection{Question 3: Informing stopping decisions with information utility}


\emph{How do stopping criteria based on the utility of retrieved information (aligned with user information needs) influence the total manual screening workload in continuous active learning for systematic reviews, and how do these criteria compare with conventional recall-based stopping rules?}


Systematic reviews in medicine commonly strive for near-total recall (e.g., 95\% or more) before concluding the title and abstract screening phase. However, there is no guarantee that screening beyond a certain point meaningfully changes a review's clinical or scientific conclusions. Such ``over-screening" can exhaust finite reviewer resources and inflate costs \cite{prabha_what_2007, ilani_analysis_2024}. Rather than enforcing a strict recall target, \emph{utility-based} stopping seeks to halt once the pool of included documents collectively provides enough evidence to address the research question. To extend this research, the author has submitted work for peer review that measures information utility through moments of a probability distribution, along with a decision threshold to reduce screening of relevant documents - see Appendix \ref{app:Utility_Based_Stopping_Methods}. As this has already been submitted (and contributed to this research question), further experimental avenues are discussed in this section.   

\paragraph{Experimental Approach}

Limitations of that work lie in that the approaches were only applied to DTA reviews. DTA reviews were chosen as the point estimates primary research reports (i.e. sensitivity and false-positive rate) are easily interpretable and can be combined into a single combined point estimate easily (via summing true positives, true negatives, false positives and false negatives across all research). Additionally, these values were readily available from the Limsi-Cochrane Dataset. Although these utility-based methods proved successful in DTA reviews, other systematic reviews (e.g., interventional, prognostic, qualitative) may require different outcome measures or stopping criteria. To have a broad impact, a stopping method needs to be applied in various review types, such as interventional studies. 

The utility of interventional studies can be derived similarly through effect measures (i.e. the mean difference between the interventions). Effect measures have a natural decision boundary (i.e. if each intervention group's upper or lower confidence boundary intersects). This can be achieved by calculating the confidence intervals around the effect measures at each relevant document, stopping when the confidence intervals of the effect measures of each group no longer intersect.

\begin{itemize}
    \item \textbf{Intervention Reviews:} 
    In place of diagnostic sensitivity/specificity, one can model effect measures such as risk ratios or mean differences. As new studies are included, a meta-analysis (fixed or random effects) is dynamically updated. Stopping can occur once the estimated effect size stabilises (e.g., if 95\% confidence intervals no longer cross a relevant clinical boundary).

    \item \textbf{Cross-Domain (Synergy Dataset):}
    The \emph{Synergy} dataset \cite{de_bruin_synergy_2023} spans multiple review types (medical and non-medical). Adapting utility-based stopping to non-clinical outcomes (e.g., engineering or education interventions) will validate whether ``representational stability" heuristics and confidence-boundary thresholds generalise beyond strictly medical contexts. For instance, educational interventions may track average test score improvements, whereas software-engineering reviews might measure defect-detection rates.

    \item \textbf{Total Participant Modelling:}
    Extensions to the stopping rules would more extensively consider the total population of included patients. In the original research, the Confidence Boundary, Skewness and Kurtosis rules did consider the total population of included patients tangentially - i.e., the bigger the population, the smaller the margin of error of the confidence intervals are likely to be. However, the integration of total participants could be more explicit. It has been long established that sample size calculations (e.g., the minimum number of patients required to demonstrate the difference between two intervention groups) could be examined further - should stopping be able to occur before that threshold is met across multiple pieces of research? 
\end{itemize}

A dataset must be created for the CLEF 2019 Intervention Reviews to achieve this. This process can be completed quickly through optical character recognition (as done with the Limsi-Cochrane dataset and by the Author in the Utility-Based Stopping Methods paper) and manual verification. Additionally, collating the true positives, false positives, true negatives, and false negatives from medical systematic reviews included in the Synergy dataset would allow a greater dataset to establish further the generalisability of the already presented utility-based stopping rules. 

\paragraph{Expected Outcomes:}
\begin{itemize}
    \item \textbf{Efficiency Gains}: Demonstration that real-time effect-size monitoring can reduce screening compared to rigid recall-based stopping using the percentage of relevant document metric.
    \item \textbf{Generalisation to Different Review Types}: Empirical evidence on whether confidence-boundary or representational-stability rules can extend from diagnostic reviews to interventional and non-medical contexts.
\end{itemize}

\section{Ethics statement}\label{sec:ethics}

This PhD research will utilise pre-existing, publicly available datasets for analysis. Crucially, this research does not involve any direct interaction with human participants, and no new data collection will be undertaken. The datasets being analysed consist of anonymised data from prior research, ensuring the privacy of the original participants is already protected. Therefore, this research poses minimal ethical risk, as it avoids direct engagement with human subjects and relies solely on data where identifiability has already been removed through prior anonymisation processes.

Furthermore, as this research re-uses existing data and archival data, its scope aligns with the University's ethical guidelines in Policy Note No. 13. Policy Note No. 9 respectively\footnote{https://students.sheffield.ac.uk/research-ethics/ethics/notes/policy-note-13}\footnote{https://students.sheffield.ac.uk/research-ethics/ethics/notes/policy-note-9}. Given these factors and by these policies, formal ethics approval was deemed unnecessary and has not been sought for this project.
\section{Data management plan}

A data management plan has been created for this PhD period and included in Appendix \ref{app:dmp}.


\section{Timeline}

A non-binding timeline for this PhD is presented in Gannt formation in Figure \ref{fig:gantt}. This timetable needs to be flexible, and will likely change \ref{fig:gantt}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{Confirmation Review/images/Gantt.png}
    \caption{Projected PhD timeline in Gantt Chart format}
    \label{fig:gantt}
\end{figure}

In creating this time-line for the PhD several assumptions were made:
\begin{itemize}
    \item \textbf{Holiday Periods:} Two weeks of holiday have been accounted for during the summer and a two-week period over Christmas and New Year. This timeline does not account for additional holidays, which will be determined later.
    \item Front-Loaded Research: The research plan is front-loaded, with significant emphasis placed on the early stages of the project. This approach allows additional literature searching, coding, and experimental setup to be completed in advance, providing a solid foundation for later stages of research, the research questions of which are currently flexible. This strategy also ensures that any necessary adaptations can be made based on early findings, reducing the risk of major delays later in the project. 
    \item \textbf{Research Flexibility:} Breaks are scheduled between work on research questions to allow for adaptation or extension if research questions need to be adjusted. In addition, there is a dedicated period for each of the three research themes before beginning coding or experimental setups. This time is intended for further literature review, acknowledging the rapidly evolving nature of this field. Advancement of the literature is expected before the start of each research question period.
    \item \textbf{Undetermined Research Questions:} A single research period has deliberately been left open, \emph{RQ4}. Depending on the findings of the first three research questions, new research opportunities or developments in the field may emerge that require investigation. Additionally, it could be utilised if there are any delays to this project. 
\end{itemize}

\section{Threat analysis}

Table \ref{tab:risk_matrix} outlines a risk matrix of all potential threats.

\begin{itemize}
    \item \textbf{Research Delays:} Unforeseen challenges in research or coding (e.g., complex research questions, experimental setup issues, or unexpected results requiring further analysis) may cause delays.  An open research question period (\emph{RQ4}) mitigates this risk.
    \item \textbf{Technological advancement:} Rapid technological advancements in the field may render parts of the planned research irrelevant or necessitate a significant change of focus. Ongoing literature review during research question experimentation and implementation mitigates this risk.
    \item \textbf{Publication risks:} 
    The research may not yield publishable results, or the publication process may take longer than anticipated, particularly if revisions are required. Extended gaps between research completion and publication, inherent to the publication system, hinder modifying the research to meet reviewer expectations.
    \item \textbf{Personal and Health Factors:} Extended high-intensity work periods can cause burnout or health problems, impacting the planned schedule. The Gantt chart cannot fully account for extended absences due to illness or other personal factors.
    \item \textbf{Open-ended research questions:} A suitable research area may not be identified by the time the open-ended research questions are reached. However, the author believes this is unlikely, as the amount of research in continuous active learning for systematic reviews is limited.
    \item \textbf{Competing time constraints from PGDip:} Concurrent PGDip and PhD requirements will sometimes constrain available time. The author has maintained good time management and believes the schedule is reasonable despite these constraints. In case of conflict, PhD work will take priority over PGDip.
\end{itemize}

\begin{landscape}
\begin{table}[t]
\centering
\footnotesize
\begin{tabular}{|c|c|c|c|>{\raggedright\arraybackslash}p{10cm}|}
\hline
\textbf{Threat} & \textbf{Likelihood (L)} & \textbf{Impact (I)} & \textbf{Risk} & \textbf{Response} \\
\hline
Research Delays & 3 & 4 & 12 & Reduce the number of research questions to mitigate delays. \\
\hline
Technological Advancement & 3 & 3 & 9 & Increase time allocated for additional literature review to stay updated with advancements. \\
\hline
Publication Risks & 3 & 3 & 9 & If results are not deemed valid during the analysis phase, proceed to the next research question without spending time on write-up. \\
\hline
Personal and Health Factors & 2 & 4& 8  & Prioritise health by taking regular breaks and managing workload effectively to avoid burnout. \\
\hline
Open-ended Research Questions & 2 & 3 & 6 & Continuously monitor new publications within the domain to ensure relevant and timely research questions. \\
\hline
Competing Time Constraints (PGDip) & 3 & 3 & 9 & Minimize involvement in PGDip activities where possible to focus on PhD work. \\
\hline
\end{tabular}
\caption{Risk Scoring Matrix for threats to PhD. Responses are provided for medium impact risks.}
\label{tab:risk_matrix}
\end{table}
\end{landscape}

\section{Conclusion}

 This project brings together critical gaps in systematic review automation, such as more sophisticated document representations, inter-document relations, and user-centric stopping rules, to advance Continuous Active Learning in medical literature screening. By combining citation information and metadata with modern encoder models and \gls*{gnn}, the planned research aims to systematically reduce the screening burden while preserving near-total recall. Moreover, by exploring utility-based stopping criteria, it seeks to align the screening endpoint with the actual value of additional studies, mitigating the inefficiency of rigid recall thresholds. The proposed investigations—spanning citation-augmented seed sets, \gls*{gnn}-driven metadata integration, and novel stopping heuristics — will be validated on widely used CLEF-TAR and Synergy datasets to ensure robust, reproducible results. Ultimately, this work can potentially reduce costs in performing systematic reviews and bolster decision-making quality in evidence-based medicine, offering an answer to the issue of the ever-growing volume of biomedical research.

\chapter{Supporting works}
\section{Completed research}

\subsection{Presented research}

\paragraph{Screening Prioritisation Lecture: } A tutorial was given by the author, along with their supervisor, at the British Computing Society on November 26th, 2024, for the Search Solutions 2024 conference on screening prioritisation\footnote{https://www.bcs.org/membership-and-registrations/member-communities/information-retrieval-specialist-group/conferences-and-events/search-solutions/search-solutions-2024}. A copy of the tutorial has been supplied - see Appendix \ref{app:Screening_Prioritisation}. The tutorial was designed to provide an accessible introduction to screening prioritisation within systematic reviews for an audience of information retrieval practitioners and researchers attending the conference. It was structured as a half-day event, with the author delivering a dedicated hour-long lecture. This lecture aimed to demystify the complexities of systematic review screening and highlight the potential of prioritisation techniques to improve efficiency and reduce workload significantly. Key topics covered included the fundamental screening concepts, the challenges of manual screening in large datasets, and a comprehensive overview of various prioritisation strategies, drawing directly from the literature discussed in Section \ref{sec:screening Priorisation}. The lecture effectively served as a practical guide, offering attendees a clear understanding of the current approaches and the potential impact of screening prioritisation in real-world applications. For the author, this lecture experience provided a way to improve their skills in effectively conveying comprehensive overviews of complex literature to a diverse audience. Furthermore, the interactive nature of the tutorial, including the question and answer session, forced engagement with the audience (from the public to specialists in information retrieval) and directly supported the communication and dissemination aspects of their PhD research.

\subsection{Research under peer review}

\paragraph{Utility-Based Stopping Methods: } This research, submitted to SIGIR 2025 conference\footnote{https://sigir2025.dei.unipd.it} and currently under peer review, introduces a novel utility-based approach to stopping rules for medical literature screening within \gls*{tar}. A copy of the paper has been supplied - See Appendix \ref{app:Utility_Based_Stopping_Methods}. Moving beyond traditional recall-based methods, this work focuses on stopping the search process based on the utility of the information retrieved thus far, specifically its value in answering the underlying research question. Several new algorithms were developed, leveraging the statistical concept of moments of a probability distribution: Harmonic Shrinkage, Confidence Boundary, Skewness, Kurtosis, and Multi-Moments. Experiments demonstrated that the Confidence Boundary algorithm achieves high decision agreement with significantly reduced document review compared to target-recall approaches. This research supports this PhD's goal of improving efficiency in the medical screening process by offering a more efficient and user-centric approach, aligning stopping decisions with the actual information needs of the user.

\paragraph{Predicting Article Retractions: } This research, submitted to the journal Research Integrity and Peer Review\footnote{https://researchintegrityjournal.biomedcentral.com} and currently under peer review, investigates the use of machine learning to predict the retraction of scientific articles. A copy of the paper has been supplied - See Appendix \ref{app:Predicting_article_retractions}. A novel dataset was created by combining information from the Retraction Watch database (containing retracted articles) and the OpenAlex API (an academic publication catalogue). The dataset includes 9,028 articles published between 2000 and 2020, equally split between retracted and non-retracted, and features article metadata, abstracts, and citation metrics. Various machine learning models were trained on this data, including traditional feature-based classifiers (like Gradient Boosting, SVM, XGBoost, and Random Forest) and \gls*{llm}s such as BERT, BioBERT, Llama 3.2, and Gemma 2. The SVM model achieved the highest precision (0.690) for identifying retracted articles, while Llama 3.2-base had the highest overall accuracy (0.682) and recall (0.689 for the retracted class). An ablation study revealed that the publication year was the most influential feature for prediction, followed by the primary topic. Interestingly, the abstract's content was less influential than expected. The dataset and code are made publicly available to facilitate further research. This research supports this PhD's main goal of efficiency by providing tools to reduce the larger work pool identified in the systematic review process.

\section{Ongoing research}

\subsection{CPET project}

A cardiopulmonary exercise test (CPET) is performed before certain anaesthetic procedures, the outcome of which is used to determine the suitability of the patient for this procedure. Current approaches use summarised data to generate decision models, whose data are derived from summary values provided by the machine. The machine also records ''breath-by-breath" data measurements, which, while they are the basis for the summary values, are not used by these models. This research project attempts to determine whether the use of deep neural networks, with these ''breath-by-breath" data, is superior to that of the traditional summary-model approach. This research was devised by an NHS researcher.

\textbf{Threat to PhD :} Low. The author is providing coding assistance to this project and will not be involved in the analysis or extensively involved in research write-up outside of the technical side. Coding is largely complete. This is also likely to result in publishable research, and will likely be published in medical domain venues, promoting interdisciplinary work.


\section{Research output venues}

A dissemination plan is crucial for any research to have an impact. Therefore, a key focus during this PhD will be publishing in high-ranking, peer-reviewed academic venues. The interdisciplinary nature of this research, spanning Information Retrieval, Natural Language Processing, Machine Learning, and Medical Informatics, necessitates targeting a diverse range of conferences and journals. While aiming for top-tier venues, the reality of the publication process means that not all outputs will be accepted at the most prestigious outlets. Furthermore, the choice between a conference or a journal will depend on the nature of the specific research output. A non-exhaustive list of potential output targets, ordered by preference within each category, is presented in Table \ref{tab:venues_detailed_landscape}; all conference impact factors were calculated in 2022~\cite{eickhoff_impact_2023}:

\begin{sidewaystable}[htbp]
  \centering
  \caption{Target Publication Venues}
  \label{tab:venues_detailed_landscape}
  \small
  \begin{tabular}{L{4cm} L{15cm} L{1.5cm} L{1.5cm}}
    \toprule
    \textbf{Category} & \textbf{Venue and Rationale} & \textbf{IF} & \textbf{Freq.} \\
    \midrule
    \multicolumn{4}{l}{\textbf{Primary Targets}} \\
    \midrule
    Conferences &
    \textbf{SIGIR} (Special Interest Group on Information Retrieval)\footnote{\url{https://sigir.org}}: The premier international conference for research on IR, strongly aligned with this PhD's core focus on improving the efficiency of systematic reviews.
    & 19.41 & Annual \\
    &
    \textbf{ACL} (Association for Computational Linguistics)\footnote{\url{https://aclanthology.org/venues/acl}}: A leading conference in NLP, relevant due to the use of language models and text analysis in this research.
    & 25.66 & Annual \\
    \midrule
    Journals &
    \textbf{TOIS} (ACM Transactions on Information Systems)\footnote{\url{https://dl.acm.org/journal/tois}}: A premier journal for IR research, providing an avenue for publishing work on the theoretical and experimental aspects of the proposed methods.
    & 5.4 & Quarterly \\
    &
    \textbf{Journal of Biomedical Informatics}\footnote{\url{https://www.sciencedirect.com/journal/journal-of-biomedical-informatics}}: A leading journal in biomedical informatics, suitable for disseminating research on the application of the developed methods to medical systematic reviews.
     & 4.0 & Monthly \\
    &
    \textbf{TACL} (Transactions of the Association for Computational Linguistics)\footnote{\url{https://transacl.org}}: A top-tier journal for NLP, relevant for publishing in-depth studies on the language modelling aspects of this research.
    & 10.9 & Cont. \\
    \midrule
        \multicolumn{4}{l}{\textbf{Secondary Targets}} \\
    \midrule
    Conferences &
    \textbf{NeurIPS} (Neural Information Processing Systems)\footnote{\url{https://neurips.cc}}: A top-tier ML conference, particularly relevant for the work on \gls*{gnn}s and their application to document representation.
    & 23.27 & Annual \\
    &
    \textbf{ECIR} (European Conference on Information Retrieval)\footnote{\url{https://ecir2025.eu}}: A major European IR conference offering a strong platform for disseminating research within the European research community.
     &  & Annual \\
    &
    \textbf{EMNLP} (Empirical Methods in Natural Language Processing)\footnote{\url{https://aclanthology.org/venues/emnlp/}}: A major NLP conference with an emphasis on empirical methods, aligning with the experimental focus of this PhD.
     &  & Annual \\
     \midrule
    Journals &
   \textbf{Information Processing \& Management}\footnote{\url{https://www.sciencedirect.com/journal/information-processing-and-management}}: A reputable journal covering a wide range of topics in information science, suitable for publishing research on the broader implications of the proposed methods.
    & 7.4 & Bimonthly \\

     &
    \textbf{Artificial Intelligence in Medicine}\footnote{\url{https://www.sciencedirect.com/journal/artificial-intelligence-in-medicine}}:  A specialised journal focusing on AI applications in healthcare, relevant for disseminating work on the medical aspects of systematic review automation.
    & 6.1 & Monthly \\
\midrule
    \multicolumn{4}{l}{\textbf{Preprint Servers}} \\
    \midrule
     & \textbf{arXiv}\footnote{\url{https://arxiv.org}} (cs.IR, cs.CL, cs.LG): Depending on the policies of journals or conferences, publishing preprints is possible. &  &  \\
     \midrule
    \multicolumn{4}{l}{\textbf{Other Dissemination Channels}} \\
    \midrule
        & \textbf{University of Sheffield's CDT Conferences}\footnote{\url{https://slt-cdt.sheffield.ac.uk/annual-conference}}:  Provides opportunities to present research and receive feedback. &  & Annual\\
    \bottomrule
  \end{tabular}
\end{sidewaystable}


The specific choice of output target will be determined based on the nature and maturity of the research findings and how well it aligns with the output venue (and not \emph{solely} by impact factor). The decision to submit to a conference or journal will be guided by factors such as the depth of the study, the extent of experimental validation, and the desired speed of dissemination. As stipulated by UKRI funding, all research outputs will be in open-access venues. Article publishing charges, when applicable, will be paid for with the UKRI grant. Conference Fees will be paid for with the author's enhanced research stipend.  

\section{Doctoral development programme}

The fulfilment of the Doctoral Development Programme has been achieved as the author is part of the CDT in Speech and Language Technologies and their Applications. The training provided as part of the PhD with integrated PGDip replaces the university's standard Doctoral Development Programme, and no further activities are required beyond that included in the CDT\footnote{https://sites.google.com/sheffield.ac.uk/slt-cdt-handbook/the-centre}.

\section{Training needs analysis}

\hl{TODO}

\printbibliography[title={References}]

\appendix
\appendixpage 


\printglossary[type=\acronymtype]

% \chapter{British Computing Society tutorial on screening prioritisation}
% \includepdf[pages=-]{Confirmation Review/supporting_work/Screening Prioritisation.pdf}
% \label{app:Screening_Prioritisation}

% \chapter{Predicting Article Retractions}
% \includepdf[pages=-]{Confirmation Review/supporting_work/Predicting_Article_Retractions.pdf}
% \label{app:Predicting_article_retractions}

% \chapter{Utility-Based Stopping Methods}
% \includepdf[pages=-]{Confirmation Review/supporting_work/Utility_Based_Stopping_Methods.pdf}
% \label{app:Utility_Based_Stopping_Methods}

% \chapter{Data Management Plan}\label{app:dmp}
% \includepdf[pages=-]{Confirmation Review/supporting_work/dmp.pdf}
\end{document}

