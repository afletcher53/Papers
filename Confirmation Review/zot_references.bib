
@article{kusa_analysis_2023,
	title = {An analysis of work saved over sampling in the evaluation of automated citation screening in systematic literature reviews},
	volume = {18},
	issn = {26673053},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2667305323000182},
	doi = {10.1016/j.iswa.2023.200193},
	language = {en},
	urldate = {2025-02-17},
	journal = {Intelligent Systems with Applications},
	author = {Kusa, Wojciech and Lipani, Aldo and Knoth, Petr and Hanbury, Allan},
	month = may,
	year = {2023},
	pages = {200193},
}

@misc{zolnai-lucas_stage_2024,
	title = {{STAGE}: {Simplified} {Text}-{Attributed} {Graph} {Embeddings} {Using} {Pre}-trained {LLMs}},
	shorttitle = {{STAGE}},
	url = {http://arxiv.org/abs/2407.12860},
	doi = {10.48550/arXiv.2407.12860},
	abstract = {We present Simplified Text-Attributed Graph Embeddings (STAGE), a straightforward yet effective method for enhancing node features in Graph Neural Network (GNN) models that encode Text-Attributed Graphs (TAGs). Our approach leverages Large-Language Models (LLMs) to generate embeddings for textual attributes. STAGE achieves competitive results on various node classification benchmarks while also maintaining a simplicity in implementation relative to current state-of-the-art (SoTA) techniques. We show that utilizing pre-trained LLMs as embedding generators provides robust features for ensemble GNN training, enabling pipelines that are simpler than current SoTA approaches which require multiple expensive training and prompting stages. We also implement diffusion-pattern GNNs in an effort to make this pipeline scalable to graphs beyond academic benchmarks.},
	urldate = {2025-02-16},
	publisher = {arXiv},
	author = {Zolnai-Lucas, Aaron and Boylan, Jack and Hokamp, Chris and Ghaffari, Parsa},
	month = jul,
	year = {2024},
	note = {arXiv:2407.12860 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{kantor_model_1987,
	title = {A model for the stopping behavior of users of online systems},
	volume = {38},
	copyright = {http://doi.wiley.com/10.1002/tdm\_license\_1.1},
	issn = {0002-8231, 1097-4571},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/(SICI)1097-4571(198705)38:3<211::AID-ASI10>3.0.CO;2-U},
	doi = {10.1002/(SICI)1097-4571(198705)38:3<211::AID-ASI10>3.0.CO;2-U},
	language = {en},
	number = {3},
	urldate = {2025-02-13},
	journal = {Journal of the American Society for Information Science},
	author = {Kantor, Paul B.},
	month = may,
	year = {1987},
	pages = {211--214},
}

@article{pennington_how_2016,
	title = {How much is enough? {An} investigation of nonprofessional investors information search and stopping rule use},
	volume = {21},
	issn = {14670895},
	shorttitle = {How much is enough?},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1467089515300804},
	doi = {10.1016/j.accinf.2016.04.003},
	language = {en},
	urldate = {2025-02-13},
	journal = {International Journal of Accounting Information Systems},
	author = {Pennington, Robin R. and Kelton, Andrea Seaton},
	month = jun,
	year = {2016},
	pages = {47--62},
}

@article{poziemski_when_2019,
	title = {When {Enough} is {Enough}: {The} {Use} of {Stopping} {Rules} in {Auditor} {Determinations} of {Evidence} {Sufficiency}},
	issn = {1556-5068},
	shorttitle = {When {Enough} is {Enough}},
	url = {https://www.ssrn.com/abstract=3341726},
	doi = {10.2139/ssrn.3341726},
	language = {en},
	urldate = {2025-02-13},
	journal = {SSRN Electronic Journal},
	author = {Poziemski, Elizabeth and Baudot, Lisa},
	year = {2019},
}

@article{pitts_stopping_2004,
	title = {Stopping {Behavior} of {Systems} {Analysts} {During} {Information} {Requirements} {Elicitation}},
	volume = {21},
	issn = {0742-1222, 1557-928X},
	url = {https://www.tandfonline.com/doi/full/10.1080/07421222.2004.11045795},
	doi = {10.1080/07421222.2004.11045795},
	language = {en},
	number = {1},
	urldate = {2025-02-13},
	journal = {Journal of Management Information Systems},
	author = {Pitts, Mitzi G. and Browne, Glenn J.},
	month = jul,
	year = {2004},
	pages = {203--226},
}

@article{berryman_what_2006,
	title = {What defines 'enough' information? {How} policy workers make judgements and decisions during information seeking: preliminary results from an exploratory study},
	volume = {11},
	url = {https://api.semanticscholar.org/CorpusID:28056127},
	journal = {Inf. Res.},
	author = {Berryman, Jennifer M.},
	year = {2006},
}

@article{creighton_university_cognitive_2017,
	title = {Cognitive {Stopping} {Rules} in a {New} {Online} {Reality}},
	volume = {3},
	issn = {24733458},
	url = {http://aisel.aisnet.org/trr/vol3/iss1/2/},
	doi = {10.17705/1atrr.00017},
	urldate = {2025-02-13},
	journal = {AIS Transactions on Replication Research},
	author = {{Creighton University} and Gerhart, Natalie and Windsor, John and {University of North Texas}},
	year = {2017},
	pages = {1--9},
}

@article{gerhart_generalizing_2020,
	title = {Generalizing stopping rule research: {Development} of scales},
	volume = {60},
	issn = {0887-4417, 2380-2057},
	shorttitle = {Generalizing stopping rule research},
	url = {https://www.tandfonline.com/doi/full/10.1080/08874417.2017.1416315},
	doi = {10.1080/08874417.2017.1416315},
	language = {en},
	number = {1},
	urldate = {2025-02-13},
	journal = {Journal of Computer Information Systems},
	author = {Gerhart, Natalie},
	month = jan,
	year = {2020},
	pages = {93--100},
}

@article{dedema_examination_2019,
	title = {Examination of online information search stopping behaviors and stopping rules by task type},
	volume = {56},
	issn = {2373-9231, 2373-9231},
	url = {https://asistdl.onlinelibrary.wiley.com/doi/10.1002/pra2.114},
	doi = {10.1002/pra2.114},
	abstract = {ABSTRACT
            This poster aims to understand how task type could influence when people would stop searching and what stopping rules people would use in the following three stopping behaviors: query stopping, read stopping, and task stopping during online information search. We conducted a pilot user experiment with four participants; each performed two different types of search tasks. The results show two different patterns of stopping behavior on well‐structured and decomposable tasks and poorly structured and indecomposable tasks: sequential mode, and simultaneous mode; and people use different stopping rules in three stopping behaviors.},
	language = {en},
	number = {1},
	urldate = {2025-02-13},
	journal = {Proceedings of the Association for Information Science and Technology},
	author = {{Dedema} and Liu, Chang},
	month = jan,
	year = {2019},
	pages = {631--633},
}

@article{browne_cognitive_2007,
	title = {Cognitive {Stopping} {Rules} for {Terminating} {Information} {Search} in {Online} {Tasks}},
	volume = {31},
	issn = {02767783},
	url = {https://www.jstor.org/stable/10.2307/25148782},
	doi = {10.2307/25148782},
	number = {1},
	urldate = {2025-02-13},
	journal = {MIS Quarterly},
	author = {{Browne} and {Pitts} and {Wetherbe}},
	year = {2007},
	pages = {89},
}

@inproceedings{wu_how_2013,
	address = {Dublin Ireland},
	title = {How far will you go?: characterizing and predicting online search stopping behavior using information scent and need for cognition},
	isbn = {978-1-4503-2034-4},
	shorttitle = {How far will you go?},
	url = {https://dl.acm.org/doi/10.1145/2484028.2484232},
	doi = {10.1145/2484028.2484232},
	language = {en},
	urldate = {2025-02-13},
	booktitle = {Proceedings of the 36th international {ACM} {SIGIR} conference on {Research} and development in information retrieval},
	publisher = {ACM},
	author = {Wu, Wan-Ching},
	month = jul,
	year = {2013},
	pages = {1149--1149},
}

@phdthesis{maxwell_modelling_2021,
	type = {{PhD} {Thesis}},
	title = {Modelling search and stopping in interactive information retrieval},
	school = {University of Glasgow},
	author = {Maxwell, David},
	year = {2021},
	doi = {10.1145/3458537.3458543},
}

@article{simon_behavioral_1955,
	title = {A {Behavioral} {Model} of {Rational} {Choice}},
	volume = {69},
	issn = {00335533},
	url = {https://academic.oup.com/qje/article-lookup/doi/10.2307/1884852},
	doi = {10.2307/1884852},
	number = {1},
	urldate = {2025-02-13},
	journal = {The Quarterly Journal of Economics},
	author = {Simon, Herbert A.},
	month = feb,
	year = {1955},
	pages = {99},
}

@article{dalton_historians_2004,
	title = {Historians and {Their} {Information} {Sources}},
	volume = {65},
	issn = {2150-6701, 0010-0870},
	url = {http://crl.acrl.org/index.php/crl/article/view/15685},
	doi = {10.5860/crl.65.5.400},
	abstract = {This article reports on a survey of historians and a citation analysis undertaken to revisit the questions treated in Margaret F. Stieg’s 1981 article published in College \& Research Libraries. It examines which materials historians consider to be the most important and how they discover them. Their attitudes toward and use of electronic materials were also studied. Many characteristics of historians’ information needs and use have not changed in a generation: informal means of discovery like book reviews and browsing remain important, as does the need for comprehensive searches. Print continues to be the principal format. What has changed is that the advent of electronic resources has increased historians’ use of catalogs and indexes in their efforts to identify appropriate primary and secondary sources of information.},
	number = {5},
	urldate = {2025-02-13},
	journal = {College \& Research Libraries},
	author = {Dalton, Margaret Stieg and Charnigo, Laurie},
	month = sep,
	year = {2004},
	pages = {400--425},
}

@inproceedings{bin-hezam_rlstop_2024,
	title = {{RLStop}: {A} {Reinforcement} {Learning} {Stopping} {Method} for {TAR}},
	shorttitle = {{RLStop}},
	url = {http://arxiv.org/abs/2405.02525},
	doi = {10.1145/3626772.3657911},
	abstract = {We present RLStop, a novel Technology Assisted Review (TAR) stopping rule based on reinforcement learning that helps minimise the number of documents that need to be manually reviewed within TAR applications. RLStop is trained on example rankings using a reward function to identify the optimal point to stop examining documents. Experiments at a range of target recall levels on multiple benchmark datasets (CLEF e-Health, TREC Total Recall, and Reuters RCV1) demonstrated that RLStop substantially reduces the workload required to screen a document collection for relevance. RLStop outperforms a wide range of alternative approaches, achieving performance close to the maximum possible for the task under some circumstances.},
	urldate = {2025-02-13},
	booktitle = {Proceedings of the 47th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	author = {Bin-Hezam, Reem and Stevenson, Mark},
	month = jul,
	year = {2024},
	note = {arXiv:2405.02525 [cs]},
	keywords = {Computer Science - Information Retrieval},
	pages = {2604--2608},
}

@article{losada_when_2019,
	title = {When to stop making relevance judgments? {A} study of stopping methods for building information retrieval test collections},
	volume = {70},
	issn = {2330-1635, 2330-1643},
	shorttitle = {When to stop making relevance judgments?},
	url = {https://asistdl.onlinelibrary.wiley.com/doi/10.1002/asi.24077},
	doi = {10.1002/asi.24077},
	abstract = {In information retrieval evaluation, pooling is a well‐known technique to extract a sample of documents to be assessed for relevance. Given the pooled documents, a number of studies have proposed different prioritization methods to adjudicate documents for judgment. These methods follow different strategies to reduce the assessment effort. However, there is no clear guidance on how many relevance judgments are required for creating a reliable test collection. In this article we investigate and further develop methods to determine when to stop making relevance judgments. We propose a highly diversified set of stopping methods and provide a comprehensive analysis of the usefulness of the resulting test collections. Some of the stopping methods introduced here combine innovative estimates of recall with time series models used in Financial Trading. Experimental results on several representative collections show that some stopping methods can reduce up to 95\% of the assessment effort and still produce a robust test collection. We demonstrate that the reduced set of judgments can be reliably employed to compare search systems using disparate effectiveness metrics such as Average Precision, NDCG, P@100, and Rank Biased Precision. With all these measures, the correlations found between
              full pool
              rankings and
              reduced pool
              rankings is very high.},
	language = {en},
	number = {1},
	urldate = {2025-02-13},
	journal = {Journal of the Association for Information Science and Technology},
	author = {Losada, David E. and Parapar, Javier and Barreiro, Alvaro},
	month = jan,
	year = {2019},
	pages = {49--60},
}

@inproceedings{ros_machine_2017,
	address = {New York, NY, USA},
	series = {{EASE} '17},
	title = {A {Machine} {Learning} {Approach} for {Semi}-{Automated} {Search} and {Selection} in {Literature} {Studies}},
	isbn = {978-1-4503-4804-1},
	url = {https://doi.org/10.1145/3084226.3084243},
	doi = {10.1145/3084226.3084243},
	abstract = {Background. Search and selection of primary studies in Systematic Literature Reviews (SLR) is labour intensive, and hard to replicate and update. Aims. We explore a machine learning approach to support semi-automated search and selection in SLRs to address these weaknesses. Method. We 1) train a classifier on an initial set of papers, 2) extend this set of papers by automated search and snowballing, 3) have the researcher validate the top paper, selected by the classifier, and 4) update the set of papers and iterate the process until a stopping criterion is met. Results. We demonstrate with a proof-of-concept tool that the proposed automated search and selection approach generates valid search strings and that the performance for subsets of primary studies can reduce the manual work by half. Conclusions. The approach is promising and the demonstrated advantages include cost savings and replicability. The next steps include further tool development and evaluate the approach on a complete SLR.},
	booktitle = {Proceedings of the 21st {International} {Conference} on {Evaluation} and {Assessment} in {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Ros, Rasmus and Bjarnason, Elizabeth and Runeson, Per},
	year = {2017},
	note = {event-place: Karlskrona, Sweden},
	keywords = {Automation, Machine learning, Reinforcement learning, Research identification, Study selection, Systematic literature review},
	pages = {118--127},
}

@inproceedings{cormack_machine_2009,
	series = {{NIST} {Special} {Publication}},
	title = {Machine {Learning} for {Information} {Retrieval}: {TREC} 2009 {Web}, {Relevance} {Feedback} and {Legal} {Tracks}},
	volume = {500-278},
	url = {http://trec.nist.gov/pubs/trec18/papers/uwaterloo-cormack.WEB.RF.LEGAL.pdf},
	booktitle = {Proceedings of {The} {Eighteenth} {Text} {REtrieval} {Conference}, {TREC} 2009, {Gaithersburg}, {Maryland}, {USA}, {November} 17-20, 2009},
	publisher = {National Institute of Standards and Technology (NIST)},
	author = {Cormack, Gordon V. and Mojdeh, Mona},
	editor = {Voorhees, Ellen M. and Buckland, Lori P.},
	year = {2009},
}

@inproceedings{del_coz_learning_2021,
	address = {New York, NY, USA},
	series = {{CIKM} '21},
	title = {Learning to {Quantify}: {Methods} and {Applications} ({LQ} 2021)},
	isbn = {978-1-4503-8446-9},
	url = {https://doi.org/10.1145/3459637.3482040},
	doi = {10.1145/3459637.3482040},
	abstract = {Learning to Quantify (LQ) is the task of training class prevalence estimators via supervised learning. The task of these estimators is to estimate, given an unlabelled set of data items D and a set of classes C =c1,...., c{\textbar}C{\textbar}, the prevalence (i.e., relative frequency) of each class c\_i in D. LQ is interesting in all applications of classification in which the final goal is not determining which class (or classes) individual unlabelled data items belong to, but estimating the distribution of the unlabelled data items across the classes of interest. Example disciplines whose interest in labelling data items is at the aggregate level (rather than at the individual level) are the social sciences, political science, market research, ecological modelling, and epidemiology. While LQ may in principle be solved by classifying each data item in D and counting how many such items have been labelled with c\_i, it has been shown that this "classify and count” (CC) method yields suboptimal quantification accuracy. As a result, quantification is now no longer considered a mere byproduct of classification and has evolved as a task of its own. The goal of this workshop is bringing together all researchers interested in methods, algorithms, and evaluation measures and methodologies for LQ, as well as practitioners interested in their practical application to managing large quantities of data.},
	booktitle = {Proceedings of the 30th {ACM} {International} {Conference} on {Information} \& {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {del Coz, Juan José and González, Pablo and Moreo, Alejandro and Sebastiani, Fabrizio},
	year = {2021},
	note = {event-place: Virtual Event, Queensland, Australia},
	keywords = {dataset shift, quantification},
	pages = {4874--4875},
}

@article{howard_swift-active_2020,
	title = {{SWIFT}-{Active} {Screener}: {Accelerated} document screening through active learning and integrated recall estimation},
	volume = {138},
	issn = {01604120},
	shorttitle = {{SWIFT}-{Active} {Screener}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0160412019314023},
	doi = {10.1016/j.envint.2020.105623},
	language = {en},
	urldate = {2025-02-11},
	journal = {Environment International},
	author = {Howard, Brian E. and Phillips, Jason and Tandon, Arpit and Maharana, Adyasha and Elmore, Rebecca and Mav, Deepak and Sedykh, Alex and Thayer, Kristina and Merrick, B. Alex and Walker, Vickie and Rooney, Andrew and Shah, Ruchir R.},
	month = may,
	year = {2020},
	pages = {105623},
}

@article{yu_fast2_2019,
	title = {{FAST2}: {An} intelligent assistant for finding relevant papers},
	volume = {120},
	issn = {0957-4174},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417418307413},
	doi = {https://doi.org/10.1016/j.eswa.2018.11.021},
	abstract = {Literature reviews are essential for any researcher trying to keep up to date with the burgeoning software engineering literature. Finding relevant papers can be hard due to the huge amount of candidates provided by search. FAST2 is a novel tool for assisting the researchers to find the next promising paper to read. This paper describes FAST2 and tests it on four large systematic literature review datasets. We show that FAST2 robustly optimizes the human effort to find most (95\%) of the relevant software engineering papers while also compensating for the errors made by humans during the review process. The effectiveness of FAST2 can be attributed to three key innovations: (1) a novel way of applying external domain knowledge (a simple two or three keyword search) to guide the initial selection of papers—which helps to find relevant research papers faster with less variances; (2) an estimator of the number of remaining relevant papers yet to be found—which helps the reviewer decide when to stop the review; (3) a novel human error correction algorithm—which corrects a majority of human misclassifications (labeling relevant papers as non-relevant or vice versa) without imposing too much extra human effort.},
	journal = {Expert Systems with Applications},
	author = {Yu, Zhe and Menzies, Tim},
	year = {2019},
	keywords = {Active learning, Literature reviews, Relevance feedback, Selection process, Semi-supervised learning, Text mining},
	pages = {57--71},
}

@inproceedings{yang_heuristic_2021,
	address = {New York, NY, USA},
	series = {{DocEng} '21},
	title = {Heuristic stopping rules for technology-assisted review},
	isbn = {978-1-4503-8596-1},
	url = {https://doi.org/10.1145/3469096.3469873},
	doi = {10.1145/3469096.3469873},
	abstract = {Technology-assisted review (TAR) refers to human-in-the-loop active learning workflows for finding relevant documents in large collections. These workflows often must meet a target for the proportion of relevant documents found (i.e. recall) while also holding down costs. A variety of heuristic stopping rules have been suggested for striking this tradeoff in particular settings, but none have been tested against a range of recall targets and tasks. We propose two new heuristic stopping rules, Quant and QuantCI based on model-based estimation techniques from survey research. We compare them against a range of proposed heuristics and find they are accurate at hitting a range of recall targets while substantially reducing review costs.},
	booktitle = {Proceedings of the 21st {ACM} {Symposium} on {Document} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Yang, Eugene and Lewis, David D. and Frieder, Ophir},
	year = {2021},
	note = {event-place: Limerick, Ireland},
}

@inproceedings{huang_contexting_2022,
	address = {Gyeongju, Republic of Korea},
	title = {{ConTextING}: {Granting} {Document}-{Wise} {Contextual} {Embeddings} to {Graph} {Neural} {Networks} for {Inductive} {Text} {Classification}},
	url = {https://aclanthology.org/2022.coling-1.100/},
	abstract = {Graph neural networks (GNNs) have been recently applied in natural language processing. Various GNN research studies are proposed to learn node interactions within the local graph of each document that contains words, sentences, or topics for inductive text classification. However, most inductive GNNs that are built on a word graph generally take global word embeddings as node features, without referring to document-wise contextual information. Consequently, we find that BERT models can perform better than inductive GNNs. An intuitive follow-up approach is used to enrich GNNs with contextual embeddings from BERT, yet there is a lack of related research. In this work, we propose a simple yet effective unified model, coined ConTextING, with a joint training mechanism to learn from both document embeddings and contextual word interactions simultaneously. Our experiments show that ConTextING outperforms pure inductive GNNs and BERT-style models. The analyses also highlight the benefits of the sub-word graph and joint training with separated classifiers.},
	booktitle = {Proceedings of the 29th {International} {Conference} on {Computational} {Linguistics}},
	publisher = {International Committee on Computational Linguistics},
	author = {Huang, Yen-Hao and Chen, Yi-Hsin and Chen, Yi-Shin},
	editor = {Calzolari, Nicoletta and Huang, Chu-Ren and Kim, Hansaem and Pustejovsky, James and Wanner, Leo and Choi, Key-Sun and Ryu, Pum-Mo and Chen, Hsin-Hsi and Donatelli, Lucia and Ji, Heng and Kurohashi, Sadao and Paggio, Patrizia and Xue, Nianwen and Kim, Seokhwan and Hahm, Younggyun and He, Zhong and Lee, Tony Kyungil and Santus, Enrico and Bond, Francis and Na, Seung-Hoon},
	month = oct,
	year = {2022},
	pages = {1163--1168},
}

@misc{wu_active_2021,
	title = {Active {Learning} for {Graph} {Neural} {Networks} via {Node} {Feature} {Propagation}},
	url = {http://arxiv.org/abs/1910.07567},
	doi = {10.48550/arXiv.1910.07567},
	abstract = {Graph Neural Networks (GNNs) for prediction tasks like node classification or edge prediction have received increasing attention in recent machine learning from graphically structured data. However, a large quantity of labeled graphs is difficult to obtain, which significantly limits the true success of GNNs. Although active learning has been widely studied for addressing label-sparse issues with other data types like text, images, etc., how to make it effective over graphs is an open question for research. In this paper, we present an investigation on active learning with GNNs for node classification tasks. Specifically, we propose a new method, which uses node feature propagation followed by K-Medoids clustering of the nodes for instance selection in active learning. With a theoretical bound analysis we justify the design choice of our approach. In our experiments on four benchmark datasets, the proposed method outperforms other representative baseline methods consistently and significantly.},
	urldate = {2025-02-05},
	publisher = {arXiv},
	author = {Wu, Yuexin and Xu, Yichong and Singh, Aarti and Yang, Yiming and Dubrawski, Artur},
	month = nov,
	year = {2021},
	note = {arXiv:1910.07567 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{eickhoff_impact_2023,
	title = {Impact {Factors} for {Computer} {Science} {Conferences}},
	url = {http://arxiv.org/abs/2310.08037},
	doi = {10.48550/arXiv.2310.08037},
	abstract = {An increasing number of CS researchers are employed in academic non-CS departments where publication output is measured in terms of journal impact factors. To foster recognition of publications in peer-reviewed CS conference proceedings, we analyzed more than 40,000 CS publications and computed journal impact factors for 88 top-ranking conferences across a representative range of fields, finding that some conferences have impact factors corresponding to those of high-ranking journals.},
	urldate = {2025-02-04},
	publisher = {arXiv},
	author = {Eickhoff, Carsten},
	month = oct,
	year = {2023},
	note = {arXiv:2310.08037 [cs]},
	keywords = {Computer Science - Digital Libraries},
}

@misc{he_harnessing_2024,
	title = {Harnessing {Explanations}: {LLM}-to-{LM} {Interpreter} for {Enhanced} {Text}-{Attributed} {Graph} {Representation} {Learning}},
	shorttitle = {Harnessing {Explanations}},
	url = {http://arxiv.org/abs/2305.19523},
	doi = {10.48550/arXiv.2305.19523},
	abstract = {Representation learning on text-attributed graphs (TAGs) has become a critical research problem in recent years. A typical example of a TAG is a paper citation graph, where the text of each paper serves as node attributes. Initial graph neural network (GNN) pipelines handled these text attributes by transforming them into shallow or hand-crafted features, such as skip-gram or bag-of-words features. Recent efforts have focused on enhancing these pipelines with language models (LMs), which typically demand intricate designs and substantial computational resources. With the advent of powerful large language models (LLMs) such as GPT or Llama2, which demonstrate an ability to reason and to utilize general knowledge, there is a growing need for techniques which combine the textual modelling abilities of LLMs with the structural learning capabilities of GNNs. Hence, in this work, we focus on leveraging LLMs to capture textual information as features, which can be used to boost GNN performance on downstream tasks. A key innovation is our use of explanations as features: we prompt an LLM to perform zero-shot classification, request textual explanations for its decision-making process, and design an LLM-to-LM interpreter to translate these explanations into informative features for downstream GNNs. Our experiments demonstrate that our method achieves state-of-the-art results on well-established TAG datasets, including Cora, PubMed, ogbn-arxiv, as well as our newly introduced dataset, tape-arxiv23. Furthermore, our method significantly speeds up training, achieving a 2.88 times improvement over the closest baseline on ogbn-arxiv. Lastly, we believe the versatility of the proposed method extends beyond TAGs and holds the potential to enhance other tasks involving graph-text data. Our codes and datasets are available at: https://github.com/XiaoxinHe/TAPE.},
	urldate = {2025-02-04},
	publisher = {arXiv},
	author = {He, Xiaoxin and Bresson, Xavier and Laurent, Thomas and Perold, Adam and LeCun, Yann and Hooi, Bryan},
	month = mar,
	year = {2024},
	note = {arXiv:2305.19523 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{omara-eves_using_2015,
	title = {Using text mining for study identification in systematic reviews: a systematic review of current approaches},
	volume = {4},
	issn = {2046-4053},
	shorttitle = {Using text mining for study identification in systematic reviews},
	url = {https://systematicreviewsjournal.biomedcentral.com/articles/10.1186/2046-4053-4-5},
	doi = {10.1186/2046-4053-4-5},
	language = {en},
	number = {1},
	urldate = {2025-02-04},
	journal = {Systematic Reviews},
	author = {O’Mara-Eves, Alison and Thomas, James and McNaught, John and Miwa, Makoto and Ananiadou, Sophia},
	month = dec,
	year = {2015},
	pages = {5},
}

@book{manning_introduction_2008,
	edition = {1},
	title = {Introduction to {Information} {Retrieval}},
	copyright = {https://www.cambridge.org/core/terms},
	isbn = {978-0-521-86571-5 978-0-511-80907-1},
	url = {https://www.cambridge.org/core/product/identifier/9780511809071/type/book},
	abstract = {Class-tested and coherent, this textbook teaches classical and web information retrieval, including web search and the related areas of text classification and text clustering from basic concepts. It gives an up-to-date treatment of all aspects of the design and implementation of systems for gathering, indexing, and searching documents; methods for evaluating systems; and an introduction to the use of machine learning methods on text collections. All the important ideas are explained using examples and figures, making it perfect for introductory courses in information retrieval for advanced undergraduates and graduate students in computer science. Based on feedback from extensive classroom experience, the book has been carefully structured in order to make teaching more natural and effective. Slides and additional exercises (with solutions for lecturers) are also available through the book's supporting website to help course instructors prepare their lectures.},
	urldate = {2025-02-04},
	publisher = {Cambridge University Press},
	author = {Manning, Christopher D. and Raghavan, Prabhakar and Schütze, Hinrich},
	month = jul,
	year = {2008},
	doi = {10.1017/CBO9780511809071},
}

@inproceedings{ren_survey_2024,
	title = {A {Survey} of {Large} {Language} {Models} for {Graphs}},
	url = {http://arxiv.org/abs/2405.08011},
	doi = {10.1145/3637528.3671460},
	abstract = {Graphs are an essential data structure utilized to represent relationships in real-world scenarios. Prior research has established that Graph Neural Networks (GNNs) deliver impressive outcomes in graph-centric tasks, such as link prediction and node classification. Despite these advancements, challenges like data sparsity and limited generalization capabilities continue to persist. Recently, Large Language Models (LLMs) have gained attention in natural language processing. They excel in language comprehension and summarization. Integrating LLMs with graph learning techniques has attracted interest as a way to enhance performance in graph learning tasks. In this survey, we conduct an in-depth review of the latest state-of-the-art LLMs applied in graph learning and introduce a novel taxonomy to categorize existing methods based on their framework design. We detail four unique designs: i) GNNs as Prefix, ii) LLMs as Prefix, iii) LLMs-Graphs Integration, and iv) LLMs-Only, highlighting key methodologies within each category. We explore the strengths and limitations of each framework, and emphasize potential avenues for future research, including overcoming current integration challenges between LLMs and graph learning techniques, and venturing into new application areas. This survey aims to serve as a valuable resource for researchers and practitioners eager to leverage large language models in graph learning, and to inspire continued progress in this dynamic field. We consistently maintain the related open-source materials at {\textbackslash}url\{https://github.com/HKUDS/Awesome-LLM4Graph-Papers\}.},
	urldate = {2025-02-04},
	booktitle = {Proceedings of the 30th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	author = {Ren, Xubin and Tang, Jiabin and Yin, Dawei and Chawla, Nitesh and Huang, Chao},
	month = aug,
	year = {2024},
	note = {arXiv:2405.08011 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	pages = {6616--6626},
}

@misc{ren_survey_2020,
	title = {A {Survey} of {Deep} {Active} {Learning}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2009.00236},
	doi = {10.48550/ARXIV.2009.00236},
	abstract = {Active learning (AL) attempts to maximize the performance gain of the model by marking the fewest samples. Deep learning (DL) is greedy for data and requires a large amount of data supply to optimize massive parameters, so that the model learns how to extract high-quality features. In recent years, due to the rapid development of internet technology, we are in an era of information torrents and we have massive amounts of data. In this way, DL has aroused strong interest of researchers and has been rapidly developed. Compared with DL, researchers have relatively low interest in AL. This is mainly because before the rise of DL, traditional machine learning requires relatively few labeled samples. Therefore, early AL is difficult to reflect the value it deserves. Although DL has made breakthroughs in various fields, most of this success is due to the publicity of the large number of existing annotation datasets. However, the acquisition of a large number of high-quality annotated datasets consumes a lot of manpower, which is not allowed in some fields that require high expertise, especially in the fields of speech recognition, information extraction, medical images, etc. Therefore, AL has gradually received due attention. A natural idea is whether AL can be used to reduce the cost of sample annotations, while retaining the powerful learning capabilities of DL. Therefore, deep active learning (DAL) has emerged. Although the related research has been quite abundant, it lacks a comprehensive survey of DAL. This article is to fill this gap, we provide a formal classification method for the existing work, and a comprehensive and systematic overview. In addition, we also analyzed and summarized the development of DAL from the perspective of application. Finally, we discussed the confusion and problems in DAL, and gave some possible development directions for DAL.},
	urldate = {2025-02-04},
	publisher = {arXiv},
	author = {Ren, Pengzhen and Xiao, Yun and Chang, Xiaojun and Huang, Po-Yao and Li, Zhihui and Gupta, Brij B. and Chen, Xiaojiang and Wang, Xin},
	year = {2020},
	note = {Version Number: 2},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Machine Learning (stat.ML)},
}

@article{madhawa_active_2020,
	title = {Active {Learning} for {Node} {Classification}: {An} {Evaluation}},
	volume = {22},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {1099-4300},
	shorttitle = {Active {Learning} for {Node} {Classification}},
	url = {https://www.mdpi.com/1099-4300/22/10/1164},
	doi = {10.3390/e22101164},
	abstract = {Current breakthroughs in the field of machine learning are fueled by the deployment of deep neural network models. Deep neural networks models are notorious for their dependence on large amounts of labeled data for training them. Active learning is being used as a solution to train classification models with less labeled instances by selecting only the most informative instances for labeling. This is especially important when the labeled data are scarce or the labeling process is expensive. In this paper, we study the application of active learning on attributed graphs. In this setting, the data instances are represented as nodes of an attributed graph. Graph neural networks achieve the current state-of-the-art classification performance on attributed graphs. The performance of graph neural networks relies on the careful tuning of their hyperparameters, usually performed using a validation set, an additional set of labeled instances. In label scarce problems, it is realistic to use all labeled instances for training the model. In this setting, we perform a fair comparison of the existing active learning algorithms proposed for graph neural networks as well as other data types such as images and text. With empirical results, we demonstrate that state-of-the-art active learning algorithms designed for other data types do not perform well on graph-structured data. We study the problem within the framework of the exploration-vs.-exploitation trade-off and propose a new count-based exploration term. With empirical evidence on multiple benchmark graphs, we highlight the importance of complementing uncertainty-based active learning models with an exploration term.},
	language = {en},
	number = {10},
	urldate = {2025-02-04},
	journal = {Entropy},
	author = {Madhawa, Kaushalya and Murata, Tsuyoshi},
	month = oct,
	year = {2020},
	pages = {1164},
}

@article{zhang_ficom_2024,
	title = {{FICOM}: an effective and scalable active learning framework for {GNNs} on semi-supervised node classification},
	volume = {33},
	issn = {1066-8888, 0949-877X},
	shorttitle = {{FICOM}},
	url = {https://link.springer.com/10.1007/s00778-024-00870-z},
	doi = {10.1007/s00778-024-00870-z},
	abstract = {Abstract
            
              Active learning for graph neural networks (GNNs) aims to select
              B
              nodes to label for the best possible GNN performance. Carefully selected labeled nodes can help improve GNN performance and hence motivates a line of research works. Unfortunately, existing methods still provide inferior GNN performance or cannot scale to large networks.Motivated by these limitations, in this paper, we present
              FICOM
              , an effective and scalable GNN active learning framework. Firstly, we formulate the node selection as an optimization problem where we consider the importance of a node from (i) the importance of a node during the feature propagation with a connection to the personalized PageRank (PPR), and (ii) the diversity of a node brings in the embedding space generated by feature propagation. We show that the defined problem is submodular, and a greedy solution can provide a
              
                
                  \$\$(1-1/e)\$\$
                  
                    
                      (
                      1
                      -
                      1
                      /
                      e
                      )
                    
                  
                
              
              -approximate solution.However, a standard greedy solution requires getting the node with the maximum marginal gain of the objective score in each iteration, which incurs a prohibitive running cost and cannot scale to large datasets. As our main contribution, we present FICOM, an efficient and scalable solution that provides
              
                
                  \$\$(1-1/e)\$\$
                  
                    
                      (
                      1
                      -
                      1
                      /
                      e
                      )
                    
                  
                
              
              -approximation guarantee and scales to graphs with millions of nodes on a single machine. The main idea is that we adaptively maintain the lower- and upper-bound of the marginal gain for each node
              v
              . In each iteration, we can first derive a small subset of candidate nodes and then compute the exact score for this subset of candidate nodes so that we can find the node with the maximum marginal gain efficiently. Extensive experiments on six benchmark datasets using four GNNs, including GCN, SGC, APPNP, and GCNII, show that our FICOM consistently outperforms existing active learning approaches on semi-supervised node classification tasks using different GNNs. Moreover, our solution can finish within 5 h on a million-node graph.},
	language = {en},
	number = {5},
	urldate = {2025-02-04},
	journal = {The VLDB Journal},
	author = {Zhang, Xingyi and Huang, Jinchao and Zhang, Fangyuan and Wang, Sibo},
	month = sep,
	year = {2024},
	pages = {1723--1742},
}

@article{katsimpras_improving_2024,
	title = {Improving {Graph} {Neural} {Networks} by combining active learning with self-training},
	volume = {38},
	issn = {1384-5810, 1573-756X},
	url = {https://link.springer.com/10.1007/s10618-023-00959-z},
	doi = {10.1007/s10618-023-00959-z},
	abstract = {Abstract
            In this paper, we propose a novel framework, called STAL, which makes use of unlabeled graph data, through a combination of Active Learning and Self-Training, in order to improve node labeling by Graph Neural Networks (GNNs). GNNs have been shown to perform well on many tasks, when sufficient labeled data are available. Such data, however, is often scarce, leading to the need for methods that leverage unlabeled data that are abundant. Active Learning and Self-training are two common approaches towards this goal and we investigate here their combination, in the context of GNN training. Specifically, we propose a new framework that first uses active learning to select highly uncertain unlabeled nodes to be labeled and be included in the training set. In each iteration of active labeling, the proposed method expands also the label set through self-training. In particular, highly certain pseudo-labels are obtained and added automatically to the training set. This process is repeated, leading to good classifiers, with a limited amount of labeled data. Our experimental results on various datasets confirm the efficiency of the proposed approach.},
	language = {en},
	number = {1},
	urldate = {2025-02-04},
	journal = {Data Mining and Knowledge Discovery},
	author = {Katsimpras, Georgios and Paliouras, Georgios},
	month = jan,
	year = {2024},
	pages = {110--127},
}

@misc{ly_article_2024,
	title = {Article {Classification} with {Graph} {Neural} {Networks} and {Multigraphs}},
	url = {http://arxiv.org/abs/2309.11341},
	doi = {10.48550/arXiv.2309.11341},
	abstract = {Classifying research output into context-specific label taxonomies is a challenging and relevant downstream task, given the volume of existing and newly published articles. We propose a method to enhance the performance of article classification by enriching simple Graph Neural Network (GNN) pipelines with multi-graph representations that simultaneously encode multiple signals of article relatedness, e.g. references, co-authorship, shared publication source, shared subject headings, as distinct edge types. Fully supervised transductive node classification experiments are conducted on the Open Graph Benchmark OGBN-arXiv dataset and the PubMed diabetes dataset, augmented with additional metadata from Microsoft Academic Graph and PubMed Central, respectively. The results demonstrate that multi-graphs consistently improve the performance of a variety of GNN models compared to the default graphs. When deployed with SOTA textual node embedding methods, the transformed multi-graphs enable simple and shallow 2-layer GNN pipelines to achieve results on par with more complex architectures.},
	urldate = {2025-02-04},
	publisher = {arXiv},
	author = {Ly, Khang and Kashnitsky, Yury and Chamezopoulos, Savvas and Krzhizhanovskaya, Valeria},
	month = may,
	year = {2024},
	note = {arXiv:2309.11341 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{zhang_every_2020,
	address = {Online},
	title = {Every {Document} {Owns} {Its} {Structure}: {Inductive} {Text} {Classification} via {Graph} {Neural} {Networks}},
	shorttitle = {Every {Document} {Owns} {Its} {Structure}},
	url = {https://www.aclweb.org/anthology/2020.acl-main.31},
	doi = {10.18653/v1/2020.acl-main.31},
	language = {en},
	urldate = {2025-02-04},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Zhang, Yufeng and Yu, Xueli and Cui, Zeyu and Wu, Shu and Wen, Zhongzhen and Wang, Liang},
	year = {2020},
	pages = {334--339},
}

@misc{piao_sparse_2022,
	title = {Sparse {Structure} {Learning} via {Graph} {Neural} {Networks} for {Inductive} {Document} {Classification}},
	url = {http://arxiv.org/abs/2112.06386},
	doi = {10.48550/arXiv.2112.06386},
	abstract = {Recently, graph neural networks (GNNs) have been widely used for document classification. However, most existing methods are based on static word co-occurrence graphs without sentence-level information, which poses three challenges:(1) word ambiguity, (2) word synonymity, and (3) dynamic contextual dependency. To address these challenges, we propose a novel GNN-based sparse structure learning model for inductive document classification. Specifically, a document-level graph is initially generated by a disjoint union of sentence-level word co-occurrence graphs. Our model collects a set of trainable edges connecting disjoint words between sentences and employs structure learning to sparsely select edges with dynamic contextual dependencies. Graphs with sparse structures can jointly exploit local and global contextual information in documents through GNNs. For inductive learning, the refined document graph is further fed into a general readout function for graph-level classification and optimization in an end-to-end manner. Extensive experiments on several real-world datasets demonstrate that the proposed model outperforms most state-of-the-art results, and reveal the necessity to learn sparse structures for each document.},
	urldate = {2025-02-04},
	publisher = {arXiv},
	author = {Piao, Yinhua and Lee, Sangseon and Lee, Dohoon and Kim, Sun},
	month = mar,
	year = {2022},
	note = {arXiv:2112.06386 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{kipf_semi-supervised_2017,
	title = {Semi-{Supervised} {Classification} with {Graph} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1609.02907},
	doi = {10.48550/arXiv.1609.02907},
	abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
	urldate = {2025-02-04},
	publisher = {arXiv},
	author = {Kipf, Thomas N. and Welling, Max},
	month = feb,
	year = {2017},
	note = {arXiv:1609.02907 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{yao_graph_2018,
	title = {Graph {Convolutional} {Networks} for {Text} {Classification}},
	url = {http://arxiv.org/abs/1809.05679},
	doi = {10.48550/arXiv.1809.05679},
	abstract = {Text classification is an important and classical problem in natural language processing. There have been a number of studies that applied convolutional neural networks (convolution on regular grid, e.g., sequence) to classification. However, only a limited number of studies have explored the more flexible graph convolutional neural networks (convolution on non-grid, e.g., arbitrary graph) for the task. In this work, we propose to use graph convolutional networks for text classification. We build a single text graph for a corpus based on word co-occurrence and document word relations, then learn a Text Graph Convolutional Network (Text GCN) for the corpus. Our Text GCN is initialized with one-hot representation for word and document, it then jointly learns the embeddings for both words and documents, as supervised by the known class labels for documents. Our experimental results on multiple benchmark datasets demonstrate that a vanilla Text GCN without any external word embeddings or knowledge outperforms state-of-the-art methods for text classification. On the other hand, Text GCN also learns predictive word and document embeddings. In addition, experimental results show that the improvement of Text GCN over state-of-the-art comparison methods become more prominent as we lower the percentage of training data, suggesting the robustness of Text GCN to less training data in text classification.},
	urldate = {2025-02-04},
	publisher = {arXiv},
	author = {Yao, Liang and Mao, Chengsheng and Luo, Yuan},
	month = nov,
	year = {2018},
	note = {arXiv:1809.05679 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{wang_graph_2024,
	title = {Graph neural networks for text classification: a survey},
	volume = {57},
	issn = {1573-7462},
	shorttitle = {Graph neural networks for text classification},
	url = {https://link.springer.com/10.1007/s10462-024-10808-0},
	doi = {10.1007/s10462-024-10808-0},
	abstract = {Abstract
            Text Classification is the most essential and fundamental problem in Natural Language Processing. While numerous recent text classification models applied the sequential deep learning technique, graph neural network-based models can directly deal with complex structured text data and exploit global information. Many real text classification applications can be naturally cast into a graph, which captures words, documents, and corpus global features. In this survey, we bring the coverage of methods up to 2023, including corpus-level and document-level graph neural networks. We discuss each of these methods in detail, dealing with the graph construction mechanisms and the graph-based learning process. As well as the technological survey, we look at issues behind and future directions addressed in text classification using graph neural networks. We also cover datasets, evaluation metrics, and experiment design and present a summary of published performance on the publicly available benchmarks. Note that we present a comprehensive comparison between different techniques and identify the pros and cons of various evaluation metrics in this survey.},
	language = {en},
	number = {8},
	urldate = {2025-02-04},
	journal = {Artificial Intelligence Review},
	author = {Wang, Kunze and Ding, Yihao and Han, Soyeon Caren},
	month = jul,
	year = {2024},
	pages = {190},
}

@misc{rong_dropedge_2020,
	title = {{DropEdge}: {Towards} {Deep} {Graph} {Convolutional} {Networks} on {Node} {Classification}},
	shorttitle = {{DropEdge}},
	url = {http://arxiv.org/abs/1907.10903},
	doi = {10.48550/arXiv.1907.10903},
	abstract = {{\textbackslash}emph\{Over-fitting\} and {\textbackslash}emph\{over-smoothing\} are two main obstacles of developing deep Graph Convolutional Networks (GCNs) for node classification. In particular, over-fitting weakens the generalization ability on small dataset, while over-smoothing impedes model training by isolating output representations from the input features with the increase in network depth. This paper proposes DropEdge, a novel and flexible technique to alleviate both issues. At its core, DropEdge randomly removes a certain number of edges from the input graph at each training epoch, acting like a data augmenter and also a message passing reducer. Furthermore, we theoretically demonstrate that DropEdge either reduces the convergence speed of over-smoothing or relieves the information loss caused by it. More importantly, our DropEdge is a general skill that can be equipped with many other backbone models (e.g. GCN, ResGCN, GraphSAGE, and JKNet) for enhanced performance. Extensive experiments on several benchmarks verify that DropEdge consistently improves the performance on a variety of both shallow and deep GCNs. The effect of DropEdge on preventing over-smoothing is empirically visualized and validated as well. Codes are released on{\textasciitilde}{\textbackslash}url\{https://github.com/DropEdge/DropEdge\}.},
	urldate = {2025-02-04},
	publisher = {arXiv},
	author = {Rong, Yu and Huang, Wenbing and Xu, Tingyang and Huang, Junzhou},
	month = mar,
	year = {2020},
	note = {arXiv:1907.10903 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Networking and Internet Architecture, Statistics - Machine Learning},
}

@misc{monti_geometric_2016,
	title = {Geometric deep learning on graphs and manifolds using mixture model {CNNs}},
	url = {http://arxiv.org/abs/1611.08402},
	doi = {10.48550/arXiv.1611.08402},
	abstract = {Deep learning has achieved a remarkable performance breakthrough in several fields, most notably in speech recognition, natural language processing, and computer vision. In particular, convolutional neural network (CNN) architectures currently produce state-of-the-art performance on a variety of image analysis tasks such as object detection and recognition. Most of deep learning research has so far focused on dealing with 1D, 2D, or 3D Euclidean-structured data such as acoustic signals, images, or videos. Recently, there has been an increasing interest in geometric deep learning, attempting to generalize deep learning methods to non-Euclidean structured data such as graphs and manifolds, with a variety of applications from the domains of network analysis, computational social science, or computer graphics. In this paper, we propose a unified framework allowing to generalize CNN architectures to non-Euclidean domains (graphs and manifolds) and learn local, stationary, and compositional task-specific features. We show that various non-Euclidean CNN methods previously proposed in the literature can be considered as particular instances of our framework. We test the proposed method on standard tasks from the realms of image-, graph- and 3D shape analysis and show that it consistently outperforms previous approaches.},
	urldate = {2025-02-04},
	publisher = {arXiv},
	author = {Monti, Federico and Boscaini, Davide and Masci, Jonathan and Rodolà, Emanuele and Svoboda, Jan and Bronstein, Michael M.},
	month = dec,
	year = {2016},
	note = {arXiv:1611.08402 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{cui_traffic_2020,
	title = {Traffic {Graph} {Convolutional} {Recurrent} {Neural} {Network}: {A} {Deep} {Learning} {Framework} for {Network}-{Scale} {Traffic} {Learning} and {Forecasting}},
	volume = {21},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {1524-9050, 1558-0016},
	shorttitle = {Traffic {Graph} {Convolutional} {Recurrent} {Neural} {Network}},
	url = {https://ieeexplore.ieee.org/document/8917706/},
	doi = {10.1109/TITS.2019.2950416},
	number = {11},
	urldate = {2025-02-04},
	journal = {IEEE Transactions on Intelligent Transportation Systems},
	author = {Cui, Zhiyong and Henrickson, Kristian and Ke, Ruimin and Wang, Yinhai},
	month = nov,
	year = {2020},
	pages = {4883--4894},
}

@misc{fan_graph_2019,
	title = {Graph {Neural} {Networks} for {Social} {Recommendation}},
	url = {http://arxiv.org/abs/1902.07243},
	doi = {10.48550/arXiv.1902.07243},
	abstract = {In recent years, Graph Neural Networks (GNNs), which can naturally integrate node information and topological structure, have been demonstrated to be powerful in learning on graph data. These advantages of GNNs provide great potential to advance social recommendation since data in social recommender systems can be represented as user-user social graph and user-item graph; and learning latent factors of users and items is the key. However, building social recommender systems based on GNNs faces challenges. For example, the user-item graph encodes both interactions and their associated opinions; social relations have heterogeneous strengths; users involve in two graphs (e.g., the user-user social graph and the user-item graph). To address the three aforementioned challenges simultaneously, in this paper, we present a novel graph neural network framework (GraphRec) for social recommendations. In particular, we provide a principled approach to jointly capture interactions and opinions in the user-item graph and propose the framework GraphRec, which coherently models two graphs and heterogeneous strengths. Extensive experiments on two real-world datasets demonstrate the effectiveness of the proposed framework GraphRec. Our code is available at {\textbackslash}url\{https://github.com/wenqifan03/GraphRec-WWW19\}},
	urldate = {2025-02-04},
	publisher = {arXiv},
	author = {Fan, Wenqi and Ma, Yao and Li, Qing and He, Yuan and Zhao, Eric and Tang, Jiliang and Yin, Dawei},
	month = nov,
	year = {2019},
	note = {arXiv:1902.07243 [cs]},
	keywords = {Computer Science - Information Retrieval, Computer Science - Machine Learning, Computer Science - Social and Information Networks},
}

@misc{berg_graph_2017,
	title = {Graph {Convolutional} {Matrix} {Completion}},
	url = {http://arxiv.org/abs/1706.02263},
	doi = {10.48550/arXiv.1706.02263},
	abstract = {We consider matrix completion for recommender systems from the point of view of link prediction on graphs. Interaction data such as movie ratings can be represented by a bipartite user-item graph with labeled edges denoting observed ratings. Building on recent progress in deep learning on graph-structured data, we propose a graph auto-encoder framework based on differentiable message passing on the bipartite interaction graph. Our model shows competitive performance on standard collaborative filtering benchmarks. In settings where complimentary feature information or structured data such as a social network is available, our framework outperforms recent state-of-the-art methods.},
	urldate = {2025-02-04},
	publisher = {arXiv},
	author = {Berg, Rianne van den and Kipf, Thomas N. and Welling, Max},
	month = oct,
	year = {2017},
	note = {arXiv:1706.02263 [stat]},
	keywords = {Computer Science - Databases, Computer Science - Information Retrieval, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{khemani_review_2024,
	title = {A review of graph neural networks: concepts, architectures, techniques, challenges, datasets, applications, and future directions},
	volume = {11},
	issn = {2196-1115},
	shorttitle = {A review of graph neural networks},
	url = {https://journalofbigdata.springeropen.com/articles/10.1186/s40537-023-00876-4},
	doi = {10.1186/s40537-023-00876-4},
	abstract = {Abstract
            Deep learning has seen significant growth recently and is now applied to a wide range of conventional use cases, including graphs. Graph data provides relational information between elements and is a standard data format for various machine learning and deep learning tasks. Models that can learn from such inputs are essential for working with graph data effectively. This paper identifies nodes and edges within specific applications, such as text, entities, and relations, to create graph structures. Different applications may require various graph neural network (GNN) models. GNNs facilitate the exchange of information between nodes in a graph, enabling them to understand dependencies within the nodes and edges. The paper delves into specific GNN models like graph convolution networks (GCNs), GraphSAGE, and graph attention networks (GATs), which are widely used in various applications today. It also discusses the message-passing mechanism employed by GNN models and examines the strengths and limitations of these models in different domains. Furthermore, the paper explores the diverse applications of GNNs, the datasets commonly used with them, and the Python libraries that support GNN models. It offers an extensive overview of the landscape of GNN research and its practical implementations.},
	language = {en},
	number = {1},
	urldate = {2025-02-04},
	journal = {Journal of Big Data},
	author = {Khemani, Bharti and Patil, Shruti and Kotecha, Ketan and Tanwar, Sudeep},
	month = jan,
	year = {2024},
	pages = {18},
}

@misc{lee_attention_2018,
	title = {Attention {Models} in {Graphs}: {A} {Survey}},
	shorttitle = {Attention {Models} in {Graphs}},
	url = {http://arxiv.org/abs/1807.07984},
	doi = {10.48550/arXiv.1807.07984},
	abstract = {Graph-structured data arise naturally in many different application domains. By representing data as graphs, we can capture entities (i.e., nodes) as well as their relationships (i.e., edges) with each other. Many useful insights can be derived from graph-structured data as demonstrated by an ever-growing body of work focused on graph mining. However, in the real-world, graphs can be both large - with many complex patterns - and noisy which can pose a problem for effective graph mining. An effective way to deal with this issue is to incorporate "attention" into graph mining solutions. An attention mechanism allows a method to focus on task-relevant parts of the graph, helping it to make better decisions. In this work, we conduct a comprehensive and focused survey of the literature on the emerging field of graph attention models. We introduce three intuitive taxonomies to group existing work. These are based on problem setting (type of input and output), the type of attention mechanism used, and the task (e.g., graph classification, link prediction, etc.). We motivate our taxonomies through detailed examples and use each to survey competing approaches from a unique standpoint. Finally, we highlight several challenges in the area and discuss promising directions for future work.},
	urldate = {2025-02-04},
	publisher = {arXiv},
	author = {Lee, John Boaz and Rossi, Ryan A. and Kim, Sungchul and Ahmed, Nesreen K. and Koh, Eunyee},
	month = jul,
	year = {2018},
	note = {arXiv:1807.07984 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Social and Information Networks},
}

@article{wu_comprehensive_2021,
	title = {A {Comprehensive} {Survey} on {Graph} {Neural} {Networks}},
	volume = {32},
	issn = {2162-237X, 2162-2388},
	url = {http://arxiv.org/abs/1901.00596},
	doi = {10.1109/TNNLS.2020.2978386},
	abstract = {Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this survey, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art graph neural networks into four categories, namely recurrent graph neural networks, convolutional graph neural networks, graph autoencoders, and spatial-temporal graph neural networks. We further discuss the applications of graph neural networks across various domains and summarize the open source codes, benchmark data sets, and model evaluation of graph neural networks. Finally, we propose potential research directions in this rapidly growing field.},
	number = {1},
	urldate = {2025-02-04},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and Zhang, Chengqi and Yu, Philip S.},
	month = jan,
	year = {2021},
	note = {arXiv:1901.00596 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {4--24},
}

@article{bronstein_geometric_2017,
	title = {Geometric deep learning: going beyond {Euclidean} data},
	volume = {34},
	issn = {1053-5888, 1558-0792},
	shorttitle = {Geometric deep learning},
	url = {http://arxiv.org/abs/1611.08097},
	doi = {10.1109/MSP.2017.2693418},
	abstract = {Many scientific fields study data with an underlying structure that is a non-Euclidean space. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions), and are natural targets for machine learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure, and in cases where the invariances of these structures are built into networks used to model them. Geometric deep learning is an umbrella term for emerging techniques attempting to generalize (structured) deep neural models to non-Euclidean domains such as graphs and manifolds. The purpose of this paper is to overview different examples of geometric deep learning problems and present available solutions, key difficulties, applications, and future research directions in this nascent field.},
	number = {4},
	urldate = {2025-02-04},
	journal = {IEEE Signal Processing Magazine},
	author = {Bronstein, Michael M. and Bruna, Joan and LeCun, Yann and Szlam, Arthur and Vandergheynst, Pierre},
	month = jul,
	year = {2017},
	note = {arXiv:1611.08097 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {18--42},
}

@inproceedings{guo_scimine_2023,
	address = {Taipei Taiwan},
	title = {{SciMine}: {An} {Efficient} {Systematic} {Prioritization} {Model} {Based} on {Richer} {Semantic} {Information}},
	isbn = {978-1-4503-9408-6},
	shorttitle = {{SciMine}},
	url = {https://dl.acm.org/doi/10.1145/3539618.3591764},
	doi = {10.1145/3539618.3591764},
	language = {en},
	urldate = {2025-02-01},
	booktitle = {Proceedings of the 46th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {ACM},
	author = {Guo, Fang and Luo, Yun and Yang, Linyi and Zhang, Yue},
	month = jul,
	year = {2023},
	pages = {205--215},
}

@misc{noauthor_mecir_nodate,
	title = {{MECIR} {Manual} {\textbackslash}textbar {Cochrane} {Community}},
	url = {https://community.cochrane.org/mecir-manual},
	abstract = {Methodological Expectations of Cochrane Intervention Reviews (MECIR) Version August 2023},
	urldate = {2024-11-13},
}

@misc{noauthor_bmc_nodate,
	title = {{BMC} {Medical} {Research} {Methodology}},
	url = {https://bmcmedresmethodol.biomedcentral.com/submission-guidelines/preparing-your-manuscript/research-article},
	abstract = {Publish your healthcare research with BMC Medical Research Methodology, with 3.9 Impact Factor and 18 days to first decision. Focusing on manuscripts ...},
	urldate = {2024-11-13},
}

@article{hirt_citation_2023,
	title = {Citation tracking for systematic literature searching: {A} scoping review},
	volume = {14},
	issn = {1759-2879, 1759-2887},
	shorttitle = {Citation tracking for systematic literature searching},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/jrsm.1635},
	doi = {10.1002/jrsm.1635},
	abstract = {Abstract
            Citation tracking (CT) collects references with citation relationships to pertinent references that are already known. This scoping review maps the benefit of and the tools and terminology used for CT in health‐related systematic literature searching. We included methodological studies on evidence retrieval by CT in health‐related literature searching without restrictions on study design, language, or publication date. We searched MEDLINE/Ovid, Web of Science Core Collection, CINAHL/EBSCOhost, LLISFT/EBSCOhost, LISTA/EBSCOhost, conducted web searching via Google Scholar, backward/forward CT of included studies and pertinent reviews, and contacting of experts. Two reviewers independently assessed eligibility. Data extraction and analysis were performed by one reviewer and checked by another. We screened 11,861 references and included 47 studies published between 1985 and 2021. Most studies (96\%) assessed the benefit of CT either as supplementary or primary/stand‐alone search method. Added value of CT for evidence retrieval was found by 96\% of them. Science Citation Index and Social Sciences Citation Index were the most common citation indexes used. Application of multiple citation indexes in parallel, co‐citing or co‐cited references, CT iterations, or software tools was rare. CT terminology was heterogeneous and frequently ambiguous. The use of CT showed an added value in most of the identified studies; however, the benefit of CT in health‐related systematic literature searching likely depends on multiple factors that could not be assessed with certainty. Application, terminology, and reporting are heterogeneous. Based on our results, we plan a Delphi study to develop recommendations for the use and reporting of CT.},
	language = {en},
	number = {3},
	urldate = {2025-02-01},
	journal = {Research Synthesis Methods},
	author = {Hirt, Julian and Nordhausen, Thomas and Appenzeller‐Herzog, Christian and Ewald, Hannah},
	month = may,
	year = {2023},
	pages = {563--579},
}

@article{noauthor_generating_2025,
	title = {Generating {Search} {Explanations} using {Large} {Language} {Models}},
	abstract = {Aspect-oriented explanations of document relevance in search results have proven to be an effective method for helping users efficiently locate relevant information. While pre-trained large language models (LLMs) have demonstrated exceptional performance for a range of problems, their potential to generate explanations has not been explored. This study addresses this gap using a finetuned text-to-text LLM (T5) and bidirectional autoregression LLM (BART) to generate explanations. The explanations generated are consistently more accurate and plausible explanations than those produced by a range of baseline models. Furthermore, utilizing natural language input representations slightly enhances the performance of the models, particularly the T5 model, resulting in clearer and more contextually appropriate explanations of document relevance.},
	language = {en},
	year = {2025},
}

@patent{cormack_systems_2016,
	title = {Systems and methods for conducting a highly autonomous technology-assisted review classification},
	url = {https://patents.google.com/patent/US20160371261A1/en},
	abstract = {Systems and methods for classifying electronic information are provided by way of a Technology-Assisted Review (“TAR”) process, specifically an “Auto-TAR” process that limits discretionary choices in an information classification effort, while still achieving superior results. In certain embodiments, Auto-TAR selects an initial relevant document from a document collection, selects a number of other documents from the document collection and assigns them a default classification, trains a classifier using a training set made up of the selected relevant document and the documents assigned a default classification, scores documents in the document collection and determines if a stopping criteria is met. If a stopping criteria has not been met, the process sorts the documents according to scores, selects a batch of documents from the collection for further review, receives user coding decisions for them, and re-trains a classifier using the received user coding decisions and an adjusted training set.},
	number = {20160371261A1},
	urldate = {2024-08-04},
	author = {Cormack, Gordon V. and Grossman, Maura R.},
	month = dec,
	year = {2016},
	note = {Type: patentus},
	keywords = {document, documents, relevant, review, stopping criteria},
}

@article{bron_combining_2024,
	title = {Combining {Large} {Language} {Model} {Classifications} and {Active} {Learning} for {Improved} {Technology}-{Assisted} {Review}},
	volume = {3770},
	issn = {1613-0073},
	abstract = {Technology-assisted review (TAR) is software that aids in high-recall information retrieval tasks, such as abstract screening for systematic literature reviews. Often, TAR systems use a form of Active Learning (AL); during this process, human reviewers label documents as relevant or irrelevant according to a screening protocol, while the system incrementally updates a classifier based on the reviewers’ previous decisions. After each model update, the system uses the classifier to rerank the remaining workload by prioritizing predicted relevant documents over irrelevant ones, enabling a reduced workload. Recently, studies have been performed that study the ability of solely using Large Language Models (LLMs) to perform this task by supplying the LLM prompts that contain the task, screening protocol, and a document from the corpus. The LLM then provides a classification of the document in question. While the results of these studies are promising, the LLM’s predictions are not error-free, resulting in a recall or precision that is lower than desired. In this work, we propose a new Active Learning method for TAR that integrates the results of the LLM in the review process that may correct some of the shortcomings of the LLM results, leveraging a reduced workload with respect to current TAR systems.},
	language = {English},
	journal = {CEUR Workshop Proceedings},
	author = {Bron, Michiel P. and Greijn, Berend and Coimbra, Bruno Messina and van de Schoot, Rens and Bagheri, Ayoub},
	month = sep,
	year = {2024},
	note = {Publisher: CEUR WS},
	keywords = {active learning, information retrieval, large language model, technology-assisted review, weak supervision},
	pages = {77--95},
}

@inproceedings{yang_contextualization_2024,
	address = {Washington DC USA},
	title = {Contextualization with {SPLADE} for {High} {Recall} {Retrieval}},
	isbn = {979-8-4007-0431-4},
	url = {c},
	doi = {10.1145/3626772.3657919},
	language = {en},
	urldate = {2025-01-23},
	booktitle = {Proceedings of the 47th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {ACM},
	author = {Yang, Eugene},
	month = jul,
	year = {2024},
	pages = {2337--2341},
}

@misc{hendrycks_measuring_2021,
	title = {Measuring {Massive} {Multitask} {Language} {Understanding}},
	url = {http://arxiv.org/abs/2009.03300},
	doi = {10.48550/arXiv.2009.03300},
	abstract = {We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.},
	urldate = {2025-01-30},
	publisher = {arXiv},
	author = {Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
	month = jan,
	year = {2021},
	note = {arXiv:2009.03300 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Machine Learning},
}

@article{jin_what_2021,
	title = {What {Disease} {Does} {This} {Patient} {Have}? {A} {Large}-{Scale} {Open} {Domain} {Question} {Answering} {Dataset} from {Medical} {Exams}},
	volume = {11},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2076-3417},
	shorttitle = {What {Disease} {Does} {This} {Patient} {Have}?},
	url = {https://www.mdpi.com/2076-3417/11/14/6421},
	doi = {10.3390/app11146421},
	abstract = {Open domain question answering (OpenQA) tasks have been recently attracting more and more attention from the natural language processing (NLP) community. In this work, we present the first free-form multiple-choice OpenQA dataset for solving medical problems, MedQA, collected from the professional medical board exams. It covers three languages: English, simplified Chinese, and traditional Chinese, and contains 12,723, 34,251, and 14,123 questions for the three languages, respectively. We implement both rule-based and popular neural methods by sequentially combining a document retriever and a machine comprehension model. Through experiments, we find that even the current best method can only achieve 36.7\%, 42.0\%, and 70.1\% of test accuracy on the English, traditional Chinese, and simplified Chinese questions, respectively. We expect MedQA to present great challenges to existing OpenQA systems and hope that it can serve as a platform to promote much stronger OpenQA models from the NLP community in the future.},
	language = {en},
	number = {14},
	urldate = {2025-01-30},
	journal = {Applied Sciences},
	author = {Jin, Di and Pan, Eileen and Oufattole, Nassim and Weng, Wei-Hung and Fang, Hanyi and Szolovits, Peter},
	month = jul,
	year = {2021},
	pages = {6421},
}

@misc{yasunaga_linkbert_2022,
	title = {{LinkBERT}: {Pretraining} {Language} {Models} with {Document} {Links}},
	shorttitle = {{LinkBERT}},
	url = {http://arxiv.org/abs/2203.15827},
	doi = {10.48550/arXiv.2203.15827},
	abstract = {Language model (LM) pretraining can learn various knowledge from text corpora, helping downstream tasks. However, existing methods such as BERT model a single document, and do not capture dependencies or knowledge that span across documents. In this work, we propose LinkBERT, an LM pretraining method that leverages links between documents, e.g., hyperlinks. Given a text corpus, we view it as a graph of documents and create LM inputs by placing linked documents in the same context. We then pretrain the LM with two joint self-supervised objectives: masked language modeling and our new proposal, document relation prediction. We show that LinkBERT outperforms BERT on various downstream tasks across two domains: the general domain (pretrained on Wikipedia with hyperlinks) and biomedical domain (pretrained on PubMed with citation links). LinkBERT is especially effective for multi-hop reasoning and few-shot QA (+5\% absolute improvement on HotpotQA and TriviaQA), and our biomedical LinkBERT sets new states of the art on various BioNLP tasks (+7\% on BioASQ and USMLE). We release our pretrained models, LinkBERT and BioLinkBERT, as well as code and data at https://github.com/michiyasunaga/LinkBERT.},
	urldate = {2025-01-30},
	publisher = {arXiv},
	author = {Yasunaga, Michihiro and Leskovec, Jure and Liang, Percy},
	month = mar,
	year = {2022},
	note = {arXiv:2203.15827 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{kontonatsios_semi-supervised_2017,
	title = {A semi-supervised approach using label propagation to support citation screening},
	volume = {72},
	issn = {15320464},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1532046417301454},
	doi = {10.1016/j.jbi.2017.06.018},
	language = {en},
	urldate = {2025-01-30},
	journal = {Journal of Biomedical Informatics},
	author = {Kontonatsios, Georgios and Brockmeier, Austin J. and Przybyła, Piotr and McNaught, John and Mu, Tingting and Goulermas, John Y. and Ananiadou, Sophia},
	month = aug,
	year = {2017},
	pages = {67--76},
}

@article{hashimoto_topic_2016,
	title = {Topic detection using paragraph vectors to support active learning in systematic reviews},
	volume = {62},
	issn = {15320464},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1532046416300442},
	doi = {10.1016/j.jbi.2016.06.001},
	language = {en},
	urldate = {2025-01-30},
	journal = {Journal of Biomedical Informatics},
	author = {Hashimoto, Kazuma and Kontonatsios, Georgios and Miwa, Makoto and Ananiadou, Sophia},
	month = aug,
	year = {2016},
	pages = {59--65},
}

@article{toth_automation_2024,
	title = {Automation of systematic reviews of biomedical literature: a scoping review of studies indexed in {PubMed}},
	volume = {13},
	issn = {2046-4053},
	shorttitle = {Automation of systematic reviews of biomedical literature},
	url = {https://systematicreviewsjournal.biomedcentral.com/articles/10.1186/s13643-024-02592-3},
	doi = {10.1186/s13643-024-02592-3},
	abstract = {Abstract
            
              Background
              The demand for high-quality systematic literature reviews (SRs) for evidence-based medical decision-making is growing. SRs are costly and require the scarce resource of highly skilled reviewers. Automation technology has been proposed to save workload and expedite the SR workflow. We aimed to provide a comprehensive overview of SR automation studies indexed in PubMed, focusing on the applicability of these technologies in real world practice.
            
            
              Methods
              In November 2022, we extracted, combined, and ran an integrated PubMed search for SRs on SR automation. Full-text English peer-reviewed articles were included if they reported studies on SR automation methods (SSAM), or automated SRs (ASR). Bibliographic analyses and knowledge-discovery studies were excluded. Record screening was performed by single reviewers, and the selection of full text papers was performed in duplicate. We summarized the publication details, automated review stages, automation goals, applied tools, data sources, methods, results, and Google Scholar citations of SR automation studies.
            
            
              Results
              From 5321 records screened by title and abstract, we included 123 full text articles, of which 108 were SSAM and 15 ASR. Automation was applied for search (19/123, 15.4\%), record screening (89/123, 72.4\%), full-text selection (6/123, 4.9\%), data extraction (13/123, 10.6\%), risk of bias assessment (9/123, 7.3\%), evidence synthesis (2/123, 1.6\%), assessment of evidence quality (2/123, 1.6\%), and reporting (2/123, 1.6\%). Multiple SR stages were automated by 11 (8.9\%) studies. The performance of automated record screening varied largely across SR topics. In published ASR, we found examples of automated search, record screening, full-text selection, and data extraction. In some ASRs, automation fully complemented manual reviews to increase sensitivity rather than to save workload. Reporting of automation details was often incomplete in ASRs.
            
            
              Conclusions
              Automation techniques are being developed for all SR stages, but with limited real-world adoption. Most SR automation tools target single SR stages, with modest time savings for the entire SR process and varying sensitivity and specificity across studies. Therefore, the real-world benefits of SR automation remain uncertain. Standardizing the terminology, reporting, and metrics of study reports could enhance the adoption of SR automation techniques in real-world practice.},
	language = {en},
	number = {1},
	urldate = {2025-01-30},
	journal = {Systematic Reviews},
	author = {Tóth, Barbara and Berek, László and Gulácsi, László and Péntek, Márta and Zrubka, Zsombor},
	month = jul,
	year = {2024},
	pages = {174},
}

@article{briscoe_conduct_2020,
	title = {Conduct and reporting of citation searching in {Cochrane} systematic reviews: {A} cross‐sectional study},
	volume = {11},
	issn = {1759-2879, 1759-2887},
	shorttitle = {Conduct and reporting of citation searching in {Cochrane} systematic reviews},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/jrsm.1355},
	doi = {10.1002/jrsm.1355},
	abstract = {Background
              The search for studies for a systematic review should be conducted systematically and reported transparently to facilitate reproduction. This study aimed to report on the conduct and reporting of backward citation searching (ie, checking reference lists) and forward citation searching in a cross section of Cochrane reviews. Citation searching uses the citation network surrounding a source study to identify additional studies.
            
            
              Methods
              Cochrane reviews were identified by searching the Cochrane Database of Systematic Reviews using the wildcard symbol and date limiting to the 3‐month period November 2016 to January 2017. Cochrane reviews thus identified were screened for mention of citation searching. Descriptive detail on the conduct and reporting of citation searching was captured in data extraction forms and described and evaluated.
            
            
              Results
              Two hundred fifteen Cochrane reviews were identified. One hundred seventy‐two reviews reported backward citation searching, and 18 reviews reported forward citation searching. Web of Science was the most frequently reported citation index. The studies used for backward citation searching consisted mainly of studies meeting the inclusion criteria. One‐third of reviews that reported forward citation searching used selected studies of importance. Reporting of citation searching was compliant with the Methodological Expectations of Cochrane Intervention Reviews (MECIR) standards, but full transparency requires additional detail that only a minority of reviews reported.
            
            
              Conclusion
              The conduct of backward citation searching was more uniform than forward citation searching. This might be due to lack of MECIR guidance for forward citation searching. Reporting was generally compliant with MECIR, but this is not always sufficient to ensure full transparency.},
	language = {en},
	number = {2},
	urldate = {2025-01-30},
	journal = {Research Synthesis Methods},
	author = {Briscoe, Simon and Bethel, Alison and Rogers, Morwenna},
	month = mar,
	year = {2020},
	pages = {169--180},
}

@article{zhang_evaluating_2020,
	title = {Evaluating sentence-level relevance feedback for high-recall information retrieval},
	volume = {23},
	issn = {1386-4564, 1573-7659},
	url = {https://link.springer.com/10.1007/s10791-019-09361-0},
	doi = {10.1007/s10791-019-09361-0},
	language = {en},
	number = {1},
	urldate = {2025-01-29},
	journal = {Information Retrieval Journal},
	author = {Zhang, Haotian and Cormack, Gordon V. and Grossman, Maura R. and Smucker, Mark D.},
	month = feb,
	year = {2020},
	pages = {1--26},
}

@article{salton_smart_1965,
	title = {The {SMART} automatic document retrieval systems—an illustration},
	volume = {8},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/364955.364990},
	doi = {10.1145/364955.364990},
	language = {en},
	number = {6},
	urldate = {2025-01-28},
	journal = {Communications of the ACM},
	author = {Salton, G. and Lesk, M. E.},
	month = jun,
	year = {1965},
	pages = {391--398},
}

@inproceedings{li_aps_2020,
	address = {Virtual Event China},
	title = {{APS}: {An} {Active} {PubMed} {Search} {System} for {Technology} {Assisted} {Reviews}},
	isbn = {978-1-4503-8016-4},
	shorttitle = {{APS}},
	url = {https://dl.acm.org/doi/10.1145/3397271.3401401},
	doi = {10.1145/3397271.3401401},
	language = {en},
	urldate = {2025-01-28},
	booktitle = {Proceedings of the 43rd {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {ACM},
	author = {Li, Dan and Zafeiriadis, Panagiotis and Kanoulas, Evangelos},
	month = jul,
	year = {2020},
	pages = {2137--2140},
}

@article{van_dijk_artificial_2023,
	title = {Artificial intelligence in systematic reviews: promising when appropriately used},
	volume = {13},
	issn = {2044-6055, 2044-6055},
	shorttitle = {Artificial intelligence in systematic reviews},
	url = {https://bmjopen.bmj.com/lookup/doi/10.1136/bmjopen-2023-072254},
	doi = {10.1136/bmjopen-2023-072254},
	abstract = {Background
              Systematic reviews provide a structured overview of the available evidence in medical-scientific research. However, due to the increasing medical-scientific research output, it is a time-consuming task to conduct systematic reviews. To accelerate this process, artificial intelligence (AI) can be used in the review process. In this communication paper, we suggest how to conduct a transparent and reliable systematic review using the AI tool ‘ASReview’ in the title and abstract screening.
            
            
              Methods
              Use of the AI tool consisted of several steps. First, the tool required training of its algorithm with several prelabelled articles prior to screening. Next, using a researcher-in-the-loop algorithm, the AI tool proposed the article with the highest probability of being relevant. The reviewer then decided on relevancy of each article proposed. This process was continued until the stopping criterion was reached. All articles labelled relevant by the reviewer were screened on full text.
            
            
              Results
              Considerations to ensure methodological quality when using AI in systematic reviews included: the choice of whether to use AI, the need of both deduplication and checking for inter-reviewer agreement, how to choose a stopping criterion and the quality of reporting. Using the tool in our review resulted in much time saved: only 23\% of the articles were assessed by the reviewer.
            
            
              Conclusion
              The AI tool is a promising innovation for the current systematic reviewing practice, as long as it is appropriately used and methodological quality can be assured.
            
            
              PROSPERO registration number
              CRD42022283952.},
	language = {en},
	number = {7},
	urldate = {2025-01-28},
	journal = {BMJ Open},
	author = {Van Dijk, Sanne H B and Brusse-Keizer, Marjolein G J and Bucsán, Charlotte C and Van Der Palen, Job and Doggen, Carine J M and Lenferink, Anke},
	month = jul,
	year = {2023},
	pages = {e072254},
}

@inproceedings{lewis_sequential_1994,
	title = {A {Sequential} {Algorithm} for {Training} {Text} {Classifiers}},
	isbn = {978-1-4471-2099-5},
	doi = {10.1007/978-1-4471-2099-5_1},
	abstract = {The ability to cheaply train text classifiers is critical to their use in information retrieval, content analysis, natural language processing, and other tasks involving data which is partly or fully textual. An algorithm for sequential sampling during machine learning of statistical classifiers was developed and tested on a newswire text categorization task. This method, which we call uncertainty sampling, reduced by as much as 500-fold the amount of training data that would have to be manually classified to achieve a given level of effectiveness.},
	booktitle = {{SIGIR} ’94},
	publisher = {Springer},
	author = {Lewis, David D. and Gale, William A.},
	editor = {Croft, Bruce W. and van Rijsbergen, C. J.},
	year = {1994},
	note = {Place: London},
	pages = {3--12},
}

@article{miwa_reducing_2014,
	title = {Reducing systematic review workload through certainty-based screening},
	volume = {51},
	issn = {15320464},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1532046414001439},
	doi = {10.1016/j.jbi.2014.06.005},
	language = {en},
	urldate = {2025-01-28},
	journal = {Journal of Biomedical Informatics},
	author = {Miwa, Makoto and Thomas, James and O’Mara-Eves, Alison and Ananiadou, Sophia},
	month = oct,
	year = {2014},
	pages = {242--253},
}

@article{yu_finding_2018,
	title = {Finding {Better} {Active} {Learners} for {Faster} {Literature} {Reviews}},
	volume = {23},
	issn = {1382-3256, 1573-7616},
	url = {http://arxiv.org/abs/1612.03224},
	doi = {10.1007/s10664-017-9587-0},
	abstract = {Literature reviews can be time-consuming and tedious to complete. By cataloging and refactoring three state-of-the-art active learning techniques from evidence-based medicine and legal electronic discovery, this paper finds and implements FASTREAD, a faster technique for studying a large corpus of documents. This paper assesses FASTREAD using datasets generated from existing SE literature reviews (Hall, Wahono, Radjenovi{\textbackslash}'c, Kitchenham et al.). Compared to manual methods, FASTREAD lets researchers find 95\% relevant studies after reviewing an order of magnitude fewer papers. Compared to other state-of-the-art automatic methods, FASTREAD reviews 20-50\% fewer studies while finding same number of relevant primary studies in a systematic literature review.},
	number = {6},
	urldate = {2025-01-28},
	journal = {Empirical Software Engineering},
	author = {Yu, Zhe and Kraft, Nicholas A. and Menzies, Tim},
	month = dec,
	year = {2018},
	note = {arXiv:1612.03224 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
	pages = {3161--3186},
}

@article{yu_fast2_2019-1,
	title = {{FAST2}: {An} intelligent assistant for finding relevant papers},
	volume = {120},
	issn = {09574174},
	shorttitle = {{FAST2}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417418307413},
	doi = {10.1016/j.eswa.2018.11.021},
	language = {en},
	urldate = {2025-01-28},
	journal = {Expert Systems with Applications},
	author = {Yu, Zhe and Menzies, Tim},
	month = apr,
	year = {2019},
	pages = {57--71},
}

@article{artstein_inter-coder_2008,
	title = {Inter-{Coder} {Agreement} for {Computational} {Linguistics}},
	volume = {34},
	issn = {0891-2017, 1530-9312},
	url = {https://direct.mit.edu/coli/article/34/4/555-596/1999},
	doi = {10.1162/coli.07-034-R2},
	abstract = {This article is a survey of methods for measuring agreement among corpus annotators. It exposes the mathematics and underlying assumptions of agreement coefficients, covering Krippendorff's alpha as well as Scott's pi and Cohen's kappa; discusses the use of coefficients in several annotation tasks; and argues that weighted, alpha-like coefficients, traditionally less used than kappa-like measures in computational linguistics, may be more appropriate for many corpus annotation tasks—but that their use makes the interpretation of the value of the coefficient even harder.},
	language = {en},
	number = {4},
	urldate = {2025-01-28},
	journal = {Computational Linguistics},
	author = {Artstein, Ron and Poesio, Massimo},
	month = dec,
	year = {2008},
	pages = {555--596},
}

@inproceedings{grossman_automatic_2017,
	address = {Shinjuku Tokyo Japan},
	title = {Automatic and {Semi}-{Automatic} {Document} {Selection} for {Technology}-{Assisted} {Review}},
	isbn = {978-1-4503-5022-8},
	url = {https://dl.acm.org/doi/10.1145/3077136.3080675},
	doi = {10.1145/3077136.3080675},
	language = {en},
	urldate = {2025-01-28},
	booktitle = {Proceedings of the 40th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {ACM},
	author = {Grossman, Maura R. and Cormack, Gordon V. and Roegiest, Adam},
	month = aug,
	year = {2017},
	pages = {905--908},
}

@patent{cormack_systems_2019,
	title = {Systems and methods for conducting a highly autonomous technology-assisted review classification},
	number = {US 10,229,117},
	author = {Cormack, Gordon V. and Grossman, Maura R.},
	year = {2019},
	note = {Type: Patent},
}

@inproceedings{cormack_technology-assisted_2017,
	title = {Technology-{Assisted} {Review} in {Empirical} {Medicine}: {Waterloo} {Participation} in {CLEF} {eHealth} 2017},
	booktitle = {{CLEF} ({Working} {Notes})},
	author = {Cormack, Gordon V. and Grossman, Maura R.},
	year = {2017},
}

@inproceedings{scells_integrating_2017,
	address = {Singapore Singapore},
	title = {Integrating the {Framing} of {Clinical} {Questions} via {PICO} into the {Retrieval} of {Medical} {Literature} for {Systematic} {Reviews}},
	isbn = {978-1-4503-4918-5},
	url = {https://dl.acm.org/doi/10.1145/3132847.3133080},
	doi = {10.1145/3132847.3133080},
	language = {en},
	urldate = {2025-01-28},
	booktitle = {Proceedings of the 2017 {ACM} on {Conference} on {Information} and {Knowledge} {Management}},
	publisher = {ACM},
	author = {Scells, Harrisen and Zuccon, Guido and Koopman, Bevan and Deacon, Anthony and Azzopardi, Leif and Geva, Shlomo},
	month = nov,
	year = {2017},
	pages = {2291--2294},
}

@article{diao_lexical_2021,
	title = {A lexical and syntactic study of research article titles in {Library} {Science} and {Scientometrics}},
	volume = {126},
	issn = {0138-9130, 1588-2861},
	url = {https://link.springer.com/10.1007/s11192-021-04018-6},
	doi = {10.1007/s11192-021-04018-6},
	language = {en},
	number = {7},
	urldate = {2025-01-28},
	journal = {Scientometrics},
	author = {Diao, Junli},
	month = jul,
	year = {2021},
	pages = {6041--6058},
}

@incollection{ferro_qut_2017,
	address = {http://www.ceur-ws.org/},
	title = {{QUT} ielab at {CLEF} {eHealth} 2017 {Technology} {Assisted} {Reviews} {Track}: {Initial} experiments with learning to rank},
	url = {https://eprints.qut.edu.au/110358/},
	abstract = {In this paper we describe our participation to the CLEF eHealth 2017 Technology Assisted Reviews track (TAR). This track aims to evaluate and advance search technologies aimed at supporting the creation of biomedical systematic reviews. In this context, the track explores the task of screening prioritisation: the ranking of studies to be screened for inclusion in a systematic review. Our solution addresses this challenge by developing ranking strategies based on learning to rank techniques and exploiting features derived by the use of the PICO framework. PICO (Population, Intervention, Control or comparison and Outcome) is a technique used in evidence based practice to frame and answer clinical questions and is used extensively in the compilation of systematic reviews. Our experiments show that the use of the PICO-based feature within learning to rank provides improvements over the use of baseline features alone.},
	booktitle = {Working {Notes} of {CLEF} 2017 - {Conference} and {Labs} of the {Evaluation} {Forum} [{CEUR} {Workshop} {Proceedings}, {Volume} 1866]},
	publisher = {Sun SITE Central Europe},
	author = {Scells, Harrisen and Zuccon, Guido and Deacon, Anthony and Koopman, Bevan},
	editor = {Ferro, N. and Mandl, T. and Goeuriot, L. and Cappellato, L.},
	year = {2017},
	pages = {1--6},
}

@inproceedings{alharbi_ranking_2019,
	series = {{CLEF} 2019 {Conference} and {Labs} of the {Evaluation} {Forum}},
	title = {Ranking studies for systematic reviews using query adaptation: {University} of {Sheffield}'s approach to {CLEF} {eHealth} 2019 {Task} 2 working notes for {CLEF} 2019},
	booktitle = {Working {Notes} of {CLEF} 2019 - {Conference} and {Labs} of the {Evaluation} {Forum}},
	publisher = {CEUR Workshop Proceedings},
	author = {Alharbi, Amal and Stevenson, Mark},
	editor = {Cappellato, L. and Ferro, N. and Losada, D. E. and Müller, H.},
	year = {2019},
}

@inproceedings{alharbi_ranking_2017,
	series = {Conference and {Labs} of the {Evaluation} {Forum} ({CLEF} 2017)},
	title = {Ranking abstracts to identify relevant evidence for systematic reviews: {The} {University} of {Sheffield}'s approach to {CLEF} {eHealth} 2017 {Task} 2: {Working} notes for {CLEF} 2017},
	booktitle = {{CEUR} {Workshop} {Proceedings}},
	publisher = {CEUR},
	author = {Alharbi, Amal and Stevenson, Mark},
	editor = {Cappellato, L. and Ferro, N. and Goeuriot, L. and Mandl, T.},
	year = {2017},
}

@inproceedings{alharbi_retrieving_2018,
	title = {Retrieving and ranking studies for systematic reviews: {University} of {Sheffield}’s approach to {CLEF} {eHealth} 2018 {Task} 2},
	volume = {2125},
	booktitle = {{CEUR} {Workshop} {Proceedings}},
	publisher = {CEUR Workshop Proceedings},
	author = {Alharbi, Amal and Briggs, William and Stevenson, Mark},
	year = {2018},
}

@article{haddaway_predicting_2019,
	title = {Predicting the time needed for environmental systematic reviews and systematic maps},
	volume = {33},
	issn = {0888-8892, 1523-1739},
	url = {https://conbio.onlinelibrary.wiley.com/doi/10.1111/cobi.13231},
	doi = {10.1111/cobi.13231},
	abstract = {Abstract
            
              Systematic reviews (SRs) and systematic mapping aim to maximize transparency and comprehensiveness while minimizing subjectivity and bias. These are time‐consuming and complex tasks, so SRs are considered resource intensive, but published estimates of systematic‐review resource requirements are largely anecdotal. We analyzed all Collaboration for Environmental Evidence (CEE) SRs (
              n
              = 66) and maps (
              n
              = 20) published from 2012 to 2017 to estimate the average number of articles retained at each review stage. We also surveyed 33 experienced systematic reviewers to collate information on the rate at which those stages could be completed. In combination, these data showed that the average CEE SR takes an estimated 164 d (full‐time equivalent) (SD 23), and the average CEE systematic map (SM) (excluding critical appraisal) takes 211 d (SD 53). While screening titles and abstracts is widely considered time‐consuming, metadata extraction and critical appraisal took as long or longer to complete, especially for SMs. Given information about the planned methods and evidence base, we created a software tool that predicts time requirements of a SR or map with evidence‐based defaults as a starting point. Our results shed light on the most time‐consuming stages of the SR and mapping processes, will inform review planning, and can direct innovation to streamline processes. Future predictions of effort required to complete SRs and maps could be improved if authors provide more details on methods and results.
            
          , 
            
              
              Pronóstico del Tiempo Necesario para las Revisiones Ambientales Sistemáticas y los Mapas Sistemáticos
              
                Resumen
                
                  El mapeo sistemático y las revisiones sistemáticas buscan maximizar la transparencia y la exhaustividad mientras minimizan la subjetividad y la parcialidad. Estas son labores complejas que consumen tiempo, por lo que las revisiones sistemáticas se consideran como intensivas en recursos, pero en el caso de los requerimientos de los recursos para las revisiones sistemáticas las estimaciones publicadas son en su mayoría anecdóticas. Analizamos todas las revisiones sistemáticas (
                  n
                  = 66) y todos los mapas (
                  n
                  = 20) de la Colaboración para la Evidencia Ambiental (CEE, en inglés) publicados entre 2012 y 2017 para estimar el número promedio de artículos retenidos en cada etapa de revisión. También encuestamos a 33 revisores sistemáticos experimentado para cotejar la información sobre la tasa a la cual se podrían completar esas etapas. La combinación de estos datos mostró que la revisión sistemática promedio del CEE tarda un estimado de 164 días (equivalente de tiempo completo) (SD 23), y que el mapa sistemático promedio del CEE (excluyendo la evaluación crítica) tarda 211 días (SD 53). Se considera ampliamente que el proceso de selección de títulos y resúmenes consume mucho tiempo, pero la extracción de meta‐datos y la evaluación crítica tarda la misma cantidad de tiempo, o más, para completarse, especialmente en el caso de los mapas sistemáticos. Con la información sobre los métodos planeados y la base de evidencias creamos una herramienta de software que predice los requerimientos de tiempo para un mapa sistemático o una revisión sistemática con defaults basados en evidencias como puntos de partida. Nuestros resultados traen a la luz las etapas de la revisión sistemática o del mapeo sistemático que más tiempo consumen, informarán sobre la planeación de revisiones, y pueden dirigir la innovación en los procesos simplificados. Los pronósticos futuros del esfuerzo requerido para completar los mapas y las revisiones sistémicos podría mejorarse si los autores proporcionar más detalles sobre los métodos y los resultados.
                
              
            
          , 
            摘要
            
              系统评估和系统绘图的目的是最大限度地提高透明度和全面性, 尽量减少主观性和偏见。这样的任务复杂且耗时, 所以系统评估被认为是资源密集型的, 然而, 已发表的关于系统评估所需资源的估计却又没有充足的依据。我们分析了《环境证据》协作组织 (Collaboration for Environmental Evidence, CEE) 在 2012 至 2017 年间发表的所有的系统评估 (
              n
              = 66) 和绘图 (
              n
              = 20) , 来估计每个评估阶段保留的文章平均数量。我们还访问了33 名有经验的系统评估专家, 分析得出各个阶段的完成速度。总的来说, 这些数据表明, CEE 系统评估平均需要约164 天 (全日制) (标准偏差 23 天) , CEE 系统绘图 ( 不包括评读文献) 平均需要 211 天 ( 标准偏差 53 天) 。虽然人们普遍认为筛选标题和摘要很费时, 但宏数据提取和严格评价也需要一样长甚至更长的时间, 特别是对于系统绘图来说。利用所设计的方法和证据基础的信息, 我们开发了一款以基于证据的默认值为起点的软件来预测进行系统评估或绘图所需时间。我们的结果找出了系统评估和绘图过程中最耗时的阶段, 可以为评估规划提供信息, 并指导方法创新来简化流程。在作者提供更多方法和结果的详细信息的情况下, 未来可以改进对完成系统评估和绘图所需工作的预测。
              【翻译: 胡怡思; 审校: 聂永刚】
            
          , 
            
              Article impact statement
              : We provide data on the effort needed to complete systematic literature reviews and maps and new software to help users apply our findings.},
	language = {en},
	number = {2},
	urldate = {2025-01-28},
	journal = {Conservation Biology},
	author = {Haddaway, Neal R. and Westgate, Martin J.},
	month = apr,
	year = {2019},
	pages = {434--443},
}

@article{godlee_more_2010,
	title = {More research is needed--but what type?},
	volume = {341},
	issn = {0959-8138, 1468-5833},
	url = {https://www.bmj.com/lookup/doi/10.1136/bmj.c4662},
	doi = {10.1136/bmj.c4662},
	language = {en},
	number = {aug25 3},
	urldate = {2025-01-27},
	journal = {BMJ},
	author = {Godlee, F.},
	month = aug,
	year = {2010},
	pages = {c4662--c4662},
}

@article{bafeta_reporting_2014,
	title = {Reporting of results from network meta-analyses: methodological systematic review},
	volume = {348},
	issn = {1756-1833},
	shorttitle = {Reporting of results from network meta-analyses},
	url = {https://www.bmj.com/lookup/doi/10.1136/bmj.g1741},
	doi = {10.1136/bmj.g1741},
	language = {en},
	number = {mar11 5},
	urldate = {2025-01-27},
	journal = {BMJ},
	author = {Bafeta, A. and Trinquart, L. and Seror, R. and Ravaud, P.},
	month = mar,
	year = {2014},
	pages = {g1741--g1741},
}

@article{santos_joanna_2018,
	title = {The {Joanna} {Briggs} {Institute} approach for systematic reviews},
	volume = {26},
	issn = {1518-8345},
	doi = {10.1590/1518-8345.2885.3074},
	language = {eng, por, spa},
	journal = {Revista Latino-Americana De Enfermagem},
	author = {Santos, Wendel Mombaque Dos and Secoli, Silvia Regina and Püschel, Vilanice Alves de Araújo},
	month = nov,
	year = {2018},
	pmid = {30462787},
	pmcid = {PMC6248737},
	keywords = {Evidence-Based Medicine, Humans, Research Design, Systematic Reviews as Topic},
	pages = {e3074},
}

@article{moher_improving_1999,
	title = {Improving the quality of reports of meta-analyses of randomised controlled trials: the {QUOROM} statement. {Quality} of {Reporting} of {Meta}-analyses},
	volume = {354},
	issn = {0140-6736},
	shorttitle = {Improving the quality of reports of meta-analyses of randomised controlled trials},
	doi = {10.1016/s0140-6736(99)04149-5},
	abstract = {BACKGROUND: The Quality of Reporting of Meta-analyses (QUOROM) conference was convened to address standards for improving the quality of reporting of meta-analyses of clinical randomised controlled trials (RCTs).
METHODS: The QUOROM group consisted of 30 clinical epidemiologists, clinicians, statisticians, editors, and researchers. In conference, the group was asked to identify items they thought should be included in a checklist of standards. Whenever possible, checklist items were guided by research evidence suggesting that failure to adhere to the item proposed could lead to biased results. A modified Delphi technique was used in assessing candidate items.
FINDINGS: The conference resulted in the QUOROM statement, a checklist, and a flow diagram. The checklist describes our preferred way to present the abstract, introduction, methods, results, and discussion sections of a report of a meta-analysis. It is organised into 21 headings and subheadings regarding searches, selection, validity assessment, data abstraction, study characteristics, and quantitative data synthesis, and in the results with "trial flow", study characteristics, and quantitative data synthesis; research documentation was identified for eight of the 18 items. The flow diagram provides information about both the numbers of RCTs identified, included, and excluded and the reasons for exclusion of trials.
INTERPRETATION: We hope this report will generate further thought about ways to improve the quality of reports of meta-analyses of RCTs and that interested readers, reviewers, researchers, and editors will use the QUOROM statement and generate ideas for its improvement.},
	language = {eng},
	number = {9193},
	journal = {Lancet (London, England)},
	author = {Moher, D. and Cook, D. J. and Eastwood, S. and Olkin, I. and Rennie, D. and Stroup, D. F.},
	month = nov,
	year = {1999},
	pmid = {10584742},
	keywords = {Authorship, Guidelines as Topic, Humans, Meta-Analysis as Topic, Randomized Controlled Trials as Topic, Software Design},
	pages = {1896--1900},
}

@article{rouse_network_2017,
	title = {Network meta-analysis: an introduction for clinicians},
	volume = {12},
	issn = {1970-9366},
	shorttitle = {Network meta-analysis},
	doi = {10.1007/s11739-016-1583-7},
	abstract = {Network meta-analysis is a technique for comparing multiple treatments simultaneously in a single analysis by combining direct and indirect evidence within a network of randomized controlled trials. Network meta-analysis may assist assessing the comparative effectiveness of different treatments regularly used in clinical practice and, therefore, has become attractive among clinicians. However, if proper caution is not taken in conducting and interpreting network meta-analysis, inferences might be biased. The aim of this paper is to illustrate the process of network meta-analysis with the aid of a working example on first-line medical treatment for primary open-angle glaucoma. We discuss the key assumption of network meta-analysis, as well as the unique considerations for developing appropriate research questions, conducting the literature search, abstracting data, performing qualitative and quantitative synthesis, presenting results, drawing conclusions, and reporting the findings in a network meta-analysis.},
	language = {eng},
	number = {1},
	journal = {Internal and Emergency Medicine},
	author = {Rouse, Benjamin and Chaimani, Anna and Li, Tianjing},
	month = feb,
	year = {2017},
	pmid = {27913917},
	pmcid = {PMC5247317},
	keywords = {Comparative effectiveness, Humans, Models, Statistical, Multiple treatment meta-analysis, Network Meta-Analysis As Topic, Network meta-analysis, Research, Research Design, Review Literature as Topic, Transitivity},
	pages = {103--111},
}

@article{wang_error_2020,
	title = {Error rates of human reviewers during abstract screening in systematic reviews},
	volume = {15},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0227742},
	doi = {10.1371/journal.pone.0227742},
	language = {en},
	number = {1},
	urldate = {2025-01-25},
	journal = {PLOS ONE},
	author = {Wang, Zhen and Nayfeh, Tarek and Tetzlaff, Jennifer and O’Blenis, Peter and Murad, Mohammad Hassan},
	editor = {Bencharit, Sompop},
	month = jan,
	year = {2020},
	pages = {e0227742},
}

@article{nama_successful_2021,
	title = {Successful incorporation of single reviewer assessments during systematic review screening: development and validation of sensitivity and work-saved of an algorithm that considers exclusion criteria and count},
	volume = {10},
	issn = {2046-4053},
	shorttitle = {Successful incorporation of single reviewer assessments during systematic review screening},
	url = {https://systematicreviewsjournal.biomedcentral.com/articles/10.1186/s13643-021-01632-6},
	doi = {10.1186/s13643-021-01632-6},
	abstract = {Abstract
            
              Background
              Accepted systematic review (SR) methodology requires citation screening by two reviewers to maximise retrieval of eligible studies. We hypothesized that records could be excluded by a single reviewer without loss of sensitivity in two conditions; the record was ineligible for multiple reasons, or the record was ineligible for one or more specific reasons that could be reliably assessed.
            
            
              Methods
              Twenty-four SRs performed at CHEO, a pediatric health care and research centre in Ottawa, Canada, were divided into derivation and validation sets. Exclusion criteria during abstract screening were sorted into 11 specific categories, with loss in sensitivity determined by individual category and by number of exclusion criteria endorsed. Five single reviewer algorithms that combined individual categories and multiple exclusion criteria were then tested on the derivation and validation sets, with success defined a priori as less than 5\% loss of sensitivity.
            
            
              Results
              
                The 24 SRs included 930 eligible and 27390 ineligible citations. The reviews were mostly focused on pediatrics (70.8\%,
                N
                =17/24), but covered various specialties. Using a single reviewer to exclude any citation led to an average loss of sensitivity of 8.6\% (95\%CI, 6.0–12.1\%). Excluding citations with ≥2 exclusion criteria led to 1.2\% average loss of sensitivity (95\%CI, 0.5–3.1\%). Five specific exclusion criteria performed with perfect sensitivity: conference abstract, ineligible age group, case report/series, not human research, and review article. In the derivation set, the five algorithms achieved a loss of sensitivity ranging from 0.0 to 1.9\% and work-saved ranging from 14.8 to 39.1\%. In the validation set, the loss of sensitivity for all 5 algorithms remained below 2.6\%, with work-saved between 10.5\% and 48.2\%.
              
            
            
              Conclusions
              Findings suggest that targeted application of single-reviewer screening, considering both type and number of exclusion criteria, could retain sensitivity and significantly decrease workload. Further research is required to investigate the potential for combining this approach with crowdsourcing or machine learning methodologies.},
	language = {en},
	number = {1},
	urldate = {2025-01-25},
	journal = {Systematic Reviews},
	author = {Nama, Nassr and Hennawy, Mirna and Barrowman, Nick and O’Hearn, Katie and Sampson, Margaret and McNally, James Dayre},
	month = dec,
	year = {2021},
	pages = {98},
}

@article{borah_analysis_2017,
	title = {Analysis of the time and workers needed to conduct systematic reviews of medical interventions using data from the {PROSPERO} registry},
	volume = {7},
	issn = {2044-6055},
	doi = {10.1136/bmjopen-2016-012545},
	abstract = {OBJECTIVES: To summarise logistical aspects of recently completed systematic reviews that were registered in the International Prospective Register of Systematic Reviews (PROSPERO) registry to quantify the time and resources required to complete such projects.
DESIGN: Meta-analysis.
DATA SOURCES AND STUDY SELECTION: All of the 195 registered and completed reviews (status from the PROSPERO registry) with associated publications at the time of our search (1 July 2014).
DATA EXTRACTION: All authors extracted data using registry entries and publication information related to the data sources used, the number of initially retrieved citations, the final number of included studies, the time between registration date to publication date and number of authors involved for completion of each publication. Information related to funding and geographical location was also recorded when reported.
RESULTS: The mean estimated time to complete the project and publish the review was 67.3 weeks (IQR=42). The number of studies found in the literature searches ranged from 27 to 92 020; the mean yield rate of included studies was 2.94\% (IQR=2.5); and the mean number of authors per review was 5, SD=3. Funded reviews took significantly longer to complete and publish (mean=42 vs 26 weeks) and involved more authors and team members (mean=6.8 vs 4.8 people) than those that did not report funding (both p{\textless}0.001).
CONCLUSIONS: Systematic reviews presently take much time and require large amounts of human resources. In the light of the ever-increasing volume of published studies, application of existing computing and informatics technology should be applied to decrease this time and resource burden. We discuss recently published guidelines that provide a framework to make finding and accessing relevant literature less burdensome.},
	language = {eng},
	number = {2},
	journal = {BMJ open},
	author = {Borah, Rohit and Brown, Andrew W. and Capers, Patrice L. and Kaiser, Kathryn A.},
	month = feb,
	year = {2017},
	pmid = {28242767},
	pmcid = {PMC5337708},
	keywords = {Biomedical Research, Databases, Bibliographic, Humans, Information Storage and Retrieval, PROSPERO registry, Publishing, Registries, Review Literature as Topic, Task Performance and Analysis, Workforce, metadata, search methods, systematic reviews},
	pages = {e012545},
}

@article{waffenschmidt_single_2019,
	title = {Single screening versus conventional double screening for study selection in systematic reviews: a methodological systematic review},
	volume = {19},
	issn = {1471-2288},
	shorttitle = {Single screening versus conventional double screening for study selection in systematic reviews},
	url = {https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-019-0782-0},
	doi = {10.1186/s12874-019-0782-0},
	language = {en},
	number = {1},
	urldate = {2025-01-25},
	journal = {BMC Medical Research Methodology},
	author = {Waffenschmidt, Siw and Knelangen, Marco and Sieben, Wiebke and Bühn, Stefanie and Pieper, Dawid},
	month = dec,
	year = {2019},
	pages = {132},
}

@article{gartlehner_single-reviewer_2020,
	title = {Single-reviewer abstract screening missed 13 percent of relevant studies: a crowd-based, randomized controlled trial},
	volume = {121},
	issn = {1878-5921},
	shorttitle = {Single-reviewer abstract screening missed 13 percent of relevant studies},
	doi = {10.1016/j.jclinepi.2020.01.005},
	abstract = {OBJECTIVES: To determine the accuracy of single-reviewer screening in correctly classifying abstracts as relevant or irrelevant for literature reviews.
STUDY DESIGN AND SETTING: We conducted a crowd-based, parallel-group randomized controlled trial. Using the Cochrane Crowd platform, we randomly assigned eligible participants to 100 abstracts each of a pharmacological or a public health topic. After completing a training exercise, participants screened abstracts online based on predefined inclusion and exclusion criteria. We calculated sensitivities and specificities of single- and dual-reviewer screening using two published systematic reviews as reference standards.
RESULTS: Two hundred and eighty participants made 24,942 screening decisions on 2,000 randomly selected abstracts from the reference standard reviews. On average, each abstract was screened 12 times. Overall, single-reviewer abstract screening missed 13\% of relevant studies (sensitivity: 86.6\%; 95\% confidence interval [CI], 80.6\%-91.2\%). By comparison, dual-reviewer abstract screening missed 3\% of relevant studies (sensitivity: 97.5\%; 95\% CI, 95.1\%-98.8\%). The corresponding specificities were 79.2\% (95\% CI, 77.4\%-80.9\%) and 68.7\% (95\% CI, 66.4\%-71.0\%), respectively.
CONCLUSIONS: Single-reviewer abstract screening does not appear to fulfill the high methodological standards that decisionmakers expect from systematic reviews. It may be a viable option for rapid reviews, which deliberately lower methodological standards to provide decision makers with accelerated evidence synthesis products.},
	language = {eng},
	journal = {Journal of Clinical Epidemiology},
	author = {Gartlehner, Gerald and Affengruber, Lisa and Titscher, Viktoria and Noel-Storr, Anna and Dooley, Gordon and Ballarini, Nicolas and König, Franz},
	month = may,
	year = {2020},
	pmid = {31972274},
	keywords = {Abstracting and Indexing, Accuracy, Adult, Data Accuracy, Depression, Female, Humans, Literature screening, Male, Peer Review, Research, Random Allocation, Randomized controlled trial, Rapid reviews, Regression Analysis, Sample Size, Sensitivity and Specificity, Sugar-Sweetened Beverages, Systematic Reviews as Topic, Systematic reviews},
	pages = {20--28},
}

@article{polanin_best_2019,
	title = {Best practice guidelines for abstract screening large‐evidence systematic reviews and meta‐analyses},
	volume = {10},
	issn = {1759-2879, 1759-2887},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/jrsm.1354},
	doi = {10.1002/jrsm.1354},
	abstract = {Abstract screening is one important aspect of conducting a high‐quality and comprehensive systematic review and meta‐analysis. Abstract screening allows the review team to conduct the tedious but vital first step to synthesize the extant literature: winnowing down the overwhelming amalgamation of citations discovered through research databases to the citations that should be “full‐text” screened and eventually included in the review. Although it is a critical process, few guidelines have been put forth since the publications of seminal systematic review textbooks. The purpose of this paper, therefore, is to provide a practical set of best practice guidelines to help future review teams and managers. Each of the 10 proposed guidelines is explained using real‐world examples or illustrations from applications. We also delineate recent experiences where a team of abstract screeners double‐screened 14 923 abstracts in 89 days.},
	language = {en},
	number = {3},
	urldate = {2025-01-25},
	journal = {Research Synthesis Methods},
	author = {Polanin, Joshua R. and Pigott, Terri D. and Espelage, Dorothy L. and Grotpeter, Jennifer K.},
	month = sep,
	year = {2019},
	pages = {330--342},
}

@article{ng_title_2014,
	title = {Title and {Abstract} {Screening} and {Evaluation} in {Systematic} {Reviews} ({TASER}): a pilot randomised controlled trial of title and abstract screening by medical students},
	volume = {3},
	issn = {2046-4053},
	shorttitle = {Title and {Abstract} {Screening} and {Evaluation} in {Systematic} {Reviews} ({TASER})},
	url = {https://systematicreviewsjournal.biomedcentral.com/articles/10.1186/2046-4053-3-121},
	doi = {10.1186/2046-4053-3-121},
	language = {en},
	number = {1},
	urldate = {2025-01-25},
	journal = {Systematic Reviews},
	author = {Ng, Lauren and Pitt, Veronica and Huckvale, Kit and Clavisi, Ornella and Turner, Tari and Gruen, Russell and Elliott, Julian H},
	month = dec,
	year = {2014},
	pages = {121},
}

@article{mateen_titles_2013,
	title = {Titles versus titles and abstracts for initial screening of articles for systematic reviews},
	volume = {5},
	issn = {1179-1349},
	doi = {10.2147/CLEP.S43118},
	abstract = {BACKGROUND: There is no consensus on whether screening titles alone or titles and abstracts together is the preferable strategy for inclusion of articles in a systematic review.
METHODS: TWO METHODS OF SCREENING ARTICLES FOR INCLUSION IN A SYSTEMATIC REVIEW WERE COMPARED: titles first versus titles and abstracts simultaneously. Each citation found in MEDLINE or Embase was reviewed by two physician reviewers for prespecified criteria: the citation included (1) primary data; (2) the exposure of interest; and (3) the outcome of interest.
RESULTS: There were 2965 unique citations. The titles first strategy resulted in an immediate rejection of 2558 (86\%) of the records after reading the title alone, requiring review of 239 titles and abstracts, and subsequently 176 full text articles. The simultaneous titles and abstracts review led to rejection of 2782 citations (94\%) and review of 183 full text articles. Interreviewer agreement to include an article for full text review using the titles-first screening strategy was 89\%-94\% (kappa = 0.54) and 96\%-97\% (kappa = 0.56) for titles and abstracts combined. The final systematic review included 13 articles, all of which were identified by both screening strategies (yield 100\%, burden 114\%). Precision was higher in the titles and abstracts method (7.1\% versus 3.2\%) but recall was the same (100\% versus 100\%), leading to a higher F-measure for the titles and abstracts approach (0.1327 versus 0.0619).
CONCLUSION: Screening via a titles-first approach may be more efficient than screening titles and abstracts together.},
	language = {eng},
	journal = {Clinical Epidemiology},
	author = {Mateen, Farrah J. and Oh, Jiwon and Tergas, Ana I. and Bhayani, Neil H. and Kamdar, Biren B.},
	year = {2013},
	pmid = {23526335},
	pmcid = {PMC3604876},
	keywords = {epidemiology, meta-analysis, research methods, systematic review},
	pages = {89--95},
}

@article{teo_title-plus-abstract_2023,
	title = {Title-plus-abstract versus title-only first-level screening approach: a case study using a systematic review of dietary patterns and sarcopenia risk to compare screening performance},
	volume = {12},
	issn = {2046-4053},
	shorttitle = {Title-plus-abstract versus title-only first-level screening approach},
	url = {https://systematicreviewsjournal.biomedcentral.com/articles/10.1186/s13643-023-02374-3},
	doi = {10.1186/s13643-023-02374-3},
	abstract = {Abstract
            
              Background
              Conducting a systematic review is a time- and resource-intensive multi-step process. Enhancing efficiency without sacrificing accuracy and rigor during the screening phase of a systematic review is of interest among the scientific community.
            
            
              Methods
              This case study compares the screening performance of a title-only (Ti/O) screening approach to the more conventional title-plus-abstract (Ti + Ab) screening approach. Both Ti/O and Ti + Ab screening approaches were performed simultaneously during first-level screening of a systematic review investigating the relationship between dietary patterns and risk factors and incidence of sarcopenia. The qualitative and quantitative performance of each screening approach was compared against the final results of studies included in the systematic review, published elsewhere, which used the standard Ti + Ab approach. A statistical analysis was conducted, and contingency tables were used to compare each screening approach in terms of false inclusions and false exclusions and subsequent sensitivity, specificity, accuracy, and positive predictive power.
            
            
              Results
              
                Thirty-eight citations were included in the final analysis, published elsewhere. The current case study found that the Ti/O first-level screening approach correctly identified 22 citations and falsely excluded 16 citations, most often due to titles lacking a clear indicator of study design or outcomes relevant to the systematic review eligibility criteria. The Ti + Ab approach correctly identified 36 citations and falsely excluded 2 citations due to limited population and intervention descriptions in the abstract. Our analysis revealed that the performance of the Ti + Ab first-level screening was statistically different compared to the average performance of both approaches (Chi-squared: 5.21,
                p
                value 0.0225) while the Ti/O approach was not (chi-squared: 2.92,
                p
                value 0.0874). The predictive power of the first-level screening was 14.3\% and 25.5\% for the Ti/O and Ti + Ab approaches, respectively. In terms of sensitivity, 57.9\% of studies were correctly identified at the first-level screening stage using the Ti/O approach versus 94.7\% by the Ti + Ab approach.
              
            
            
              Conclusions
              In the current case study comparing two screening approaches, the Ti + Ab screening approach captured more relevant studies compared to the Ti/O approach by including a higher number of accurately eligible citations. Ti/O screening may increase the likelihood of missing evidence leading to evidence selection bias.
            
            
              Systematic review registration
              PROSPERO Protocol Number: CRD42020172655.},
	language = {en},
	number = {1},
	urldate = {2025-01-25},
	journal = {Systematic Reviews},
	author = {Teo, Lynn and Van Elswyk, Mary E. and Lau, Clara S. and Shanahan, Christopher J.},
	month = nov,
	year = {2023},
	pages = {211},
}

@article{ramirez_adherence_2022,
	title = {Adherence to systematic review standards: {Impact} of librarian involvement in {Campbell} {Collaboration}'s education reviews},
	volume = {48},
	issn = {00991333},
	shorttitle = {Adherence to systematic review standards},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0099133322000830},
	doi = {10.1016/j.acalib.2022.102567},
	language = {en},
	number = {5},
	urldate = {2025-01-25},
	journal = {The Journal of Academic Librarianship},
	author = {Ramirez, Diana and Foster, Margaret J. and Kogut, Ashlynn and Xiao, Daniel},
	month = sep,
	year = {2022},
	pages = {102567},
}

@article{pawliuk_librarian_2024,
	title = {Librarian involvement in systematic reviews was associated with higher quality of reported search methods: a cross-sectional survey},
	volume = {166},
	issn = {1878-5921},
	shorttitle = {Librarian involvement in systematic reviews was associated with higher quality of reported search methods},
	doi = {10.1016/j.jclinepi.2023.111237},
	abstract = {OBJECTIVES: Systematic reviews (SRs) are considered the gold standard of evidence, but many published SRs are of poor quality. This study identifies how librarian involvement in SRs is associated with quality-reported methods and examines the lack of motivation for involving a librarian in SRs.
STUDY DESIGN AND SETTING: We searched databases for SRs that were published by a first or last author affiliated to a Vancouver hospital or biomedical research site and published between 2015 and 2019. Corresponding authors of included SRs were contacted through an e-mail survey to determine if a librarian was involved in the SR. If a librarian was involved in the SR, the survey asked at what level the librarian was involved and if a librarian was not involved, the survey asked why. Quality of reported search methods was scored independently by two reviewers. A linear regression model was used to determine the association between quality of reported search methods scores and the level at which a librarian was involved in the study.
RESULTS: One hundred ninety one SRs were included in this study and 118 (62\%) of the SRs authors indicated whether a librarian was involved in the SR. SRs that included a librarian as a co-author had a 15.4\% higher quality assessment score than SRs that did not include a librarian. Most authors (27; 75\%) who did not include a librarian in their SR did not do so because they did not believe it was necessary.
CONCLUSION: Higher level of librarian involvement in SRs is correlated with higher scores in reported search methods. Greater advocacy or changes at the policy level is necessary to increase librarian involvement in SRs and as a result the quality of their search methods.},
	language = {eng},
	journal = {Journal of Clinical Epidemiology},
	author = {Pawliuk, Colleen and Cheng, Shannon and Zheng, Alex and Siden, Harold Hal},
	month = feb,
	year = {2024},
	pmid = {38072177},
	keywords = {Attitude, Barriers to evidence, Cross-Sectional Studies, Humans, Librarian contribution, Librarians, Library services, Methodological quality, Publications, Reporting, Research collaboration, Search methods, Surveys and Questionnaires, Systematic Reviews as Topic, Systematic review},
	pages = {111237},
}

@article{giroudon_qualite_2023,
	title = {Qualité en revue systématique : apport du documentaliste:},
	volume = {n° 1},
	issn = {2428-2111},
	shorttitle = {Qualité en revue systématique},
	url = {https://www.cairn.info/revue-i2d-information-donnees-et-documents-2023-1-page-114.htm?ref=doi},
	doi = {10.3917/i2d.231.0114},
	abstract = {Les documentalistes ont un rôle à jouer dans la démarche quantitative et qualitative de la recherche biomédicale, pour peu que s’établisse une relation de confiance entre eux et les chercheurs-praticiens. En effet, les revues systématiques (RS) de la littérature, bien conduites, peuvent appuyer la pratique de la médecine factuelle. Ce partage d’expérience est un regard croisé d’un documentaliste et de deux cliniciens hospitaliers sur leur collaboration dans la réalisation de huit RS. Il détaille le soutien apporté par le documentaliste et illustre combien la posture des cliniciens influence la qualité du service rendu, et par conséquent la crédibilité qui découle de ces travaux.},
	number = {1},
	urldate = {2025-01-25},
	journal = {I2D - Information, données \& documents},
	author = {Giroudon, Caroline and Armoiry, Xavier and Fieux, Maxime},
	month = jul,
	year = {2023},
	pages = {114--125},
}

@article{schvaneveldt_assessing_2021,
	title = {Assessing the roles and challenges of librarians in dental systematic and scoping reviews},
	volume = {109},
	issn = {1558-9439},
	doi = {10.5195/jmla.2021.1031},
	abstract = {OBJECTIVE: The objective of this study was to determine the scope of experience, roles, and challenges that librarians face in participating in dental and oral health systematic and scoping reviews to inform outreach efforts to researchers and identify areas for librarian professional development.
METHODS: The authors developed a twenty-three-item survey based on the findings of two recent articles about health sciences librarians' roles and challenges in conducting systematic and scoping reviews. The survey was distributed via electronic mailing lists to librarians who were likely to have participated in conducting dental systematic and scoping reviews.
RESULTS: While survey respondents reported participating in many dental reviews, they participated more commonly in systematic reviews than in scoping reviews. Also, they worked less commonly on dental and oral health reviews than on non-dental reviews. Librarian roles in dental reviews tended to follow traditional librarian roles: all respondents had participated in planning and information retrieval stages, whereas fewer respondents had participated in screening and assessing articles. The most frequently reported challenges involved the lead reviewer or review team rather than the librarians themselves, with time- and methodology-related challenges being most common.
CONCLUSIONS: Although librarians might not be highly involved in dental and oral health systematic and scoping reviews, more librarian participation in these reviews, either as methodologists or information experts, may improve their reviews' overall quality.},
	language = {eng},
	number = {1},
	journal = {Journal of the Medical Library Association: JMLA},
	author = {Schvaneveldt, Nena and Stellrecht, Elizabeth M.},
	month = jan,
	year = {2021},
	pmid = {33424464},
	pmcid = {PMC7772970},
	keywords = {Education, Dental, Evidence-Based Dentistry, Humans, Information Storage and Retrieval, Librarians, Libraries, Medical, Library Services, Professional Role, Review Literature as Topic},
	pages = {52--61},
}

@article{brunskill_case_2022,
	title = {The case of the disappearing librarians: analyzing documentation of librarians' contributions to systematic reviews},
	volume = {110},
	issn = {1558-9439},
	shorttitle = {The case of the disappearing librarians},
	doi = {10.5195/jmla.2022.1505},
	abstract = {OBJECTIVE: The study aimed to analyze the documented role of a librarian in published systematic reviews and meta-analyses whose registered protocols mentioned librarian involvement. The intention was to identify how, or if, librarians' involvement was formally documented, how their contributions were described, and if there were any potential connections between this documentation and basic metrics of search reproducibility and quality.
METHODS: Reviews whose PROSPERO protocols were registered in 2017 and 2018 and that also specifically mentioned a librarian were analyzed for documentation of the librarian's involvement. Language describing the librarian and their involvement was gathered and coded, and additional information about the review, including search strategy details, was also collected.
RESULTS: A total of 209 reviews were found and analyzed. Of these, 28\% had a librarian co-author, 41\% named a librarian in the acknowledgements section, and 78\% mentioned the contribution of a librarian within the body of the review. However, mentions of a librarian within the review were often generic ("a librarian") and in 31\% of all reviews analyzed no librarian was specified by name. In 9\% of the reviews, there was no reference to a librarian found at all. Language about librarians' contributions usually only referenced their work with search strategy development. Reviews with librarian coauthors typically described the librarian's work in active voice centering the librarian, unlike reviews without librarian coauthors. Most reviews had reproducible search strategies that utilized subject headings and keywords, but some had flawed or missing strategies.
CONCLUSION: Even among this set of reviews, where librarian involvement was specified at the protocol level, librarians' contributions were often described with minimal, or even no, language in the final published review. Much room for improvement appears to remain in terms of how librarians' work is documented.},
	language = {eng},
	number = {4},
	journal = {Journal of the Medical Library Association: JMLA},
	author = {Brunskill, Amelia and Hanneke, Rosie},
	month = oct,
	year = {2022},
	pmid = {37101926},
	pmcid = {PMC10124603},
	keywords = {Benchmarking, Documentation, Humans, Librarians, Reproducibility of Results, Systematic reviews, documentation, librarians},
	pages = {409--418},
}

@misc{le_benchmarking_2023,
	title = {Benchmarking {Librarian} {Support} of {Systematic} {Reviews} in the {Sciences}, {Humanities}, and {Social} {Sciences}},
	copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
	url = {https://osf.io/v7m9y},
	doi = {10.31219/osf.io/v7m9y},
	abstract = {Systematic reviews, along with other types of knowledge synthesis, are a research methodology that attempt to find all available evidence on a topic to help answer specific questions. Librarian involvement in systematic reviews is well established in the health sciences and in recent years there has been growing awareness of, and literature about, librarians outside of health supporting systematic reviews. This study benchmarks librarian support of systematic reviews in the sciences, humanities, and social sciences (SHSS) by looking at the growth of demand for support, the disciplines requesting this kind of librarian support, and the specific types of support needed. It also delves into what SHSS librarians need to be successful in this type of work, including administrative support and workload adjustments.},
	urldate = {2025-01-25},
	publisher = {Open Science Framework},
	author = {Lê, Mê-Linh and Neilson, Christine Joann and Winkler, Janice},
	month = apr,
	year = {2023},
}

@incollection{cumpston_chapter_2024,
	edition = {Version 6.5},
	title = {Chapter {II}: {Planning} a {Cochrane} {Review}},
	url = {https://training.cochrane.org/handbook/current/chapter-ii#section-ii-1-4},
	booktitle = {Cochrane {Handbook} for {Systematic} {Reviews} of {Interventions}},
	publisher = {Cochrane},
	author = {Cumpston, Miranda and Flemyng, Ella},
	editor = {Higgins, Julian P. T. and Thomas, James and Chandler, Jacqueline and Cumpston, Miranda and Li, Tianjing and Page, Matthew J. and Welch, Vivian A.},
	year = {2024},
}

@article{cochrane_z_inactive_heart_group_preoperative_2024,
	title = {Preoperative statin therapy for adults undergoing cardiac surgery},
	volume = {2024},
	issn = {14651858},
	url = {http://doi.wiley.com/10.1002/14651858.CD008493.pub5},
	doi = {10.1002/14651858.CD008493.pub5},
	language = {en},
	number = {7},
	urldate = {2025-01-25},
	journal = {Cochrane Database of Systematic Reviews},
	author = {Marques Antunes, Miguel and Nunes-Ferreira, Afonso and Duarte, Gonçalo S and Gouveia E Melo, Ryan and Sucena Rodrigues, Bárbara and Guerra, Nuno C and Nobre, Angelo and Pinto, Fausto J and Costa, João and Caldeira, Daniel},
	editor = {{Cochrane Z\_INACTIVE\_Heart Group}},
	month = jul,
	year = {2024},
}

@article{cipriani_what_2011,
	title = {What is a {Cochrane} review?},
	volume = {20},
	issn = {2045-7960},
	doi = {10.1017/s2045796011000436},
	abstract = {Systematic reviews carried out by Cochrane Collaboration (an international network of researchers belonging to this independent, not-for-profit organization) are recognized worldwide as the highest standard in evidence-based healthcare. The main reason is that Cochrane reviews follow a common and specific methodology to limit bias and random error. In this issue, we highlight the most important methodological features of Cochrane reviews, also reporting details on the editorial process to publish the review in the Cochrane Library.},
	language = {eng},
	number = {3},
	journal = {Epidemiology and Psychiatric Sciences},
	author = {Cipriani, A. and Furukawa, T. A. and Barbui, C.},
	month = sep,
	year = {2011},
	pmid = {21922964},
	keywords = {Databases, Bibliographic, Evidence-Based Medicine, Humans, Information Services, International Cooperation, Publishing, Review Literature as Topic},
	pages = {231--233},
}

@article{munn_what_2018,
	title = {What kind of systematic review should {I} conduct? {A} proposed typology and guidance for systematic reviewers in the medical and health sciences},
	volume = {18},
	issn = {1471-2288},
	shorttitle = {What kind of systematic review should {I} conduct?},
	url = {https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-017-0468-4},
	doi = {10.1186/s12874-017-0468-4},
	language = {en},
	number = {1},
	urldate = {2025-01-24},
	journal = {BMC Medical Research Methodology},
	author = {Munn, Zachary and Stern, Cindy and Aromataris, Edoardo and Lockwood, Craig and Jordan, Zoe},
	month = dec,
	year = {2018},
	pages = {5},
}

@article{azzam_anemia_2025,
	title = {Anemia in pregnancy: a systematic review and meta-analysis of prevalence, determinants, and health impacts in {Egypt}},
	volume = {25},
	issn = {1471-2393},
	shorttitle = {Anemia in pregnancy},
	url = {https://bmcpregnancychildbirth.biomedcentral.com/articles/10.1186/s12884-024-07111-9},
	doi = {10.1186/s12884-024-07111-9},
	abstract = {Abstract
            
              Background
              The WHO considers anemia in pregnancy a severe public health issue when prevalence surpasses 40\%. In response, we conducted a systematic review and meta-analysis to examine anemia among pregnant women in Egypt, focusing on its prevalence, determinants, and associated complications.
            
            
              Methods
              We conducted a systematic literature search for studies published between January 1, 2010, and August 18, 2024, to identify studies from Egypt reporting on anemia in pregnant women, including its prevalence, associated determinants, and complications. A meta-analysis was conducted using a random-effects model to estimate pooled prevalence, odds ratios (OR), and standardized mean differences (SMD). Sensitivity analyses and publication bias were performed. All statistical analyses were conducted using R software.
            
            
              Results
              
                Eighteen studies met the eligibility criteria with a total sample size of 14,548. The overall prevalence of anemia among pregnant women was 49\% (95\% CI: 42–57), with no significant difference between Upper and Lower Egypt (
                P
                 = 0.66). The sensitivity analysis demonstrated the absence of influential outliers and Egger’s test indicated no evidence of publication bias (
                P
                 = 0.17). Anemia prevalence was significantly higher in the third trimester (65\%) compared to the second trimester (47\%) (
                P
                 = 0.03). Among anemic pregnant women, most cases were mild (47\%) and moderate (47\%). The determinants of anemia among pregnant women included being over 30 years old (OR: 1.95), residing in rural areas (OR: 1.76), illiteracy (OR: 1.93), birth spacing {\textless} 2 years (OR: 2.04), lack of iron supplementation (OR: 2.59), presence of intestinal parasites (OR: 1.38), antenatal visits {\textless} 5 (OR: 5.27), multiparity, and low income, all with statistical significance (
                p
                 {\textless} 0.05). Regarding dietary determinants, a low intake of meat, vegetables, fruits, and high tea consumption was consistently associated with a higher risk of anemia. For neonatal complications, infants born to anemic mothers had significantly lower Apgar scores, gestational ages, and birth weights (
                P
                 {\textless} 0.05), with birth weight being the most adversely impacted (SMD = -1.3).
              
            
            
              Conclusions
              This meta-analysis shows 49\% anemia prevalence in pregnant Egyptian women, indicating severe health concern. The findings highlight the urgent need for targeted interventions aimed at addressing the key determinants identified in this study.},
	language = {en},
	number = {1},
	urldate = {2025-01-24},
	journal = {BMC Pregnancy and Childbirth},
	author = {Azzam, Ahmed and Khaled, Heba and Alrefaey, Alrefaey K. and Basil, Amar and Ibrahim, Sarah and Elsayed, Mohamed S. and Khattab, Muhammad and Nabil, Nashwa and Abdalwanees, Esraa and Halim, Hala Waheed Abdel},
	month = jan,
	year = {2025},
	pages = {29},
}

@article{turkmen_systematic_2025,
	title = {Systematic {Review} and {Meta}-{Analysis}: {The} {Association} {Between} {Newer} {Generation} {Antidepressants} and {Insomnia} in {Children} and {Adolescents} {With} {Major} {Depressive} {Disorder}},
	issn = {1527-5418},
	shorttitle = {Systematic {Review} and {Meta}-{Analysis}},
	doi = {10.1016/j.jaac.2025.01.006},
	abstract = {OBJECTIVE: To examine the association between newer generation antidepressants and insomnia as an adverse event (AE) in the treatment of children and adolescents with major depressive disorder (MDD).
METHOD: A systematic search was performed in major databases (inception to August 31st, 2023) to retrieve double-blind, placebo-controlled, randomized controlled trials (RCTs) evaluating the safety of 19 antidepressants in the acute treatment (initial 6 to 12 weeks) of children and adolescents aged ≤ 18 years with MDD (primary analyses). RCTs in anxiety disorders and obsessive-compulsive disorder (OCD) were retrieved from a recent meta-analysis and included in complementary analyses. A mixed-effects logistic regression model was used to compare the frequency of insomnia in the antidepressant relative to the placebo group. Risk of bias was evaluated using the Cochrane Risk of Bias 2 tool.
RESULTS: In total, 20 trials in MDD (N = 5,357) and 8 trials in anxiety disorders and OCD (N = 1,271) evaluating selective serotonin reuptake inhibitors (SSRIs) or serotonin-norepinephrine reuptake inhibitors (SNRIs) were included. In MDD, antidepressant treatment was associated with a modest increase in the odds of insomnia compared with placebo (OR = 1.65, 95\% CI = 1.21-2.27, p = 0.002), with no significant difference between SSRIs and SNRIs. The RCTs showed low risk of bias or minor concerns for the assessment of insomnia. The odds of treatment-emergent insomnia were significantly lower in MDD (OR = 1.62; 95\% CI = 1.21-2.15) compared to anxiety disorders and OCD (OR = 2.89; 95\% CI = 1.83-4.57) for treatment with SSRIs (p = 0.03). Among individual antidepressants with evidence from ≥ 3 studies, sertraline had the highest OR (3.45; 95\% CI = 1.91-6.24), while duloxetine had the lowest OR (1.38; 95\% CI = 0.79 - 2.43).
CONCLUSION: Children and adolescents are at a modestly increased risk of experiencing insomnia during the first 6 to 12 weeks of treatment with SSRIs and SNRIs. Antidepressant- and disorder-specific variability in the risk of treatment-emergent insomnia may be relevant to consider in clinical decision-making.},
	language = {eng},
	journal = {Journal of the American Academy of Child and Adolescent Psychiatry},
	author = {Türkmen, Cagdas and Machunze, Noah and Lee, Alycia M. and Bougelet, Emilie and Ludin, Nicola M. and de Cates, Angharad N. and Vollstädt-Klein, Sabine and Bach, Patrick and Kiefer, Falk and Andreas, Jasmina Burdzovic and Kamphuis, Jeanine and Schoevers, Robert A. and Emslie, Graham J. and Hetrick, Sarah E. and Viechtbauer, Wolfgang and van Dalfsen, Jens H.},
	month = jan,
	year = {2025},
	pmid = {39828036},
	keywords = {antidepressants, children and adolescents, insomnia, major depressive disorder, meta-analysis},
	pages = {S0890--8567(25)00013--9},
}

@article{parsa_endometriosis_2025,
	title = {Endometriosis and risk of cardiovascular disease: a systematic review and meta-analysis},
	volume = {25},
	issn = {1471-2458},
	shorttitle = {Endometriosis and risk of cardiovascular disease},
	url = {https://bmcpublichealth.biomedcentral.com/articles/10.1186/s12889-025-21486-0},
	doi = {10.1186/s12889-025-21486-0},
	language = {en},
	number = {1},
	urldate = {2025-01-24},
	journal = {BMC Public Health},
	author = {Parsa, Sina and Noroozpoor, Rashin and Dehghanbanadaki, Hojat and Khateri, Sorour and Moradi, Yousef},
	month = jan,
	year = {2025},
	pages = {245},
}

@article{ioannidis_mass_2016,
	title = {The {Mass} {Production} of {Redundant}, {Misleading}, and {Conflicted} {Systematic} {Reviews} and {Meta}‐analyses},
	volume = {94},
	issn = {0887-378X, 1468-0009},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/1468-0009.12210},
	doi = {10.1111/1468-0009.12210},
	abstract = {Policy Points
                    :
                  
                  
                    
                      
                        Currently, there is massive production of unnecessary, misleading, and conflicted systematic reviews and meta‐analyses. Instead of promoting evidence‐based medicine and health care, these instruments often serve mostly as easily produced publishable units or marketing tools.
                      
                      
                        Suboptimal systematic reviews and meta‐analyses can be harmful given the major prestige and influence these types of studies have acquired.
                      
                      
                        The publication of systematic reviews and meta‐analyses should be realigned to remove biases and vested interests and to integrate them better with the primary production of evidence.
                      
                    
                  
                
              
            
            
              Context
              Currently, most systematic reviews and meta‐analyses are done retrospectively with fragmented published information. This article aims to explore the growth of published systematic reviews and meta‐analyses and to estimate how often they are redundant, misleading, or serving conflicted interests.
            
            
              Methods
              Data included information from PubMed surveys and from empirical evaluations of meta‐analyses.
            
            
              Findings
              Publication of systematic reviews and meta‐analyses has increased rapidly. In the period January 1, 1986, to December 4, 2015, PubMed tags 266,782 items as “systematic reviews” and 58,611 as “meta‐analyses.” Annual publications between 1991 and 2014 increased 2,728\% for systematic reviews and 2,635\% for meta‐analyses versus only 153\% for all PubMed‐indexed items. Currently, probably more systematic reviews of trials than new randomized trials are published annually. Most topics addressed by meta‐analyses of randomized trials have overlapping, redundant meta‐analyses; same‐topic meta‐analyses may exceed 20 sometimes. Some fields produce massive numbers of meta‐analyses; for example, 185 meta‐analyses of antidepressants for depression were published between 2007 and 2014. These meta‐analyses are often produced either by industry employees or by authors with industry ties and results are aligned with sponsor interests. China has rapidly become the most prolific producer of English‐language, PubMed‐indexed meta‐analyses. The most massive presence of Chinese meta‐analyses is on genetic associations (63\% of global production in 2014), where almost all results are misleading since they combine fragmented information from mostly abandoned era of candidate genes. Furthermore, many contracting companies working on evidence synthesis receive industry contracts to produce meta‐analyses, many of which probably remain unpublished. Many other meta‐analyses have serious flaws. Of the remaining, most have weak or insufficient evidence to inform decision making. Few systematic reviews and meta‐analyses are both non‐misleading and useful.
            
            
              Conclusions
              The production of systematic reviews and meta‐analyses has reached epidemic proportions. Possibly, the large majority of produced systematic reviews and meta‐analyses are unnecessary, misleading, and/or conflicted.},
	language = {en},
	number = {3},
	urldate = {2025-01-24},
	journal = {The Milbank Quarterly},
	author = {Ioannidis, John P.A.},
	month = sep,
	year = {2016},
	pages = {485--514},
}

@article{collaborative_working_group_from_the_conference_keeping_the_pool_clean_prevention_and_management_of_misconduct_related_retractions_repair_2018,
	title = {{RePAIR} consensus guidelines: {Responsibilities} of {Publishers}, {Agencies}, {Institutions}, and {Researchers} in protecting the integrity of the research record},
	volume = {3},
	issn = {2058-8615},
	shorttitle = {{RePAIR} consensus guidelines},
	url = {https://researchintegrityjournal.biomedcentral.com/articles/10.1186/s41073-018-0055-1},
	doi = {10.1186/s41073-018-0055-1},
	language = {en},
	number = {1},
	urldate = {2025-01-24},
	journal = {Research Integrity and Peer Review},
	author = {{Collaborative Working Group from the conference “Keeping the Pool Clean: Prevention and Management of Misconduct Related Retractions”}},
	month = dec,
	year = {2018},
	pages = {15},
}

@incollection{congiunti_ethics_2023,
	address = {Cham},
	title = {Ethics and {Integrity} in {Academic} {Publishing}},
	isbn = {978-3-031-24059-1 978-3-031-24060-7},
	url = {https://link.springer.com/10.1007/978-3-031-24060-7_5},
	language = {en},
	urldate = {2025-01-24},
	booktitle = {Ethics in {Research}},
	publisher = {Springer Nature Switzerland},
	author = {Caporale, Cinzia and Zagarella, Roberta Martina},
	editor = {Congiunti, Lorella and Lo Piccolo, Francesco and Russo, Antonio and Serio, Mario},
	year = {2023},
	doi = {10.1007/978-3-031-24060-7_5},
	note = {Series Title: UNIPA Springer Series},
	pages = {53--69},
}

@article{kearney_research_2024,
	title = {Research integrity and academic medicine: the pressure to publish and research misconduct},
	volume = {124},
	issn = {2702-3648},
	shorttitle = {Research integrity and academic medicine},
	doi = {10.1515/jom-2023-0211},
	abstract = {CONTEXT: This narrative review article explores research integrity and the implications of scholarly work in medical education. The paper describes how the current landscape of medical education emphasizes research and scholarly activity for medical students, resident physicians, and faculty physician educators. There is a gap in the existing literature that fully explores research integrity, the challenges surrounding the significant pressure to perform scholarly activity, and the potential for ethical lapses by those involved in medical education.
OBJECTIVES: The objectives of this review article are to provide a background on authorship and publication safeguards, outline common types of research misconduct, describe the implications of publication in medical education, discuss the consequences of ethical breaches, and outline possible solutions to promote research integrity in academic medicine.
METHODS: To complete this narrative review, the authors explored the current literature utilizing multiple databases beginning in June of 2021, and they completed the literature review in January of 2023. To capture the wide scope of the review, numerous searches were performed. A number of Medical Subject Headings (MeSH) terms were utilized to identify relevant articles. The MeSH terms included "scientific misconduct," "research misconduct," "authorship," "plagiarism," "biomedical research/ethics," "faculty, medical," "fellowships and scholarships," and "internship and residency." Additional references were accessed to include medical school and residency accreditation standards, residency match statistics, regulatory guidelines, and standard definitions.
RESULTS: Within the realm of academic medicine, research misconduct and misrepresentation continue to occur without clear solutions. There is a wide range of severity in breaches of research integrity, ranging from minor infractions to fraud. Throughout the medical education system in the United States, there is pressure to publish research and scholarly work. Higher rates of publications are associated with a successful residency match for students and academic promotion for faculty physicians. For those who participate in research misconduct, there is a multitude of potential adverse consequences. Potential solutions to ensure research integrity exist but are not without barriers to implementation.
CONCLUSIONS: Pressure in the world of academic medicine to publish contributes to the potential for research misconduct and authorship misrepresentation. Lapses in research integrity can result in a wide range of potentially adverse consequences for the offender, their institution, the scientific community, and the public. If adopted, universal research integrity policies and procedures could make major strides in eliminating research misconduct in the realm of academic medicine.},
	language = {eng},
	number = {5},
	journal = {Journal of Osteopathic Medicine},
	author = {Kearney, Molly and Downing, Maren and Gignac, Elizabeth A.},
	month = may,
	year = {2024},
	pmid = {38407191},
	keywords = {Authorship, Biomedical Research, Education, Medical, Ethics, Research, Humans, Publishing, Scientific Misconduct, authorship, internship, medical education, plagiarism, research integrity, residency},
	pages = {187--194},
}

@book{noblit_meta-ethnography_1988,
	series = {Qualitative {Research} {Methods}},
	title = {Meta-{Ethnography}: {Synthesizing} {Qualitative} {Studies}},
	volume = {11},
	isbn = {978-0-8039-3023-0},
	url = {https://methods.sagepub.com/book/meta-ethnography},
	publisher = {SAGE Publications, Inc},
	author = {Noblit, George W. and Hare, R. Dwight},
	year = {1988},
}

@article{moher_preferred_2010,
	title = {Preferred reporting items for systematic reviews and meta-analyses: the {PRISMA} statement},
	volume = {8},
	issn = {1743-9159},
	shorttitle = {Preferred reporting items for systematic reviews and meta-analyses},
	doi = {10.1016/j.ijsu.2010.02.007},
	language = {eng},
	number = {5},
	journal = {International Journal of Surgery (London, England)},
	author = {Moher, David and Liberati, Alessandro and Tetzlaff, Jennifer and Altman, Douglas G. and {PRISMA Group}},
	year = {2010},
	pmid = {20171303},
	keywords = {Evidence-Based Practice, Humans, Meta-Analysis as Topic, Periodicals as Topic, Publication Bias, Publishing, Quality Control, Research Design, Review Literature as Topic, Terminology as Topic},
	pages = {336--341},
}

@article{lefebvre_cochrane_2011,
	title = {Cochrane handbook for systematic reviews of interventions},
	journal = {Oxfordshire, UK: The Cochrane Collaboration},
	author = {Lefebvre, Carol and Manheimer, E and Glanville, J and Higgins, JPT and Green, S and {others}},
	year = {2011},
}

@article{tawfik_protocol_2020,
	title = {Protocol registration issues of systematic review and meta-analysis studies: a survey of global researchers},
	volume = {20},
	issn = {1471-2288},
	shorttitle = {Protocol registration issues of systematic review and meta-analysis studies},
	url = {https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-020-01094-9},
	doi = {10.1186/s12874-020-01094-9},
	abstract = {Abstract
            
              Background
              Although protocol registration of systematic reviews/meta-analysis (SR/MA) is still not mandatory, it is highly recommended that authors publish their SR/MA protocols prior to submitting their manuscripts for publication as recommended by the Cochrane guidelines for conducting SR/MAs. our aim was to assess the awareness, obstacles, and opinions of SR/MA authors about the protocol registration process.
            
            
              Methods
              A cross-sectional survey study included the authors who published SR/MAs during the period from 2010 to 2016, and they were contacted for participation in our survey study. They were identified through the literature search of SR/MAs in Scopus database. An online questionnaire was sent to each participant via e-mail after receiving their approval to join the study. We have sent 6650 emails and received 275 responses.
            
            
              Results
              A total of 270 authors responses were complete and included in the final analysis. Our results has shown that PROSPERO was the most common database used for protocol registration (71.3\%). The registration-to-acceptance time interval in PROSPERO was less than 1 month (99.1\%). Almost half of the authors (44.2\%) did not register their protocols prior to publishing their SR/MAs and according to their opinion that the other authors lack knowledge of protocol importance and mandance to be registered, was the most commonly reported reason (44.9\%). A significant percenatge of respondents (37.4\%) believed that people would steal their ideas from protocol databases, while only 5.3\% reported that their SR/MA had been stolen. However, the majority (72.9\%) of participants have agreed that protocol registries play a role in preventing unnecessary duplication of reviews. Finally, 37.4\% of participants agree that SR/MA protocol registration should be mandatory.
            
            
              Conclusion
              About half of the participants believes that the main reason for not registering protocols, is that the other authors lack knowledge concerning obligation and importance to register the SR/MA protocols in advance. Therefore, tools should be available to mandate protocol registration of any SRs beforehand and increasing awareness about the benefits of protocol registration among researchers.},
	language = {en},
	number = {1},
	urldate = {2025-01-23},
	journal = {BMC Medical Research Methodology},
	author = {Tawfik, Gehad Mohamed and Giang, Hoang Thi Nam and Ghozy, Sherief and Altibi, Ahmed M. and Kandil, Hend and Le, Huu-Hoai and Eid, Peter Samuel and Radwan, Ibrahim and Makram, Omar Mohamed and Hien, Tong Thi Thu and Sherif, Mahmoud and Hossain, As-Saba and Thang, Tai Luu Lam and Puljak, Livia and Salem, Hosni and Numair, Tarek and Moji, Kazuhiko and Huy, Nguyen Tien},
	month = dec,
	year = {2020},
	pages = {213},
}

@article{whiting_proposed_2017,
	title = {A proposed framework for developing quality assessment tools},
	volume = {6},
	issn = {2046-4053},
	url = {http://systematicreviewsjournal.biomedcentral.com/articles/10.1186/s13643-017-0604-6},
	doi = {10.1186/s13643-017-0604-6},
	language = {en},
	number = {1},
	urldate = {2025-01-23},
	journal = {Systematic Reviews},
	author = {Whiting, Penny and Wolff, Robert and Mallett, Susan and Simera, Iveta and Savović, Jelena},
	month = dec,
	year = {2017},
	pages = {204},
}

@article{rethlefsen_prisma-s_2021,
	title = {{PRISMA}-{S}: an extension to the {PRISMA} {Statement} for {Reporting} {Literature} {Searches} in {Systematic} {Reviews}},
	volume = {10},
	issn = {2046-4053},
	shorttitle = {{PRISMA}-{S}},
	url = {https://systematicreviewsjournal.biomedcentral.com/articles/10.1186/s13643-020-01542-z},
	doi = {10.1186/s13643-020-01542-z},
	abstract = {Abstract
            
              Background
              Literature searches underlie the foundations of systematic reviews and related review types. Yet, the literature searching component of systematic reviews and related review types is often poorly reported. Guidance for literature search reporting has been diverse, and, in many cases, does not offer enough detail to authors who need more specific information about reporting search methods and information sources in a clear, reproducible way. This document presents the PRISMA-S (Preferred Reporting Items for Systematic reviews and Meta-Analyses literature search extension) checklist, and explanation and elaboration.
            
            
              Methods
              The checklist was developed using a 3-stage Delphi survey process, followed by a consensus conference and public review process.
            
            
              Results
              The final checklist includes 16 reporting items, each of which is detailed with exemplar reporting and rationale.
            
            
              Conclusions
              The intent of PRISMA-S is to complement the PRISMA Statement and its extensions by providing a checklist that could be used by interdisciplinary authors, editors, and peer reviewers to verify that each component of a search is completely reported and therefore reproducible.},
	language = {en},
	number = {1},
	urldate = {2025-01-23},
	journal = {Systematic Reviews},
	author = {Rethlefsen, Melissa L. and Kirtley, Shona and Waffenschmidt, Siw and Ayala, Ana Patricia and Moher, David and Page, Matthew J. and Koffel, Jonathan B. and {PRISMA-S Group} and Blunt, Heather and Brigham, Tara and Chang, Steven and Clark, Justin and Conway, Aislinn and Couban, Rachel and De Kock, Shelley and Farrah, Kelly and Fehrmann, Paul and Foster, Margaret and Fowler, Susan A. and Glanville, Julie and Harris, Elizabeth and Hoffecker, Lilian and Isojarvi, Jaana and Kaunelis, David and Ket, Hans and Levay, Paul and Lyon, Jennifer and McGowan, Jessie and Murad, M. Hassan and Nicholson, Joey and Pannabecker, Virginia and Paynter, Robin and Pinotti, Rachel and Ross-White, Amanda and Sampson, Margaret and Shields, Tracy and Stevens, Adrienne and Sutton, Anthea and Weinfurter, Elizabeth and Wright, Kath and Young, Sarah},
	month = jan,
	year = {2021},
	pages = {39},
}

@article{rethlefsen_prisma-s_2021-1,
	title = {{PRISMA}-{S}: an extension to the {PRISMA} statement for reporting literature searches in systematic reviews},
	volume = {109},
	copyright = {https://creativecommons.org/licenses/by/4.0},
	issn = {1558-9439, 1536-5050},
	shorttitle = {{PRISMA}-{S}},
	url = {http://jmla.pitt.edu/ojs/jmla/article/view/962},
	doi = {10.5195/jmla.2021.962},
	abstract = {Background: Literature searches underlie the foundations of systematic reviews and related review types. Yet, the literature searching component of systematic reviews and related review types is often poorly reported. Guidance for literature search reporting has been diverse and, in many cases, does not offer enough detail to authors who need more specific information about reporting search methods and information sources in a clear, reproducible way. This document presents the PRISMA-S (Preferred Reporting Items for Systematic reviews and Meta-Analyses literature search extension) checklist, and explanation and elaboration.Methods: The checklist was developed using a three-stage Delphi survey process, followed by a consensus conference and public review process.Results: The final checklist includes sixteen reporting items, each of which is detailed with exemplar reporting and rationale.Conclusions:The intent of PRISMA-S is to complement the PRISMA Statement and its extensions by providing a checklist that could be used by interdisciplinary authors, editors, and peer reviewers to verify that each component of a search is completely reported and, therefore, reproducible.},
	number = {2},
	urldate = {2025-01-23},
	journal = {Journal of the Medical Library Association},
	author = {Rethlefsen, Melissa L. and Kirtley, Shona and Waffenschmidt, Siw and Ayala, Ana Patricia and Moher, David and Page, Matthew J. and Koffel, Jonathan B. and Group, Prisma-S},
	month = jul,
	year = {2021},
}

@article{sarkis-onofre_how_2021,
	title = {How to properly use the {PRISMA} {Statement}},
	volume = {10},
	issn = {2046-4053},
	url = {https://systematicreviewsjournal.biomedcentral.com/articles/10.1186/s13643-021-01671-z},
	doi = {10.1186/s13643-021-01671-z},
	language = {en},
	number = {1},
	urldate = {2025-01-23},
	journal = {Systematic Reviews},
	author = {Sarkis-Onofre, Rafael and Catalá-López, Ferrán and Aromataris, Edoardo and Lockwood, Craig},
	month = dec,
	year = {2021},
	pages = {117, s13643--021--01671--z},
}

@article{tricco_prisma_2018,
	title = {{PRISMA} {Extension} for {Scoping} {Reviews} ({PRISMA}-{ScR}): {Checklist} and {Explanation}},
	volume = {169},
	issn = {0003-4819, 1539-3704},
	shorttitle = {{PRISMA} {Extension} for {Scoping} {Reviews} ({PRISMA}-{ScR})},
	url = {https://www.acpjournals.org/doi/10.7326/M18-0850},
	doi = {10.7326/M18-0850},
	language = {en},
	number = {7},
	urldate = {2025-01-23},
	journal = {Annals of Internal Medicine},
	author = {Tricco, Andrea C. and Lillie, Erin and Zarin, Wasifa and O'Brien, Kelly K. and Colquhoun, Heather and Levac, Danielle and Moher, David and Peters, Micah D.J. and Horsley, Tanya and Weeks, Laura and Hempel, Susanne and Akl, Elie A. and Chang, Christine and McGowan, Jessie and Stewart, Lesley and Hartling, Lisa and Aldcroft, Adrian and Wilson, Michael G. and Garritty, Chantelle and Lewin, Simon and Godfrey, Christina M. and Macdonald, Marilyn T. and Langlois, Etienne V. and Soares-Weiser, Karla and Moriarty, Jo and Clifford, Tammy and Tunçalp, Özge and Straus, Sharon E.},
	month = oct,
	year = {2018},
	pages = {467--473},
}

@article{dalton_potential_2017,
	title = {Potential value of systematic reviews of qualitative evidence in informing user-centered health and social care: findings from a descriptive overview},
	volume = {88},
	issn = {08954356},
	shorttitle = {Potential value of systematic reviews of qualitative evidence in informing user-centered health and social care},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0895435616303031},
	doi = {10.1016/j.jclinepi.2017.04.020},
	language = {en},
	urldate = {2025-01-23},
	journal = {Journal of Clinical Epidemiology},
	author = {Dalton, Jane and Booth, Andrew and Noyes, Jane and Sowden, Amanda J.},
	month = aug,
	year = {2017},
	pages = {37--46},
}

@article{france_methodological_2014,
	title = {A methodological systematic review of what’s wrong with meta-ethnography reporting},
	volume = {14},
	issn = {1471-2288},
	url = {https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/1471-2288-14-119},
	doi = {10.1186/1471-2288-14-119},
	language = {en},
	number = {1},
	urldate = {2025-01-23},
	journal = {BMC Medical Research Methodology},
	author = {France, Emma F and Ring, Nicola and Thomas, Rebecca and Noyes, Jane and Maxwell, Margaret and Jepson, Ruth},
	month = dec,
	year = {2014},
	pages = {119},
}

@article{ofori-boateng_enhancing_2024,
	title = {Enhancing systematic reviews: {An} in-depth analysis on the impact of active learning parameter combinations for biomedical abstract screening},
	volume = {157},
	issn = {09333657},
	shorttitle = {Enhancing systematic reviews},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0933365724002318},
	doi = {10.1016/j.artmed.2024.102989},
	language = {en},
	urldate = {2025-01-23},
	journal = {Artificial Intelligence in Medicine},
	author = {Ofori-Boateng, Regina and Trujillo-Escobar, Tamy Goretty and Aceves-Martins, Magaly and Wiratunga, Nirmalie and Moreno-Garcia, Carlos Francisco},
	month = nov,
	year = {2024},
	pages = {102989},
}

@misc{de_bruin_synergy_2023,
	title = {{SYNERGY} - {Open} machine learning dataset on study selection in systematic reviews},
	url = {https://dataverse.nl/citation?persistentId=doi:10.34894/HE6NAQ},
	doi = {10.34894/HE6NAQ},
	abstract = {SYNERGY is a free and open dataset on study selection in systematic reviews, comprising 169,288 academic works from 26 systematic reviews. Only 2,834 (1.67\%) of the academic works in the binary classified dataset are included in the systematic reviews. This makes the SYNERGY dataset a unique dataset for the development of information retrieval algorithms, especially for sparse labels. Due to the many available variables available per record (i.e. titles, abstracts, authors, references, topics), this dataset is useful for researchers in NLP, machine learning, network analysis, and more. In total, the dataset contains 82,668,134 trainable data points. The easiest way to get the SYNERGY dataset is via the synergy-dataset Python package. See https://github.com/asreview/synergy-dataset for all information.},
	urldate = {2025-01-23},
	publisher = {DataverseNL},
	author = {De Bruin, Jonathan and Ma, Yongchao and Ferdinands, Gerbrich and Teijema, Jelle and Van De Schoot, Rens},
	collaborator = {De Bruin, Jonathan and Van De Schoot, Rens},
	year = {2023},
}

@misc{susnjak_automating_2024,
	title = {Automating {Research} {Synthesis} with {Domain}-{Specific} {Large} {Language} {Model} {Fine}-{Tuning}},
	url = {http://arxiv.org/abs/2404.08680},
	doi = {10.48550/arXiv.2404.08680},
	abstract = {This research pioneers the use of fine-tuned Large Language Models (LLMs) to automate Systematic Literature Reviews (SLRs), presenting a significant and novel contribution in integrating AI to enhance academic research methodologies. Our study employed the latest fine-tuning methodologies together with open-sourced LLMs, and demonstrated a practical and efficient approach to automating the final execution stages of an SLR process that involves knowledge synthesis. The results maintained high fidelity in factual accuracy in LLM responses, and were validated through the replication of an existing PRISMA-conforming SLR. Our research proposed solutions for mitigating LLM hallucination and proposed mechanisms for tracking LLM responses to their sources of information, thus demonstrating how this approach can meet the rigorous demands of scholarly research. The findings ultimately confirmed the potential of fine-tuned LLMs in streamlining various labor-intensive processes of conducting literature reviews. Given the potential of this approach and its applicability across all research domains, this foundational study also advocated for updating PRISMA reporting guidelines to incorporate AI-driven processes, ensuring methodological transparency and reliability in future SLRs. This study broadens the appeal of AI-enhanced tools across various academic and research fields, setting a new standard for conducting comprehensive and accurate literature reviews with more efficiency in the face of ever-increasing volumes of academic studies.},
	urldate = {2025-01-23},
	publisher = {arXiv},
	author = {Susnjak, Teo and Hwang, Peter and Reyes, Napoleon H. and Barczak, Andre L. C. and McIntosh, Timothy R. and Ranathunga, Surangika},
	month = apr,
	year = {2024},
	note = {arXiv:2404.08680 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Digital Libraries, Computer Science - Information Retrieval},
}

@misc{zhuang_promptreps_2024,
	title = {{PromptReps}: {Prompting} {Large} {Language} {Models} to {Generate} {Dense} and {Sparse} {Representations} for {Zero}-{Shot} {Document} {Retrieval}},
	shorttitle = {{PromptReps}},
	url = {http://arxiv.org/abs/2404.18424},
	doi = {10.48550/arXiv.2404.18424},
	abstract = {Utilizing large language models (LLMs) for zero-shot document ranking is done in one of two ways: (1) prompt-based re-ranking methods, which require no further training but are only feasible for re-ranking a handful of candidate documents due to computational costs; and (2) unsupervised contrastive trained dense retrieval methods, which can retrieve relevant documents from the entire corpus but require a large amount of paired text data for contrastive training. In this paper, we propose PromptReps, which combines the advantages of both categories: no need for training and the ability to retrieve from the whole corpus. Our method only requires prompts to guide an LLM to generate query and document representations for effective document retrieval. Specifically, we prompt the LLMs to represent a given text using a single word, and then use the last token's hidden states and the corresponding logits associated with the prediction of the next token to construct a hybrid document retrieval system. The retrieval system harnesses both dense text embedding and sparse bag-of-words representations given by the LLM. Our experimental evaluation on the MSMARCO, TREC deep learning and BEIR zero-shot document retrieval datasets illustrates that this simple prompt-based LLM retrieval method can achieve a similar or higher retrieval effectiveness than state-of-the-art LLM embedding methods that are trained with large amounts of unsupervised data, especially when using a larger LLM.},
	urldate = {2025-01-23},
	publisher = {arXiv},
	author = {Zhuang, Shengyao and Ma, Xueguang and Koopman, Bevan and Lin, Jimmy and Zuccon, Guido},
	month = oct,
	year = {2024},
	note = {arXiv:2404.18424 [cs]},
	keywords = {Computer Science - Information Retrieval},
}

@misc{margatina_importance_2022,
	title = {On the {Importance} of {Effectively} {Adapting} {Pretrained} {Language} {Models} for {Active} {Learning}},
	url = {http://arxiv.org/abs/2104.08320},
	doi = {10.48550/arXiv.2104.08320},
	abstract = {Recent Active Learning (AL) approaches in Natural Language Processing (NLP) proposed using off-the-shelf pretrained language models (LMs). In this paper, we argue that these LMs are not adapted effectively to the downstream task during AL and we explore ways to address this issue. We suggest to first adapt the pretrained LM to the target task by continuing training with all the available unlabeled data and then use it for AL. We also propose a simple yet effective fine-tuning method to ensure that the adapted LM is properly trained in both low and high resource scenarios during AL. Our experiments demonstrate that our approach provides substantial data efficiency improvements compared to the standard fine-tuning approach, suggesting that a poor training strategy can be catastrophic for AL.},
	urldate = {2025-01-23},
	publisher = {arXiv},
	author = {Margatina, Katerina and Barrault, Loïc and Aletras, Nikolaos},
	month = mar,
	year = {2022},
	note = {arXiv:2104.08320 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{margatina_importance_2022-1,
	title = {On the {Importance} of {Effectively} {Adapting} {Pretrained} {Language} {Models} for {Active} {Learning}},
	url = {http://arxiv.org/abs/2104.08320},
	doi = {10.48550/arXiv.2104.08320},
	abstract = {Recent Active Learning (AL) approaches in Natural Language Processing (NLP) proposed using off-the-shelf pretrained language models (LMs). In this paper, we argue that these LMs are not adapted effectively to the downstream task during AL and we explore ways to address this issue. We suggest to first adapt the pretrained LM to the target task by continuing training with all the available unlabeled data and then use it for AL. We also propose a simple yet effective fine-tuning method to ensure that the adapted LM is properly trained in both low and high resource scenarios during AL. Our experiments demonstrate that our approach provides substantial data efficiency improvements compared to the standard fine-tuning approach, suggesting that a poor training strategy can be catastrophic for AL.},
	urldate = {2025-01-23},
	publisher = {arXiv},
	author = {Margatina, Katerina and Barrault, Loïc and Aletras, Nikolaos},
	month = mar,
	year = {2022},
	note = {arXiv:2104.08320 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{yang_goldilocks_2022,
	title = {Goldilocks: {Just}-{Right} {Tuning} of {BERT} for {Technology}-{Assisted} {Review}},
	shorttitle = {Goldilocks},
	url = {http://arxiv.org/abs/2105.01044},
	doi = {10.48550/arXiv.2105.01044},
	abstract = {Technology-assisted review (TAR) refers to iterative active learning workflows for document review in high recall retrieval (HRR) tasks. TAR research and most commercial TAR software have applied linear models such as logistic regression to lexical features. Transformer-based models with supervised tuning are known to improve effectiveness on many text classification tasks, suggesting their use in TAR. We indeed find that the pre-trained BERT model reduces review cost by 10\% to 15\% in TAR workflows simulated on the RCV1-v2 newswire collection. In contrast, we likewise determined that linear models outperform BERT for simulated legal discovery topics on the Jeb Bush e-mail collection. This suggests the match between transformer pre-training corpora and the task domain is of greater significance than generally appreciated. Additionally, we show that just-right language model fine-tuning on the task collection before starting active learning is critical. Too little or too much fine-tuning hinders performance, worse than that of linear models, even for a favorable corpus such as RCV1-v2.},
	urldate = {2025-01-23},
	publisher = {arXiv},
	author = {Yang, Eugene and MacAvaney, Sean and Lewis, David D. and Frieder, Ophir},
	month = jan,
	year = {2022},
	note = {arXiv:2105.01044 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
}

@incollection{rehm_enriched_2024,
	address = {Cham},
	title = {Enriched {BERT} {Embeddings} for {Scholarly} {Publication} {Classification}},
	volume = {14770},
	isbn = {978-3-031-65793-1 978-3-031-65794-8},
	url = {https://link.springer.com/10.1007/978-3-031-65794-8_16},
	abstract = {Abstract
            With the rapid expansion of academic literature and the proliferation of preprints, researchers face growing challenges in manually organizing and labeling large volumes of articles. The NSLP 2024 FoRC Shared Task I addresses this challenge organized as a competition. The goal is to develop a classifier capable of predicting one of 123 predefined classes from the Open Research Knowledge Graph (ORKG) taxonomy of research fields for a given article. This paper presents our results.
            Initially, we enrich the dataset (containing English scholarly articles sourced from ORKG and arXiv), then leverage different pre-trained language Models (PLMs), specifically BERT, and explore their efficacy in transfer learning for this downstream task. Our experiments encompass feature-based and fine-tuned transfer learning approaches using diverse PLMs, optimized for scientific tasks, including SciBERT, SciNCL, and SPECTER2. We conduct hyperparameter tuning and investigate the impact of data augmentation from bibliographic databases such as OpenAlex, Semantic Scholar, and Crossref. Our results demonstrate that fine-tuning pre-trained models substantially enhances classification performance, with SPECTER2 emerging as the most accurate model. Moreover, enriching the dataset with additional metadata improves classification outcomes significantly, especially when integrating information from S2AG, OpenAlex and Crossref. Our best-performing approach achieves a weighted F1-score of 0.7415. Overall, our study contributes to the advancement of reliable automated systems for scholarly publication categorization, offering a potential solution to the laborious manual curation process, thereby facilitating researchers in efficiently locating relevant resources.},
	language = {en},
	urldate = {2025-01-23},
	booktitle = {Natural {Scientific} {Language} {Processing} and {Research} {Knowledge} {Graphs}},
	publisher = {Springer Nature Switzerland},
	author = {Wolff, Benjamin and Seidlmayer, Eva and Förstner, Konrad U.},
	editor = {Rehm, Georg and Dietze, Stefan and Schimmler, Sonja and Krüger, Frank},
	year = {2024},
	doi = {10.1007/978-3-031-65794-8_16},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {234--243},
}

@misc{ostendorff_enriching_2019,
	title = {Enriching {BERT} with {Knowledge} {Graph} {Embeddings} for {Document} {Classification}},
	url = {http://arxiv.org/abs/1909.08402},
	doi = {10.48550/arXiv.1909.08402},
	abstract = {In this paper, we focus on the classification of books using short descriptive texts (cover blurbs) and additional metadata. Building upon BERT, a deep neural language model, we demonstrate how to combine text representations with metadata and knowledge graph embeddings, which encode author information. Compared to the standard BERT approach we achieve considerably better results for the classification task. For a more coarse-grained classification using eight labels we achieve an F1- score of 87.20, while a detailed classification using 343 labels yields an F1-score of 64.70. We make the source code and trained models of our experiments publicly available},
	urldate = {2025-01-23},
	publisher = {arXiv},
	author = {Ostendorff, Malte and Bourgonje, Peter and Berger, Maria and Moreno-Schneider, Julian and Rehm, Georg and Gipp, Bela},
	month = sep,
	year = {2019},
	note = {arXiv:1909.08402 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval, Computer Science - Machine Learning},
}

@misc{ganti_building_2013,
	title = {Building {Bridges}: {Viewing} {Active} {Learning} from the {Multi}-{Armed} {Bandit} {Lens}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Building {Bridges}},
	url = {https://arxiv.org/abs/1309.6830},
	doi = {10.48550/ARXIV.1309.6830},
	abstract = {In this paper we propose a multi-armed bandit inspired, pool based active learning algorithm for the problem of binary classification. By carefully constructing an analogy between active learning and multi-armed bandits, we utilize ideas such as lower confidence bounds, and self-concordant regularization from the multi-armed bandit literature to design our proposed algorithm. Our algorithm is a sequential algorithm, which in each round assigns a sampling distribution on the pool, samples one point from this distribution, and queries the oracle for the label of this sampled point. The design of this sampling distribution is also inspired by the analogy between active learning and multi-armed bandits. We show how to derive lower confidence bounds required by our algorithm. Experimental comparisons to previously proposed active learning algorithms show superior performance on some standard UCI datasets.},
	urldate = {2025-01-23},
	publisher = {arXiv},
	author = {Ganti, Ravi and Gray, Alexander G.},
	year = {2013},
	note = {Version Number: 1},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Machine Learning (stat.ML)},
}

@misc{hino_active_2022,
	title = {Active {Learning} by {Query} by {Committee} with {Robust} {Divergences}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2211.10013},
	doi = {10.48550/ARXIV.2211.10013},
	abstract = {Active learning is a widely used methodology for various problems with high measurement costs. In active learning, the next object to be measured is selected by an acquisition function, and measurements are performed sequentially. The query by committee is a well-known acquisition function. In conventional methods, committee disagreement is quantified by the Kullback--Leibler divergence. In this paper, the measure of disagreement is defined by the Bregman divergence, which includes the Kullback--Leibler divergence as an instance, and the dual \$γ\$-power divergence. As a particular class of the Bregman divergence, the \$β\$-divergence is considered. By deriving the influence function, we show that the proposed method using \$β\$-divergence and dual \$γ\$-power divergence are more robust than the conventional method in which the measure of disagreement is defined by the Kullback--Leibler divergence. Experimental results show that the proposed method performs as well as or better than the conventional method.},
	urldate = {2025-01-23},
	publisher = {arXiv},
	author = {Hino, Hideitsu and Eguchi, Shinto},
	year = {2022},
	note = {Version Number: 1},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Machine Learning (stat.ML)},
}

@incollection{goharian_reproducibility_2024,
	address = {Cham},
	title = {A {Reproducibility} {Study} of {Goldilocks}: {Just}-{Right} {Tuning} of {BERT} for {TAR}},
	volume = {14611},
	isbn = {978-3-031-56065-1 978-3-031-56066-8},
	shorttitle = {A {Reproducibility} {Study} of {Goldilocks}},
	url = {https://link.springer.com/10.1007/978-3-031-56066-8_13},
	language = {en},
	urldate = {2025-01-23},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer Nature Switzerland},
	author = {Mao, Xinyu and Koopman, Bevan and Zuccon, Guido},
	editor = {Goharian, Nazli and Tonellotto, Nicola and He, Yulan and Lipani, Aldo and McDonald, Graham and Macdonald, Craig and Ounis, Iadh},
	year = {2024},
	doi = {10.1007/978-3-031-56066-8_13},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {132--146},
}

@misc{akinseloyin_novel_2023,
	title = {A {Novel} {Question}-{Answering} {Framework} for {Automated} {Abstract} {Screening} {Using} {Large} {Language} {Models}},
	url = {http://medrxiv.org/lookup/doi/10.1101/2023.12.17.23300102},
	doi = {10.1101/2023.12.17.23300102},
	abstract = {Abstract
          
            Objective
            This paper aims to address the challenges in abstract screening within Systematic Reviews (SR) by leveraging the zero-shot capabilities of large language models (LLMs).
          
          
            Methods
            We employ LLM to prioritise candidate studies by aligning abstracts with the selection criteria outlined in an SR protocol. Abstract screening was transformed into a novel question-answering (QA) framework, treating each selection criterion as a question addressed by LLM. The framework involves breaking down the selection criteria into multiple questions, properly prompting LLM to answer each question, scoring and re-ranking each answer, and combining the responses to make nuanced inclusion or exclusion decisions.
          
          
            Results
            Large-scale validation was performed on the benchmark of CLEF eHealth 2019 Task 2: Technology- Assisted Reviews in Empirical Medicine. Focusing on GPT-3.5 as a case study, the proposed QA framework consistently exhibited a clear advantage over traditional information retrieval approaches and bespoke BERT- family models that were fine-tuned for prioritising candidate studies (i.e., from the BERT to PubMedBERT) across 31 datasets of four categories of SRs, underscoring their high potential in facilitating abstract screening.
          
          
            Conclusion
            Investigation justified the indispensable value of leveraging selection criteria to improve the performance of automated abstract screening. LLMs demonstrated proficiency in prioritising candidate studies for abstract screening using the proposed QA framework. Significant performance improvements were obtained by re-ranking answers using the semantic alignment between abstracts and selection criteria. This further highlighted the pertinence of utilizing selection criteria to enhance abstract screening.},
	language = {en},
	urldate = {2025-01-23},
	publisher = {Health Informatics},
	author = {Akinseloyin, Opeoluwa and Jiang, Xiaorui and Palade, Vasile},
	month = dec,
	year = {2023},
}

@inproceedings{wang_new_2014,
	title = {A new active labeling method for deep learning},
	doi = {10.1109/IJCNN.2014.6889457},
	booktitle = {2014 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Wang, Dan and Shang, Yi},
	year = {2014},
	keywords = {Classification algorithms, Entropy, Labeling, Measurement, Neural networks, Training, Uncertainty},
	pages = {112--119},
}

@article{caton_fairness_2024,
	title = {Fairness in machine learning: {A} survey},
	volume = {56},
	doi = {10.1145/3616865},
	number = {7},
	journal = {ACM Comput. Surv.},
	author = {Caton, S. and Haas, C.},
	year = {2024},
}

@article{bai_constitutional_2022,
	title = {Constitutional {AI}: {Harmlessness} from {AI} {Feedback}},
	url = {https://arxiv.org/abs/2212.08073},
	author = {Bai, Y. and Kadavath, S. and Kundu, S. and Askell, A. and Kernion, J. and Jones, A. and Chen, A. and Goldie, A. and Mirhoseini, A. and McKinnon, C. and Chen, C. and Olsson, C. and Olah, C. and Hernandez, D. and Drain, D. and Ganguli, D. and Li, D. and Tran-Johnson, E. and Perez, E. and Kerr, J. and Mueller, J. and Ladish, J. and Landau, J. and Ndousse, K. and Lukosuite, K. and Lovitt, L. and Sellitto, M. and Elhage, N. and Schiefer, N. and Mercado, N. and DasSarma, N. and Lasenby, R. and Larson, R. and Ringer, S. and Johnston, S. and Kravec, S. and Showk, S. E. and Fort, S. and Lanham, T. and Telleen-Lawton, T. and Conerly, T. and Henighan, T. and Hume, T. and Bowman, S. R. and Hatfield-Dodds, Z. and Mann, B. and Amodei, D. and Joseph, N. and McCandlish, S. and Brown, T. and Kaplan, J.},
	year = {2022},
}

@misc{noauthor_introducing_2024,
	title = {Introducing {Claude} 3.5 {Sonnet}},
	url = {https://www.anthropic.com/news/claude-3-5-sonnet},
	year = {2024},
}

@misc{noauthor_openai_2024,
	title = {{OpenAI} {Platform}},
	url = {https://platform.openai.com},
	year = {2024},
}

@article{robertson_probabilistic_2009,
	title = {The probabilistic relevance framework: {Bm25} and beyond},
	volume = {3},
	doi = {10.1561/1500000019},
	number = {4},
	journal = {Found. Trends Inf. Retr.},
	author = {Robertson, S. and Zaragoza, H.},
	year = {2009},
	pages = {333--389},
}

@misc{noauthor_anonymised_2025,
	title = {Anonymised {Repository} - {Anonymous} {GitHub}},
	url = {https://anonymous.4open.science/r/RetractionWatch/README.md},
	year = {2025},
}

@article{mehrabi_survey_2021,
	title = {A survey on bias and fairness in machine learning},
	volume = {54},
	doi = {10.1145/3457607},
	number = {6},
	journal = {ACM Comput. Surv.},
	author = {Mehrabi, N. and Morstatter, F. and Saxena, N. and Lerman, K. and Galstyan, A.},
	year = {2021},
}

@article{li_survey_2022,
	title = {A survey on text classification: {From} traditional to deep learning},
	volume = {13},
	doi = {10.1145/3495162},
	number = {2},
	journal = {ACM Trans. Intell. Syst. Technol.},
	author = {Li, Q. and Peng, H. and Li, J. and Xia, C. and Yang, R. and Sun, L. and Yu, P. S. and He, L.},
	year = {2022},
}

@article{sebastiani_machine_2002,
	title = {Machine learning in automated text categorization},
	volume = {34},
	doi = {10.1145/505282.505282},
	number = {1},
	journal = {ACM Comput. Surv.},
	author = {Sebastiani, F.},
	year = {2002},
	pages = {1--47},
}

@article{warne_rewarding_2016,
	title = {Rewarding reviewers – sense or sensibility? {A} {Wiley} study explained},
	volume = {29},
	doi = {10.1002/leap.1002},
	number = {1},
	journal = {Learned Publishing},
	author = {Warne, V.},
	year = {2016},
	pages = {41--50},
}

@article{van_noorden_more_2023,
	title = {More than 10,000 research papers were retracted in 2023 — a new record},
	volume = {624},
	doi = {10.1038/d41586-023-03974-8},
	number = {7992},
	journal = {Nature},
	author = {Van Noorden, R.},
	year = {2023},
	pages = {479--481},
}

@article{heibi_qualitative_2021,
	title = {A qualitative and quantitative analysis of open citations to retracted articles: the wakefield 1998 et al.’s case},
	volume = {126},
	doi = {10.1007/s11192-021-04097-5},
	number = {10},
	journal = {Scientometrics},
	author = {Heibi, I. and Peroni, S.},
	year = {2021},
	pages = {8433--8470},
}

@article{hsiao_continued_2021,
	title = {Continued use of retracted papers: {Temporal} trends in citations and (lack of) awareness of retractions shown in citation contexts in biomedicine},
	volume = {2},
	doi = {10.1162/qss_a_00155},
	number = {4},
	journal = {Quantitative Science Studies},
	author = {Hsiao, T.-K. and Schneider, J.},
	year = {2021},
	pages = {1144--1169},
}

@article{schneider_continued_2020,
	title = {Continued post-retraction citation of a fraudulent clinical trial report, 11 years after it was retracted for falsifying data},
	volume = {125},
	doi = {10.1007/s11192-020-03631-1},
	number = {3},
	journal = {Scientometrics},
	author = {Schneider, J. and Ye, D. and Hill, A. M. and Whitehorn, A. S.},
	year = {2020},
	pages = {2877--2913},
}

@article{steer_peer_2021,
	title = {Peer review - {Why}, when and how},
	volume = {2},
	doi = {10.1016/j.ijcchd.2021.100083},
	journal = {International Journal of Cardiology Congenital Heart Disease},
	author = {Steer, P. J. and Ernst, S.},
	year = {2021},
	pages = {100083},
}

@article{banks_thoughts_2018,
	title = {Thoughts on {Publishing} the {Research} {Article} over the {Centuries}},
	volume = {6},
	doi = {10.3390/publications6010010},
	number = {1},
	journal = {Publications},
	author = {Banks, D.},
	year = {2018},
	pages = {10},
}

@article{priem_openalex_2022,
	title = {{OpenAlex}: {A} fully-open index of scholarly works, authors, venues, institutions, and concepts},
	doi = {10.48550/arXiv.2205.01833},
	journal = {arXiv},
	author = {Priem, J. and Piwowar, H. and Orr, R.},
	year = {2022},
}

@misc{noauthor_retraction_2024,
	title = {Retraction {Watch} {Database} {User} {Guide}},
	url = {https://retractionwatch.com/wp-content/uploads/2023/12/Building-The-Database.pdf},
	year = {2024},
}

@misc{retraction_watch_center_2018,
	title = {The {Center} for {Scientific} {Integrity}},
	url = {https://retractionwatch.com/},
	author = {{Retraction Watch}},
	year = {2018},
	note = {Place: New York},
}

@article{vet_propagation_2016,
	title = {Propagation of errors in citation networks: a study involving the entire citation network of a widely cited paper published in, and later retracted from, the journal nature},
	volume = {1},
	doi = {10.1186/s41073-016-0008-5},
	number = {1},
	journal = {Research Integrity and Peer Review},
	author = {Vet, P. E. and Nijveen, H.},
	year = {2016},
	pages = {3},
}

@article{stretton_publication_2012,
	title = {Publication misconduct and plagiarism retractions: a systematic, retrospective study},
	volume = {28},
	doi = {10.1185/03007995.2012.728131},
	number = {10},
	journal = {Current Medical Research and Opinion},
	author = {Stretton, S. and Bramich, N. J. and Keys, J. R. and Monk, J. A. and Ely, J. A. and Haley, C. and Woolley, M. J. and Woolley, K. L.},
	year = {2012},
	pages = {1575--1583},
}

@article{byrne_digital_2020,
	title = {Digital magic, or the dark arts of the 21st century—how can journals and peer reviewers detect manuscripts and publications from paper mills?},
	volume = {594},
	doi = {10.1002/1873-3468.13747},
	number = {4},
	journal = {FEBS Letters},
	author = {Byrne, J. A. and Christopher, J.},
	year = {2020},
	pages = {583--589},
}

@article{gaudino_trends_2021,
	title = {Trends and {Characteristics} of {Retracted} {Articles} in the {Biomedical} {Literature}, 1971 to 2020},
	volume = {181},
	doi = {10.1001/jamainternmed.2021.1807},
	number = {8},
	journal = {JAMA Internal Medicine},
	author = {Gaudino, M. and Robinson, N. B. and Audisio, K. and Rahouma, M. and Benedetto, U. and Kurlansky, P. and Fremes, S. E.},
	year = {2021},
	pages = {1118--1121},
}

@article{candal-pedreira_retracted_2022,
	title = {Retracted papers originating from paper mills: cross sectional study},
	volume = {379},
	doi = {10.1136/bmj-2022-071517},
	journal = {BMJ},
	author = {Candal-Pedreira, C. and Ross, J. S. and Ruano-Ravina, A. and Egilman, D. S. and Fernández, E. and Pérez-Ríos, M.},
	year = {2022},
	pages = {071517},
}

@article{liu_journal_2018,
	title = {Journal retractions: {Some} unique features of research misconduct in china},
	volume = {49},
	doi = {10.3138/jsp.49.3.02},
	number = {3},
	journal = {Journal of Scholarly Publishing},
	author = {Liu, X. and Chen, X.},
	year = {2018},
	pages = {305--319},
}

@article{hvistendahl_chinas_2013,
	title = {China’s publication bazaar},
	volume = {342},
	doi = {10.1126/science.342.6162.1035},
	number = {6162},
	journal = {Science},
	author = {Hvistendahl, M.},
	year = {2013},
	pages = {1035--1039},
}

@book{hastie_elements_2009,
	edition = {2nd},
	series = {Springer {Series} in {Statistics}},
	title = {The {Elements} of {Statistical} {Learning}},
	publisher = {Springer},
	author = {Hastie, T. and Tibshirani, R. and Friedman, J.},
	year = {2009},
	doi = {10.1007/978-0-387-84858-7},
}

@article{moylan_why_2016,
	title = {Why articles are retracted: a retrospective crosssectional study of retraction notices at biomed central},
	volume = {6},
	doi = {10.1136/bmjopen-2016-012047},
	number = {11},
	journal = {BMJ Open},
	author = {Moylan, E. C. and Kowalczuk, M. K.},
	year = {2016},
}

@article{avenell_investigation_2019,
	title = {An investigation into the impact and implications of published papers from retracted research: systematic search of affected literature},
	volume = {9},
	doi = {10.1136/bmjopen-2019-031909},
	number = {10},
	journal = {BMJ Open},
	author = {Avenell, A. and Stewart, F. and Grey, A. and Gamble, G. and Bolland, M.},
	year = {2019},
}

@article{tian_perish_2016,
	title = {Perish or publish in china: {Pressures} on young chinese scholars to publish in internationally indexed journals},
	volume = {4},
	doi = {10.3390/publications4020009},
	number = {2},
	journal = {Publications},
	author = {Tian, M. and Su, Y. and Ru, X.},
	year = {2016},
}

@article{minaee_deep_2021,
	title = {Deep {Learning}–based {Text} {Classification}: {A} {Comprehensive} {Review}},
	volume = {54},
	doi = {10.1145/3439726},
	number = {3},
	journal = {ACM Comput. Surv.},
	author = {Minaee, S. and Kalchbrenner, N. and Cambria, E. and Nikzad, N. and Chenaghlu, M. and Gao, J.},
	year = {2021},
	pages = {62--16240},
}

@article{kuhberger_self-correction_2022,
	title = {Self-correction in science: {The} effect of retraction on the frequency of citations},
	volume = {17},
	doi = {10.1371/journal.pone.0277814},
	number = {12},
	journal = {PloS One},
	author = {Kühberger, A. and Streit, D. and Scherndl, T.},
	year = {2022},
	pages = {0277814},
}

@article{steen_retractions_2011,
	title = {Retractions in the scientific literature: do authors deliberately commit research fraud?},
	volume = {37},
	doi = {10.1136/jme.2010.038125},
	number = {2},
	journal = {Journal of Medical Ethics},
	author = {Steen, R. G.},
	year = {2011},
	pages = {113--117},
}

@article{steen_retractions_2011-1,
	title = {Retractions in the scientific literature: is the incidence of research fraud increasing?},
	volume = {37},
	doi = {10.1136/jme.2010.040923},
	number = {4},
	journal = {Journal of Medical Ethics},
	author = {Steen, R. G.},
	year = {2011},
	pages = {249--253},
}

@article{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	doi = {10.48550/arXiv.1810.04805},
	journal = {arXiv},
	author = {Devlin, J. and Chang, M.-W. and Lee, K. and Toutanova, K.},
	year = {2019},
}

@book{jurafsky_speech_2025,
	edition = {3rd},
	title = {Speech and {Language} {Processing}: {An} {Introduction} to {Natural} {Language} {Processing}, {Computational} {Linguistics}, and {Speech} {Recognition} with {Language} {Models}},
	url = {https://web.stanford.edu/ jurafsky/slp3/},
	author = {Jurafsky, D. and Martin, J. H.},
	month = jan,
	year = {2025},
}

@misc{arxivorg_e-print_archive_arxivorg_2025,
	title = {{arXiv}.org e-{Print} {Archive}},
	url = {https://arxiv.org/},
	author = {{arXiv.org e-Print Archive}},
	year = {2025},
}

@article{perera_recent_2017,
	title = {Recent {Advances} in {Natural} {Language} {Generation}: {A} {Survey} and {Classification} of the {Empirical} {Literature}},
	volume = {36},
	doi = {10.4149/cai_2017_1_1},
	number = {1},
	journal = {Computing and Informatics},
	author = {Perera, R. and Nand, P.},
	year = {2017},
	pages = {1--32},
}

@article{teixeira_da_silva_silent_2016,
	title = {Silent or stealth retractions, the dangerous voices of the unknown, deleted literature},
	volume = {32},
	doi = {10.1007/s12109-015-9439-y},
	number = {1},
	journal = {Publishing Research Quarterly},
	author = {Teixeira Da Silva, J. A.},
	year = {2016},
	pages = {44--53},
}

@article{steen_why_2013,
	title = {Why has the number of scientific retractions increased?},
	volume = {8},
	doi = {10.1371/journal.pone.0068397},
	number = {7},
	journal = {PLoS ONE},
	author = {Steen, R. G. and Casadevall, A. and Fang, F. C.},
	year = {2013},
	pages = {68397},
}

@article{wu_online_2014,
	title = {Online search stopping behaviors: {An} investigation of query abandonment and task stopping},
	volume = {51},
	doi = {10.1002/meet.2014.14505101030},
	number = {1},
	journal = {Proceedings of the American Society for Information Science and Technology},
	author = {Wu, Wan-Ching and Kelly, Diane},
	year = {2014},
	pages = {1--10},
}

@book{wackerly_mathematical_2002,
	edition = {sixth},
	title = {Mathematical {Statistics} with {Applications}},
	publisher = {Duxbury Advanced Series},
	author = {Wackerly, Dennis D. and Mendenhall III, William and Scheaffer, Richard L.},
	year = {2002},
}

@article{stevenson_stopping_2023,
	title = {Stopping {Methods} for {Technology}-assisted {Reviews} {Based} on {Point} {Processes}},
	volume = {42},
	doi = {10.1145/3631990},
	number = {3},
	journal = {ACM Trans. Inf. Syst.},
	author = {Stevenson, Mark and Bin-Hezam, Reem},
	month = dec,
	year = {2023},
	pages = {73},
}

@book{saracevic_notion_2022,
	title = {The {Notion} of {Relevance} in {Information} {Science}: {Everybody} knows what relevance is. {But}, what is it really?},
	publisher = {Springer Nature},
	author = {Saracevic, Tefko},
	year = {2022},
}

@inproceedings{roegiest_trec_2015,
	address = {Gaithersburg, MD, USA},
	title = {{TREC} 2015 {Total} {Recall} {Track} {Overview}.},
	booktitle = {{TREC}},
	publisher = {National Institute of Standards and Technology (NIST)},
	author = {Roegiest, Adam and Cormack, Gordon V and Clarke, Charles LA and Grossman, Maura R},
	year = {2015},
}

@article{zach_when_2005,
	title = {When is “enough” enough? {Modeling} the information-seeking and stopping behavior of senior arts administrators},
	volume = {56},
	doi = {10.1002/asi.20092},
	number = {1},
	journal = {Journal of the American Society for Information Science and Technology},
	author = {Zach, Lisl},
	year = {2005},
	pages = {23--35},
}

@inproceedings{yang_minimizing_2021,
	address = {Limerick, Ireland},
	series = {{DocEng} '21},
	title = {On minimizing cost in legal document review workflows},
	doi = {10.1145/3469096.3469872},
	booktitle = {Proceedings of the 21st {ACM} {Symposium} on {Document} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Yang, Eugene and Lewis, David D. and Frieder, Ophir},
	year = {2021},
	pages = {30},
}

@article{shemilt_pinpointing_2014,
	title = {Pinpointing needles in giant haystacks: use of text mining to reduce impractical screening workload in extremely large scoping reviews},
	volume = {5},
	doi = {10.1002/jrsm.1093},
	number = {1},
	journal = {Research Synthesis Methods},
	author = {Shemilt, Ian and Simon, Antonia and Hollands, Gareth J. and Marteau, Theresa M. and Ogilvie, David and O'Mara-Eves, Alison and Kelly, Michael P. and Thomas, James},
	year = {2014},
	pages = {31--49},
}

@article{saracevic_study_1988,
	title = {A study of information seeking and retrieving. {I}. {Background} and methodology},
	volume = {39},
	doi = {10.1002/(SICI)1097-4571(198805)39:3<161::AID-ASI2>3.0.CO;2-0},
	number = {3},
	journal = {Journal of the American Society for Information Science},
	author = {Saracevic, Tefko and Kantor, Paul and Chamis, Alice Y. and Trivison, Donna},
	year = {1988},
	pages = {161--176},
}

@article{reitsma_bivariate_2005,
	title = {Bivariate analysis of sensitivity and specificity produces informative summary measures in diagnostic reviews},
	volume = {58},
	doi = {10.1016/j.jclinepi.2005.02.022},
	number = {10},
	journal = {Journal of Clinical Epidemiology},
	author = {Reitsma, Johannes B. and Glas, Afina S. and Rutjes, Anne W.S. and Scholten, Rob J.P.M. and Bossuyt, Patrick M. and Zwinderman, Aeilko H.},
	year = {2005},
	pages = {982--990},
}

@article{prabha_what_2007,
	title = {What is {Enough}? {Satisficing} {Information} {Needs}},
	volume = {63},
	doi = {10.1108/00220410710723894},
	number = {01},
	journal = {Journal of Documentation},
	author = {Prabha, Chandra and Connaway, Lynn and Olszewski, Lawrence and Jenkins, Lillie},
	month = jan,
	year = {2007},
	pages = {74--89},
}

@article{pennington_how_2016,
	title = {How much is enough? {An} investigation of nonprofessional investors information search and stopping rule use},
	volume = {21},
	doi = {10.1016/j.accinf.2016.04.003},
	journal = {International Journal of Accounting Information Systems},
	author = {Pennington, Robin R. and Kelton, Andrea Seaton},
	year = {2016},
	pages = {47--62},
}

@article{oard_jointly_2018,
	title = {Jointly {Minimizing} the {Expected} {Costs} of {Review} for {Responsiveness} and {Privilege} in {E}-{Discovery}},
	volume = {37},
	doi = {10.1145/3268928},
	number = {1},
	journal = {ACM Trans. Inf. Syst.},
	author = {Oard, Douglas W. and Sebastiani, Fabrizio and Vinjumur, Jyothi K.},
	month = nov,
	year = {2018},
	pages = {11},
}

@article{norman_measuring_2019,
	title = {Measuring the impact of screening automation on meta-analyses of diagnostic test accuracy},
	volume = {8},
	journal = {Systematic reviews},
	author = {Norman, Christopher R and Leeflang, Mariska MG and Porcher, Raphaël and Neveol, Aurelie},
	year = {2019},
	pages = {1--18},
}

@inproceedings{norman_data_2018,
	title = {Data {Extraction} and {Synthesis} in {Systematic} {Reviews} of {Diagnostic} {Test} {Accuracy}: {A} {Corpus} for {Automating} and {Evaluating} the {Process}},
	booktitle = {{AMIA} ... {Annual} {Symposium} proceedings. {AMIA} {Symposium}},
	publisher = {AMIA},
	author = {Norman, Christopher and Leeflang, Mariska and Névéol, Aurélie},
	year = {2018},
	pages = {817--826},
}

@phdthesis{nickles_judgment-based_1995,
	type = {{PhD} {Thesis}},
	title = {Judgment-based and reasoning-based stopping rules in decision-making under uncertainty},
	school = {University of Minnesota},
	author = {Nickles, Kathryn Ritgerod},
	year = {1995},
}

@article{mcdonald_how_2020,
	title = {How the {Accuracy} and {Confidence} of {Sensitivity} {Classification} {Affects} {Digital} {Sensitivity} {Review}},
	volume = {39},
	doi = {10.1145/3417334},
	number = {1},
	journal = {ACM Trans. Inf. Syst.},
	author = {Mcdonald, Graham and Macdonald, Craig and Ounis, Iadh},
	month = oct,
	year = {2020},
	pages = {4},
}

@inproceedings{maxwell_searching_2015,
	address = {Melbourne, Australia},
	series = {{CIKM} '15},
	title = {Searching and {Stopping}: {An} {Analysis} of {Stopping} {Rules} and {Strategies}},
	doi = {10.1145/2806416.2806476},
	booktitle = {Proceedings of the 24th {ACM} {International} on {Conference} on {Information} and {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Maxwell, David and Azzopardi, Leif and Järvelin, Kalervo and Keskustalo, Heikki},
	year = {2015},
	pages = {313--322},
}

@inproceedings{maxwell_initial_2015,
	address = {Santiago, Chile},
	series = {{SIGIR} '15},
	title = {An {Initial} {Investigation} into {Fixed} and {Adaptive} {Stopping} {Strategies}},
	doi = {10.1145/2766462.2767802},
	booktitle = {Proceedings of the 38th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Maxwell, David and Azzopardi, Leif and Järvelin, Kalervo and Keskustalo, Heikki},
	year = {2015},
	pages = {903--906},
}

@article{li_when_2020,
	title = {When to {Stop} {Reviewing} in {Technology}-{Assisted} {Reviews}: {Sampling} from an {Adaptive} {Distribution} to {Estimate} {Residual} {Relevant} {Documents}},
	volume = {38},
	doi = {10.1145/3411755},
	number = {4},
	journal = {ACM Trans. Inf. Syst.},
	author = {Li, Dan and Kanoulas, Evangelos},
	month = sep,
	year = {2020},
	pages = {41},
}

@inproceedings{lewis_certifying_2021,
	address = {Virtual Event, Queensland, Australia},
	series = {{CIKM} '21},
	title = {Certifying {One}-{Phase} {Technology}-{Assisted} {Reviews}},
	doi = {10.1145/3459637.3482415},
	booktitle = {Proceedings of the 30th {ACM} {International} {Conference} on {Information} \& {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Lewis, David D. and Yang, Eugene and Frieder, Ophir},
	year = {2021},
	pages = {893--902},
}

@article{kraft_stopping_1979,
	title = {Stopping rules and their effect on expected search length},
	volume = {15},
	doi = {10.1016/0306-4573(79)90007-4},
	number = {1},
	journal = {Information Processing and Management},
	author = {Kraft, Donald H and Lee, T},
	year = {1979},
	pages = {47--58},
}

@inproceedings{kanoulas_clef_2018,
	title = {{CLEF} 2018 technologically assisted reviews in empirical medicine overview},
	volume = {2125},
	url = {https://strathprints.strath.ac.uk/66446/},
	booktitle = {{CEUR} {Workshop} {Proceedings}},
	author = {Kanoulas, Evangelos and Li, Dan and Azzopardi, Leif and Spijker, Rene},
	year = {2018},
}

@inproceedings{kusa_outcome-based_2023,
	address = {Taipei, Taiwan},
	series = {{ICTIR} '23},
	title = {Outcome-based {Evaluation} of {Systematic} {Review} {Automation}},
	doi = {10.1145/3578337.3605135},
	booktitle = {Proceedings of the 2023 {ACM} {SIGIR} {International} {Conference} on {Theory} of {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Kusa, Wojciech and Zuccon, Guido and Knoth, Petr and Hanbury, Allan},
	year = {2023},
	pages = {125--133},
}

@inproceedings{kanoulas_clef_2019,
	title = {{CLEF} 2019 technology assisted reviews in empirical medicine overview},
	volume = {2380},
	url = {https://strathprints.strath.ac.uk/71253/},
	booktitle = {{CEUR} {Workshop} {Proceedings}},
	author = {Kanoulas, Evangelos and Li, Dan and Azzopardi, Leif and Spijker, Rene},
	year = {2019},
}

@inproceedings{kanoulas_clef_2017,
	title = {{CLEF} 2017 technologically assisted reviews in empirical medicine overview},
	volume = {1866},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034732447&partnerID=40&md5=a183b346edceb1918338abf473a69dcd},
	booktitle = {{CEUR} {Workshop} {Proceedings}},
	author = {Kanoulas, Evangelos and Li, Dan and Azzopardi, Leif and Spijker, Rene},
	year = {2017},
	pages = {1--29},
}

@article{ilani_analysis_2024,
	title = {Analysis of the factors affecting information search stopping behavior: {A} systematic review},
	volume = {56},
	doi = {10.1177/09610006231157091},
	number = {3},
	journal = {Journal of Librarianship and Information Science},
	author = {Ilani, Fereshte and Nowkarizi, Mohsen and Arastoopoor, Sholeh},
	year = {2024},
	pages = {796--808},
}

@inproceedings{hollmann_ranking_2017,
	title = {Ranking and {Feedback}-based {Stopping} for {Recall}-{Centric} {Document} {Retrieval}},
	booktitle = {{CLEF} (working notes)},
	author = {Hollmann, Noah and Eickhoff, Carsten},
	year = {2017},
	pages = {7--8},
}

@article{fawcett_introduction_2006,
	title = {An introduction to {ROC} analysis},
	volume = {27},
	doi = {10.1016/j.patrec.2005.10.010},
	number = {8},
	journal = {Pattern Recognition Letters},
	author = {Fawcett, Tom},
	year = {2006},
	pages = {861--874},
}

@article{dinnes_rapid_2022,
	title = {Rapid, point-of-care antigen tests for diagnosis of {SARS}-{CoV}-2 infection},
	volume = {2022},
	doi = {10.1002/14651858.CD013705.pub3},
	number = {7},
	journal = {Cochrane Database of Systematic Reviews},
	author = {Dinnes, Jacqueline and Sharma, Pawana and Berhane, Sarah and Van Wyk, Susanna S and Nyaaba, Nicholas and Domen, Julie and Taylor, Melissa and Cunningham, Jane and Davenport, Clare and Dittrich, Sabine and Emperador, Devy and Hooft, Lotty and Leeflang, Mariska Mg and McInnes, Matthew Df and Spijker, René and Verbakel, Jan Y and Takwoingi, Yemisi and Taylor-Phillips, Sian and Van Den Bruel, Ann and Deeks, Jonathan J and {Cochrane COVID-19 Diagnostic Test Accuracy Group}},
	year = {2022},
}

@article{decarlo_meaning_1997,
	title = {On the meaning and use of kurtosis},
	volume = {2},
	doi = {10.1037/1082-989X.2.3.292},
	number = {3},
	journal = {Psychological Methods},
	author = {DeCarlo, Lawrence T.},
	year = {1997},
	pages = {292--307},
}

@inproceedings{cormack_overview_2010,
	address = {Gaithersburg, MD, USA},
	series = {{NIST} {Special} {Publication}},
	title = {Overview of the {TREC} 2010 {Legal} {Track}},
	volume = {500-294},
	booktitle = {Proceedings of {The} {Nineteenth} {Text} {REtrieval} {Conference}, {TREC} 2010, {Gaithersburg}, {Maryland}, {USA}, {November} 16-19, 2010},
	publisher = {National Institute of Standards and Technology (NIST)},
	author = {Cormack, Gordon V. and Grossman, Maura R. and Hedin, Bruce and Oard, Douglas W.},
	year = {2010},
	pages = {1--9},
}

@inproceedings{cormack_scalability_2016,
	address = {Indianapolis, Indiana, USA},
	series = {{CIKM} '16},
	title = {Scalability of {Continuous} {Active} {Learning} for {Reliable} {High}-{Recall} {Text} {Classification}},
	doi = {10.1145/2983323.2983776},
	booktitle = {Proceedings of the 25th {ACM} {International} on {Conference} on {Information} and {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Cormack, Gordon V. and Grossman, Maura R.},
	year = {2016},
	pages = {1039--1048},
}

@book{higgins_cochrane_2019,
	address = {Chichester, UK},
	title = {Cochrane handbook for systematic reviews of interventions},
	publisher = {John Wiley \& Sons},
	author = {Higgins, Julian PT and Thomas, James and Chandler, Jacqueline and Cumpston, Miranda and Li, Tianjing and Page, Matthew J and Welch, Vivian A},
	year = {2019},
}

@inproceedings{grossman_trec_2016,
	address = {Gaithersburg, MD, USA},
	title = {{TREC} 2016 {Total} {Recall} {Track} {Overview}.},
	booktitle = {{TREC}},
	publisher = {National Institute of Standards and Technology (NIST)},
	author = {Grossman, Maura R and Cormack, Gordon V and Roegiest, Adam},
	year = {2016},
}

@inproceedings{dostert_users_2009,
	address = {Boston, MA, USA},
	series = {{SIGIR} '09},
	title = {Users' stopping behaviors and estimates of recall},
	doi = {10.1145/1571941.1572145},
	booktitle = {Proceedings of the 32nd {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Dostert, Maureen and Kelly, Diane},
	year = {2009},
	pages = {820--821},
}

@inproceedings{di_nunzio_study_2018,
	address = {Cham},
	title = {A {Study} of an {Automatic} {Stopping} {Strategy} for {Technologically} {Assisted} {Medical} {Reviews}},
	doi = {10.1007/978-3-319-76941-7_61},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer International Publishing},
	author = {Di Nunzio, Giorgio Maria},
	editor = {Pasi, Gabriella and Piwowarski, Benjamin and Azzopardi, Leif and Hanbury, Allan},
	year = {2018},
	pages = {672--677},
}

@inproceedings{cormack_engineering_2016,
	address = {Pisa, Italy},
	series = {{SIGIR} '16},
	title = {Engineering {Quality} and {Reliability} in {Technology}-{Assisted} {Review}},
	doi = {10.1145/2911451.2911510},
	booktitle = {Proceedings of the 39th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Cormack, Gordon V. and Grossman, Maura R.},
	year = {2016},
	pages = {75--84},
}

@article{cormack_autonomy_2015,
	title = {Autonomy and {Reliability} of {Continuous} {Active} {Learning} for {Technology}-{Assisted} {Review}},
	journal = {arXiv preprint arXiv:1504.06868},
	author = {Cormack, Gordon and Grossman, Maura},
	month = apr,
	year = {2015},
}

@article{cooper_paradoxical_1976,
	title = {The paradoxical role of unexamined documents in the evaluation of retrieval effectiveness},
	volume = {12},
	doi = {10.1016/0306-4573(76)90034-0},
	number = {6},
	journal = {Information Processing and Management},
	author = {Cooper, William S.},
	year = {1976},
	pages = {367--375},
}

@article{cooper_selecting_1973,
	title = {On selecting a measure of retrieval effectiveness part {II}. {Implementation} of the philosophy},
	volume = {24},
	doi = {10.1002/asi.4630240603},
	number = {6},
	journal = {Journal of the American Society for Information Science},
	author = {Cooper, William S.},
	year = {1973},
	pages = {413--424},
}

@article{callaghan_statistical_2020,
	title = {Statistical stopping criteria for automated screening in systematic reviews},
	volume = {9},
	doi = {10.1186/s13643-020-01521-4},
	number = {1},
	journal = {Syst. Rev.},
	author = {Callaghan, Max W and Müller-Hansen, Finn},
	month = nov,
	year = {2020},
	pages = {273},
}

@inproceedings{azzopardi_how_2013,
	address = {Dublin, Ireland},
	series = {{SIGIR} '13},
	title = {How query cost affects search behavior},
	doi = {10.1145/2484028.2484049},
	booktitle = {Proceedings of the 36th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Azzopardi, Leif and Kelly, Diane and Brennan, Kathy},
	year = {2013},
	pages = {23--32},
}

@inproceedings{browne_stopping_2005,
	title = {Stopping {Rule} {Use} {During} {Web}-{Based} {Search}},
	doi = {10.1109/HICSS.2005.556},
	booktitle = {Proceedings of the 38th {Annual} {Hawaii} {International} {Conference} on {System} {Sciences}},
	author = {Browne, G.J. and Pitts, M.G. and Wetherbe, J.C.},
	year = {2005},
	pages = {271b--271b},
}

@article{blanca_skewness_2013,
	title = {Skewness and {Kurtosis} in {Real} {Data} {Samples}},
	volume = {9},
	doi = {10.1027/1614-2241/a000057},
	number = {2},
	journal = {Methodology},
	author = {Blanca, María J. and Arnau, Jaume and López-Montiel, Dolores and Bono, Roser and Bendayan, Rebecca},
	year = {2013},
	pages = {78--84},
}

@article{balanda_kurtosis_1988,
	title = {Kurtosis: {A} {Critical} {Review}},
	volume = {42},
	url = {http://www.jstor.org/stable/2684482},
	number = {2},
	journal = {The American Statistician},
	author = {Balanda, Kevin P. and MacGillivray, H. L.},
	year = {1988},
	pages = {111--119},
}

@inproceedings{azzopardi_economics_2011,
	address = {Beijing, China},
	series = {{SIGIR} '11},
	title = {The economics in interactive information retrieval},
	doi = {10.1145/2009916.2009923},
	booktitle = {Proceedings of the 34th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Azzopardi, Leif},
	year = {2011},
	pages = {15--24},
}

@article{altman_diagnostic_1994,
	title = {Diagnostic tests. 1: {Sensitivity} and specificity},
	volume = {308},
	doi = {10.1136/bmj.308.6943.1552},
	number = {6943},
	journal = {BMJ (Clinical research ed.)},
	author = {Altman, D. G. and Bland, J. M.},
	year = {1994},
	pages = {1552},
}

@article{teixeira_da_silva_silent_2016,
	title = {Silent or {Stealth} {Retractions}, the {Dangerous} {Voices} of the {Unknown}, {Deleted} {Literature}},
	volume = {32},
	issn = {1053-8801, 1936-4792},
	url = {http://link.springer.com/10.1007/s12109-015-9439-y},
	doi = {10.1007/s12109-015-9439-y},
	language = {en},
	number = {1},
	urldate = {2025-01-16},
	journal = {Publishing Research Quarterly},
	author = {Teixeira Da Silva, Jaime A.},
	month = mar,
	year = {2016},
	pages = {44--53},
}

@article{steen_why_2013,
	title = {Why {Has} the {Number} of {Scientific} {Retractions} {Increased}?},
	volume = {8},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0068397},
	doi = {10.1371/journal.pone.0068397},
	language = {en},
	number = {7},
	urldate = {2025-01-16},
	journal = {PLoS ONE},
	author = {Steen, R. Grant and Casadevall, Arturo and Fang, Ferric C.},
	editor = {Derrick, Gemma Elizabeth},
	month = jul,
	year = {2013},
	pages = {e68397},
}

@article{kuhberger_self-correction_2022,
	title = {Self-correction in science: {The} effect of retraction on the frequency of citations},
	volume = {17},
	issn = {1932-6203},
	shorttitle = {Self-correction in science},
	doi = {10.1371/journal.pone.0277814},
	abstract = {We investigate the citation frequency of retracted scientific papers in science. For the period of five years before and after retraction, we counted the citations to papers in a sample of over 3,000 retracted, and a matched sample of another 3,000 non-retracted papers. Retraction led to a decrease in average annual citation frequency from about 5 before, to 2 citations after retraction. In contrast, for non-retracted control papers the citation counts were 4, and 5, respectively. Put differently, we found only a limited effect of retraction: retraction decreased citation frequency only by about 60\%, as compared to non-retracted papers. Thus, retracted papers often live on. For effective self-correction the scientific enterprise needs to be more effective in removing retracted papers from the scientific record. We discuss recent proposals to do so.},
	language = {eng},
	number = {12},
	journal = {PloS One},
	author = {Kühberger, Anton and Streit, Daniel and Scherndl, Thomas},
	year = {2022},
	pmid = {36477092},
	pmcid = {PMC9728909},
	pages = {e0277814},
}

@article{cochrane_infectious_diseases_group_rapid_2022,
	title = {Rapid, point-of-care antigen tests for diagnosis of {SARS}-{CoV}-2 infection},
	volume = {2022},
	issn = {14651858},
	url = {http://doi.wiley.com/10.1002/14651858.CD013705.pub3},
	doi = {10.1002/14651858.CD013705.pub3},
	language = {en},
	number = {7},
	urldate = {2025-01-15},
	journal = {Cochrane Database of Systematic Reviews},
	author = {Dinnes, Jacqueline and Sharma, Pawana and Berhane, Sarah and Van Wyk, Susanna S and Nyaaba, Nicholas and Domen, Julie and Taylor, Melissa and Cunningham, Jane and Davenport, Clare and Dittrich, Sabine and Emperador, Devy and Hooft, Lotty and Leeflang, Mariska Mg and McInnes, Matthew Df and Spijker, René and Verbakel, Jan Y and Takwoingi, Yemisi and Taylor-Phillips, Sian and Van Den Bruel, Ann and Deeks, Jonathan J and {Cochrane COVID-19 Diagnostic Test Accuracy Group}},
	editor = {{Cochrane Infectious Diseases Group}},
	month = jul,
	year = {2022},
}

@misc{noauthor_anonymized_nodate,
	title = {Anonymized {Repository} - {Anonymous} {GitHub}},
	url = {https://anonymous.4open.science/r/RetractionWatch/README.md},
	urldate = {2025-01-15},
}

@misc{noauthor_introducing_nodate,
	title = {Introducing {Claude} 3.5 {Sonnet}},
	url = {https://www.anthropic.com/news/claude-3-5-sonnet},
	abstract = {Introducing Claude 3.5 Sonnet—our most intelligent model yet. Sonnet now outperforms competitor models and Claude 3 Opus on key evaluations, at twice the speed.},
	language = {en},
	urldate = {2025-01-15},
}

@misc{noauthor_openai_nodate,
	title = {{OpenAI} {Platform}},
	url = {https://platform.openai.com},
	abstract = {Explore developer resources, tutorials, API docs, and dynamic examples to get the most out of OpenAI's platform.},
	language = {en},
	urldate = {2025-01-15},
}

@misc{noauthor_arxivorg_nodate,
	title = {{arXiv}.org e-{Print} archive},
	url = {https://arxiv.org/},
	urldate = {2025-01-15},
}

@article{van_der_vet_propagation_2016,
	title = {Propagation of errors in citation networks: a study involving the entire citation network of a widely cited paper published in, and later retracted from, the journal {Nature}},
	volume = {1},
	issn = {2058-8615},
	shorttitle = {Propagation of errors in citation networks},
	url = {https://researchintegrityjournal.biomedcentral.com/articles/10.1186/s41073-016-0008-5},
	doi = {10.1186/s41073-016-0008-5},
	abstract = {Abstract
            
              Background
              In about one in 10,000 cases, a published article is retracted. This very often means that the results it reports are flawed. Several authors have voiced concerns about the presence of retracted research in the memory of science. In particular, a retracted result is propagated by citing it. In the published literature, many instances are given of retracted articles that are cited both before and after their retraction. Even worse is the possibility that these articles in turn are cited in such a way that the retracted result is propagated further.
            
            
              Methods
              We have conducted a case study to find out how a retracted article is cited and whether retracted results are propagated through indirect citations. We have constructed the entire citation network for this case.
            
            
              Results
              We show that directly citing articles is an important source of propagation of retracted research results. In contrast, in our case study, indirect citations do not contribute to the propagation of the retracted result.
            
            
              Conclusions
              While admitting the limitations of a study involving a single case, we think there are reasons for the non-contribution of indirect citations that hold beyond our case study.},
	language = {en},
	number = {1},
	urldate = {2025-01-14},
	journal = {Research Integrity and Peer Review},
	author = {Van Der Vet, Paul E. and Nijveen, Harm},
	month = may,
	year = {2016},
	pages = {3},
}

@article{heibi_qualitative_2021,
	title = {A qualitative and quantitative analysis of open citations to retracted articles: the {Wakefield} 1998 et al.'s case},
	volume = {126},
	issn = {0138-9130, 1588-2861},
	shorttitle = {A qualitative and quantitative analysis of open citations to retracted articles},
	url = {https://link.springer.com/10.1007/s11192-021-04097-5},
	doi = {10.1007/s11192-021-04097-5},
	abstract = {Abstract
            
              In this article, we show the results of a quantitative and qualitative analysis of open citations on a popular and highly cited retracted paper: “Ileal-lymphoid-nodular hyperplasia, non-specific colitis and pervasive developmental disorder in children” by Wakefield et al
              .
              , published in 1998. The main purpose of our study is to understand the behavior of the publications citing one retracted article and the characteristics of the citations the retracted article accumulated over time. Our analysis is based on a methodology which illustrates how we gathered the data, extracted the topics of the citing articles and visualized the results. The data and services used are all open and free to foster the reproducibility of the analysis. The outcomes concerned the analysis of the entities citing Wakefield et al
              .
              ’s article and their related in-text citations. We observed a constant increasing number of citations in the last 20 years, accompanied with a constant increment in the percentage of those acknowledging its retraction. Citing articles have started either discussing or dealing with the retraction of Wakefield et al
              .
              ’s article even before its full retraction happened in 2010. Articles in the social sciences domain citing the Wakefield et al
              .
              ’s one were among those that have mostly discussed its retraction. In addition, when observing the in-text citations, we noticed that a large number of the citations received by Wakefield et al
              .
              ’s article has focused on general discussions without recalling strictly medical details, especially after the full retraction. Medical studies did not hesitate in acknowledging the retraction of the Wakefield et al
              .
              ’s article and often provided strong negative statements on it.},
	language = {en},
	number = {10},
	urldate = {2025-01-14},
	journal = {Scientometrics},
	author = {Heibi, Ivan and Peroni, Silvio},
	month = oct,
	year = {2021},
	pages = {8433--8470},
}

@article{schneider_continued_2020,
	title = {Continued post-retraction citation of a fraudulent clinical trial report, 11 years after it was retracted for falsifying data},
	volume = {125},
	issn = {0138-9130, 1588-2861},
	url = {https://link.springer.com/10.1007/s11192-020-03631-1},
	doi = {10.1007/s11192-020-03631-1},
	abstract = {Abstract
            
              This paper presents a case study of long-term post-retraction citation to falsified clinical trial data (Matsuyama et al. in Chest 128(6):3817–3827, 2005.
              10.1378/chest.128.6.3817
              ), demonstrating problems with how the current digital library environment communicates retraction status. Eleven years after its retraction, the paper continues to be cited positively and uncritically to support a medical nutrition intervention, without mention of its 2008 retraction for falsifying data. To date no high quality clinical trials reporting on the efficacy of omega-3 fatty acids on reducing inflammatory markers have been published. Our paper uses network analysis, citation context analysis, and retraction status visibility analysis to illustrate the potential for extended propagation of misinformation over a citation network, updating and extending a case study of the first 6 years of post-retraction citation (Fulton et al. in Publications 3(1):7–26, 2015.
              10.3390/publications3010017
              ). The current study covers 148 direct citations from 2006 through 2019 and their 2542 second-generation citations and assesses retraction status visibility of the case study paper and its retraction notice on 12 digital platforms as of 2020. The retraction is not mentioned in 96\% (107/112) of direct post-retraction citations for which we were able to conduct citation context analysis. Over 41\% (44/107) of direct post-retraction citations that do not mention the retraction describe the case study paper in detail, giving a risk of diffusing misinformation from the case paper. We analyze 152 second-generation citations to the most recent 35 direct citations (2010–2019) that do not mention the retraction but do mention methods or results of the case paper, finding 23 possible diffusions of misinformation from these non-direct citations to the case paper. Link resolving errors from databases show a significant challenge in a reader reaching the retraction notice via a database search. Only 1/8 databases (and 1/9 database records) consistently resolved the retraction notice to its full-text correctly in our tests. Although limited to evaluation of a single case (
              N 
              = 1), this work demonstrates how retracted research can continue to spread and how the current information environment contributes to this problem.},
	language = {en},
	number = {3},
	urldate = {2025-01-14},
	journal = {Scientometrics},
	author = {Schneider, Jodi and Ye, Di and Hill, Alison M. and Whitehorn, Ashley S.},
	month = dec,
	year = {2020},
	pages = {2877--2913},
}

@article{byrne_digital_2020,
	title = {Digital magic, or the dark arts of the 21st century—how can journals and peer reviewers detect manuscripts and publications from paper mills?},
	volume = {594},
	url = {https://febs.onlinelibrary.wiley.com/doi/abs/10.1002/1873-3468.13747},
	doi = {https://doi.org/10.1002/1873-3468.13747},
	abstract = {In recent years, it has been proposed that unrealistic requirements for academics and medical doctors to publish in scientific journals, combined with monetary publication rewards, have led to forms of contract cheating offered by organizations known as paper mills. Paper mills are alleged to offer products ranging from research data through to ghostwritten fraudulent or fabricated manuscripts and submission services. While paper mill operations remain poorly understood, it seems likely that paper mills need to balance product quantity and quality, such that they produce or contribute to large numbers of manuscripts that will be accepted for publication. Producing manuscripts at scale may be facilitated by the use of manuscript templates, which could give rise to shared features such as textual and organizational similarities, the description of highly generic study hypotheses and experimental approaches, digital images that show evidence of manipulation and/or reuse, and/or errors affecting verifiable experimental reagents. Based on these features, we propose practical steps that editors, journal staff, and peer reviewers can take to recognize and respond to research manuscripts and publications that may have been produced with undeclared assistance from paper mills.},
	number = {4},
	journal = {FEBS Letters},
	author = {Byrne, Jennifer A. and Christopher, Jana},
	year = {2020},
	note = {\_eprint: https://febs.onlinelibrary.wiley.com/doi/pdf/10.1002/1873-3468.13747},
	pages = {583--589},
}

@book{hastie_elements_2009,
	address = {New York, NY},
	edition = {Second},
	series = {Springer {Series} in {Statistics}},
	title = {The {Elements} of {Statistical} {Learning}},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-0-387-84857-0 978-0-387-84858-7},
	url = {http://link.springer.com/10.1007/978-0-387-84858-7},
	urldate = {2025-01-12},
	publisher = {Springer New York},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	year = {2009},
	doi = {10.1007/978-0-387-84858-7},
}

@article{fawcett_introduction_2006-1,
	title = {An introduction to {ROC} analysis},
	volume = {27},
	issn = {0167-8655},
	url = {https://www.sciencedirect.com/science/article/pii/S016786550500303X},
	doi = {https://doi.org/10.1016/j.patrec.2005.10.010},
	abstract = {Receiver operating characteristics (ROC) graphs are useful for organizing classifiers and visualizing their performance. ROC graphs are commonly used in medical decision making, and in recent years have been used increasingly in machine learning and data mining research. Although ROC graphs are apparently simple, there are some common misconceptions and pitfalls when using them in practice. The purpose of this article is to serve as an introduction to ROC graphs and as a guide for using them in research.},
	number = {8},
	journal = {Pattern Recognition Letters},
	author = {Fawcett, Tom},
	year = {2006},
	keywords = {Classifier evaluation, Evaluation metrics, ROC analysis},
	pages = {861--874},
}

@article{altman_diagnostic_1994-1,
	title = {Diagnostic tests. 1: {Sensitivity} and specificity},
	volume = {308},
	issn = {0959-8138},
	shorttitle = {Diagnostic tests. 1},
	doi = {10.1136/bmj.308.6943.1552},
	language = {eng},
	number = {6943},
	journal = {BMJ (Clinical research ed.)},
	author = {Altman, D. G. and Bland, J. M.},
	month = jun,
	year = {1994},
	pmid = {8019315},
	pmcid = {PMC2540489},
	keywords = {Clinical Laboratory Techniques, Humans, Sensitivity and Specificity},
	pages = {1552},
}

@article{reitsma_bivariate_2005,
	title = {Bivariate analysis of sensitivity and specificity produces informative summary measures in diagnostic reviews},
	volume = {58},
	issn = {0895-4356},
	url = {https://doi.org/10.1016/j.jclinepi.2005.02.022},
	doi = {10.1016/j.jclinepi.2005.02.022},
	number = {10},
	urldate = {2024-12-23},
	journal = {Journal of Clinical Epidemiology},
	author = {Reitsma, Johannes B. and Glas, Afina S. and Rutjes, Anne W.S. and Scholten, Rob J.P.M. and Bossuyt, Patrick M. and Zwinderman, Aeilko H.},
	month = oct,
	year = {2005},
	note = {Publisher: Elsevier},
	pages = {982--990},
}

@article{blanca_skewness_2013-1,
	title = {Skewness and {Kurtosis} in {Real} {Data} {Samples}},
	volume = {9},
	issn = {1614-1881, 1614-2241},
	url = {https://econtent.hogrefe.com/doi/10.1027/1614-2241/a000057},
	doi = {10.1027/1614-2241/a000057},
	abstract = {Parametric statistics are based on the assumption of normality. Recent findings suggest that Type I error and power can be adversely affected when data are non-normal. This paper aims to assess the distributional shape of real data by examining the values of the third and fourth central moments as a measurement of skewness and kurtosis in small samples. The analysis concerned 693 distributions with a sample size ranging from 10 to 30. Measures of cognitive ability and of other psychological variables were included. The results showed that skewness ranged between −2.49 and 2.33. The values of kurtosis ranged between −1.92 and 7.41. Considering skewness and kurtosis together the results indicated that only 5.5\% of distributions were close to expected values under normality. Although extreme contamination does not seem to be very frequent, the findings are consistent with previous research suggesting that normality is not the rule with real data.},
	language = {en},
	number = {2},
	urldate = {2024-12-23},
	journal = {Methodology},
	author = {Blanca, María J. and Arnau, Jaume and López-Montiel, Dolores and Bono, Roser and Bendayan, Rebecca},
	month = may,
	year = {2013},
	pages = {78--84},
}

@article{decarlo_meaning_1997-1,
	title = {On the meaning and use of kurtosis.},
	volume = {2},
	issn = {1939-1463, 1082-989X},
	url = {https://doi.apa.org/doi/10.1037/1082-989X.2.3.292},
	doi = {10.1037/1082-989X.2.3.292},
	language = {en},
	number = {3},
	urldate = {2024-12-23},
	journal = {Psychological Methods},
	author = {DeCarlo, Lawrence T.},
	month = sep,
	year = {1997},
	pages = {292--307},
}

@misc{gu_domain-specific_2021,
	title = {Domain-{Specific} {Language} {Model} {Pretraining} for {Biomedical} {Natural} {Language} {Processing}},
	url = {http://arxiv.org/abs/2007.15779},
	abstract = {Pretraining large neural language models, such as BERT, has led to impressive gains on many natural language processing (NLP) tasks. However, most pretraining efforts focus on general domain corpora, such as newswire and Web. A prevailing assumption is that even domain-specific pretraining can benefit by starting from general-domain language models. In this paper, we challenge this assumption by showing that for domains with abundant unlabeled text, such as biomedicine, pretraining language models from scratch results in substantial gains over continual pretraining of general-domain language models. To facilitate this investigation, we compile a comprehensive biomedical NLP benchmark from publicly-available datasets. Our experiments show that domain-specific pretraining serves as a solid foundation for a wide range of biomedical NLP tasks, leading to new state-of-the-art results across the board. Further, in conducting a thorough evaluation of modeling choices, both for pretraining and task-specific fine-tuning, we discover that some common practices are unnecessary with BERT models, such as using complex tagging schemes in named entity recognition (NER). To help accelerate research in biomedical NLP, we have released our state-of-the-art pretrained and task-specific models for the community, and created a leaderboard featuring our BLURB benchmark (short for Biomedical Language Understanding \& Reasoning Benchmark) at https://aka.ms/BLURB.},
	urldate = {2024-11-13},
	publisher = {arXiv},
	author = {Gu, Yu and Tinn, Robert and Cheng, Hao and Lucas, Michael and Usuyama, Naoto and Liu, Xiaodong and Naumann, Tristan and Gao, Jianfeng and Poon, Hoifung},
	month = sep,
	year = {2021},
	note = {arXiv:2007.15779 
version: 6},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{noauthor_blurb_nodate,
	title = {{BLURB} {Leaderboard}},
	url = {https://microsoft.github.io/BLURB/},
	urldate = {2024-11-13},
}

@misc{noauthor_mecir_nodate,
	title = {{MECIR} {Manual} {\textbar} {Cochrane} {Community}},
	url = {https://community.cochrane.org/mecir-manual},
	abstract = {Methodological Expectations of Cochrane Intervention Reviews (MECIR) Version August 2023},
	language = {en},
	urldate = {2024-11-13},
}

@article{briscoe_conduct_2019,
	title = {Conduct and reporting of citation searching in {Cochrane} systematic reviews: {A} cross‐sectional study},
	volume = {11},
	shorttitle = {Conduct and reporting of citation searching in {Cochrane} systematic reviews},
	url = {https://pmc.ncbi.nlm.nih.gov/articles/PMC7079050/},
	doi = {10.1002/jrsm.1355},
	abstract = {The search for studies for a systematic review should be conducted systematically and reported transparently to facilitate reproduction. This study aimed to report on the conduct and reporting of backward citation searching (ie, checking reference ...},
	language = {en},
	number = {2},
	urldate = {2024-11-13},
	journal = {Research Synthesis Methods},
	author = {Briscoe, Simon and Bethel, Alison and Rogers, Morwenna},
	month = jul,
	year = {2019},
	pmid = {31127978},
	pages = {169},
}

@misc{zhuang_promptreps_2024,
	title = {{PromptReps}: {Prompting} {Large} {Language} {Models} to {Generate} {Dense} and {Sparse} {Representations} for {Zero}-{Shot} {Document} {Retrieval}},
	shorttitle = {{PromptReps}},
	url = {http://arxiv.org/abs/2404.18424},
	abstract = {Utilizing large language models (LLMs) for zero-shot document ranking is done in one of two ways: 1) prompt-based re-ranking methods, which require no further training but are only feasible for re-ranking a handful of candidate documents due to computational costs; and 2) unsupervised contrastive trained dense retrieval methods, which can retrieve relevant documents from the entire corpus but require a large amount of paired text data for contrastive training. In this paper, we propose PromptReps, which combines the advantages of both categories: no need for training and the ability to retrieve from the whole corpus. Our method only requires prompts to guide an LLM to generate query and document representations for effective document retrieval. Specifically, we prompt the LLMs to represent a given text using a single word, and then use the last token's hidden states and the corresponding logits associated with the prediction of the next token to construct a hybrid document retrieval system. The retrieval system harnesses both dense text embedding and sparse bag-of-words representations given by the LLM. We further explore variations of this core idea that consider the generation of multiple words, and representations that rely on multiple embeddings and sparse distributions. Our experimental evaluation on the MSMARCO, TREC deep learning and BEIR zero-shot document retrieval datasets illustrates that this simple prompt-based LLM retrieval method can achieve a similar or higher retrieval effectiveness than state-of-the-art LLM embedding methods that are trained with large amounts of unsupervised data, especially when using a larger LLM.},
	urldate = {2024-06-27},
	publisher = {arXiv},
	author = {Zhuang, Shengyao and Ma, Xueguang and Koopman, Bevan and Lin, Jimmy and Zuccon, Guido},
	month = jun,
	year = {2024},
	doi = {10.48550/arXiv.2404.18424},
	keywords = {Computer Science - Information Retrieval},
}

@misc{akinseloyin_novel_2024,
	title = {A {Novel} {Question}-{Answering} {Framework} for {Automated} {Abstract} {Screening} {Using} {Large} {Language} {Models}},
	url = {https://www.medrxiv.org/content/10.1101/2023.12.17.23300102v3},
	abstract = {Objective This paper aims to address the challenges in abstract screening within Systematic Reviews (SR) by leveraging the zero-shot capabilities of large language models (LLMs). Methods We employ LLM to prioritise candidate studies by aligning abstracts with the selection criteria outlined in an SR protocol. Abstract screening was transformed into a novel question-answering (QA) framework, treating each selection criterion as a question addressed by LLM. The framework involves breaking down the selection criteria into multiple questions, properly prompting LLM to answer each question, scoring and re-ranking each answer, and combining the responses to make nuanced inclusion or exclusion decisions. Results Large-scale validation was performed on the benchmark of CLEF eHealth 2019 Task 2: Technology- Assisted Reviews in Empirical Medicine. Focusing on GPT-3.5 as a case study, the proposed QA framework consistently exhibited a clear advantage over traditional information retrieval approaches and bespoke BERT- family models that were fine-tuned for prioritising candidate studies (i.e., from the BERT to PubMedBERT) across 31 datasets of four categories of SRs, underscoring their high potential in facilitating abstract screening. Conclusion Investigation justified the indispensable value of leveraging selection criteria to improve the performance of automated abstract screening. LLMs demonstrated proficiency in prioritising candidate studies for abstract screening using the proposed QA framework. Significant performance improvements were obtained by re-ranking answers using the semantic alignment between abstracts and selection criteria. This further highlighted the pertinence of utilizing selection criteria to enhance abstract screening.},
	language = {en},
	urldate = {2024-06-27},
	publisher = {medRxiv},
	author = {Akinseloyin, Opeoluwa and Jiang, Xiaorui and Palade, Vasile},
	month = jun,
	year = {2024},
	doi = {10.1101/2023.12.17.23300102},
	note = {tex.copyright: © 2024, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
}

@article{smith_less_2018,
	title = {Less is more: {Sampling} chemical space with active learning},
	volume = {148},
	issn = {0021-9606},
	shorttitle = {Less is more},
	url = {https://doi.org/10.1063/1.5023802},
	doi = {10.1063/1.5023802},
	abstract = {The development of accurate and transferable machine learning (ML) potentials for predicting molecular energetics is a challenging task. The process of data generation to train such ML potentials is a task neither well understood nor researched in detail. In this work, we present a fully automated approach for the generation of datasets with the intent of training universal ML potentials. It is based on the concept of active learning (AL) via Query by Committee (QBC), which uses the disagreement between an ensemble of ML potentials to infer the reliability of the ensemble’s prediction. QBC allows the presented AL algorithm to automatically sample regions of chemical space where the ML potential fails to accurately predict the potential energy. AL improves the overall fitness of ANAKIN-ME (ANI) deep learning potentials in rigorous test cases by mitigating human biases in deciding what new training data to use. AL also reduces the training set size to a fraction of the data required when using naive random sampling techniques. To provide validation of our AL approach, we develop the COmprehensive Machine-learning Potential (COMP6) benchmark (publicly available on GitHub) which contains a diverse set of organic molecules. Active learning-based ANI potentials outperform the original random sampled ANI-1 potential with only 10\% of the data, while the final active learning-based model vastly outperforms ANI-1 on the COMP6 benchmark after training to only 25\% of the data. Finally, we show that our proposed AL technique develops a universal ANI potential (ANI-1x) that provides accurate energy and force predictions on the entire COMP6 benchmark. This universal ML potential achieves a level of accuracy on par with the best ML potentials for single molecules or materials, while remaining applicable to the general class of organic molecules composed of the elements CHNO.},
	number = {24},
	urldate = {2024-07-30},
	journal = {The Journal of Chemical Physics},
	author = {Smith, Justin S. and Nebgen, Ben and Lubbers, Nicholas and Isayev, Olexandr and Roitberg, Adrian E.},
	month = may,
	year = {2018},
	pages = {241733},
}

@inproceedings{wang_neural_2022,
	title = {Neural {Rankers} for {Effective} {Screening} {Prioritisation} in {Medical} {Systematic} {Review} {Literature} {Search}},
	url = {http://arxiv.org/abs/2212.09017},
	doi = {10.1145/3572960.3572980},
	abstract = {Medical systematic reviews typically require assessing all the documents retrieved by a search. The reason is two-fold: the task aims for “total recall”; and documents retrieved using Boolean search are an unordered set, and thus it is unclear how an assessor could examine only a subset. Screening prioritisation is the process of ranking the (unordered) set of retrieved documents, allowing assessors to begin the downstream processes of the systematic review creation earlier, leading to earlier completion of the review, or even avoiding screening documents ranked least relevant. Screening prioritisation requires highly effective ranking methods. Pre-trained language models are state-of-the-art on many IR tasks but have yet to be applied to systematic review screening prioritisation. In this paper, we apply several pre-trained language models to the systematic review document ranking task, both directly and fine-tuned. An empirical analysis compares how effective neural methods compare to traditional methods for this task. We also investigate different types of document representations for neural methods and their impact on ranking performance. Our results show that BERT-based rankers outperform the current state-of-the-art screening prioritisation methods. However, BERT rankers and existing methods can actually be complementary, and thus, further improvements may be achieved if used in conjunction.},
	urldate = {2024-06-27},
	booktitle = {Proceedings of the 26th {Australasian} {Document} {Computing} {Symposium}},
	author = {Wang, Shuai and Scells, Harrisen and Koopman, Bevan and Zuccon, Guido},
	month = dec,
	year = {2022},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Information Retrieval, Computer Science - Machine Learning},
	pages = {1--10},
}

@article{shemilt_use_2016,
	title = {Use of cost-effectiveness analysis to compare the efficiency of study identification methods in systematic reviews},
	volume = {5},
	issn = {2046-4053},
	doi = {10.1186/s13643-016-0315-4},
	abstract = {BACKGROUND: Meta-research studies investigating methods, systems, and processes designed to improve the efficiency of systematic review workflows can contribute to building an evidence base that can help to increase value and reduce waste in research. This study demonstrates the use of an economic evaluation framework to compare the costs and effects of four variant approaches to identifying eligible studies for consideration in systematic reviews. METHODS: A cost-effectiveness analysis was conducted using a basic decision-analytic model, to compare the relative efficiency of 'safety first', 'double screening', 'single screening' and 'single screening with text mining' approaches in the title-abstract screening stage of a 'case study' systematic review about undergraduate medical education in UK general practice settings. Incremental cost-effectiveness ratios (ICERs) were calculated as the 'incremental cost per citation 'saved' from inappropriate exclusion' from the review. Resource use and effect parameters were estimated based on retrospective analysis of 'review process' meta-data curated alongside the 'case study' review, in conjunction with retrospective simulation studies to model the integrated use of text mining. Unit cost parameters were estimated based on the 'case study' review's project budget. A base case analysis was conducted, with deterministic sensitivity analyses to investigate the impact of variations in values of key parameters. RESULTS: Use of 'single screening with text mining' would have resulted in title-abstract screening workload reductions (base case analysis) of {\textgreater}60 \% compared with other approaches. Across modelled scenarios, the 'safety first' approach was, consistently, equally effective and less costly than conventional 'double screening'. Compared with 'single screening with text mining', estimated ICERs for the two non-dominated approaches (base case analyses) ranged from £1975 ('single screening' without a 'provisionally included' code) to £4427 ('safety first' with a 'provisionally included' code) per citation 'saved'. Patterns of results were consistent between base case and sensitivity analyses. CONCLUSIONS: Alternatives to the conventional 'double screening' approach, integrating text mining, warrant further consideration as potentially more efficient approaches to identifying eligible studies for systematic reviews. Comparable economic evaluations conducted using other systematic review datasets are needed to determine the generalisability of these findings and to build an evidence base to inform guidance for review authors.},
	language = {eng},
	number = {1},
	journal = {Systematic Reviews},
	author = {Shemilt, Ian and Khan, Nada and Park, Sophie and Thomas, James},
	month = aug,
	year = {2016},
	pmid = {27535658},
	note = {tex.pmcid: PMC4989498},
	keywords = {Cost-Benefit Analysis, Data Mining, Humans, Patient Safety, Research Design},
	pages = {140},
}

@article{molinari_transferring_2022,
	title = {Transferring knowledge between topics in systematic reviews},
	volume = {16},
	issn = {2667-3053},
	url = {https://www.sciencedirect.com/science/article/pii/S2667305322000874},
	doi = {10.1016/j.iswa.2022.200150},
	abstract = {In the medical domain, a systematic review (SR) is a well-structured process aimed to review all available literature on a research question. This is however a laborious task, both in terms of money and time. As such, the automation of a SR with the aid of technology has received interest in several research communities, among which the Information Retrieval community. In this work, we experiment on the possibility of leveraging previously conducted systematic reviews to train a classifier/ranker which is later applied to a new SR. We also investigate on the possibility of pre-training Deep Learning models and eventually tuning them in an Active Learning process. Our results show that the pre-training of these models deliver a good zero-shot (i.e., with no fine-tuning) ranking, achieving an improvement of 79\% for the MAP metric, with respect to a standard classifier trained on few in-domain documents. However, the pre-trained deep learning algorithms fail to deliver consistent results when continuously trained in an Active Learning scenario: our analysis shows that using smaller sized models and employing adapter modules might enable an effective active learning training.},
	urldate = {2024-06-27},
	journal = {Intelligent Systems with Applications},
	author = {Molinari, Alessio and Kanoulas, Evangelos},
	month = nov,
	year = {2022},
	keywords = {Deep learning, Machine learning, Systematic reviews, Technology-assisted review, Transfer learning},
	pages = {200150},
}

@article{molinari_sal_2024,
	title = {{SALτ}: efficiently stopping {TAR} by improving priors estimates},
	volume = {38},
	issn = {1573-756X},
	shorttitle = {{SALτ}},
	url = {https://doi.org/10.1007/s10618-023-00961-5},
	doi = {10.1007/s10618-023-00961-5},
	abstract = {In high recall retrieval tasks, human experts review a large pool of documents with the goal of satisfying an information need. Documents are prioritized for review through an active learning policy, and the process is usually referred to as Technology-Assisted Review (TAR). TAR tasks also aim to stop the review process once the target recall is achieved to minimize the annotation cost. In this paper, we introduce a new stopping rule called SAL\$\$\_{\textbackslash}tau {\textasciicircum}R\$\$(SLD for Active Learning), a modified version of the Saerens–Latinne–Decaestecker algorithm (SLD) that has been adapted for use in active learning. Experiments show that our algorithm stops the review well ahead of the current state-of-the-art methods, while providing the same guarantees of achieving the target recall.},
	language = {en},
	number = {2},
	urldate = {2024-06-27},
	journal = {Data Mining and Knowledge Discovery},
	author = {Molinari, Alessio and Esuli, Andrea},
	month = mar,
	year = {2024},
	keywords = {Active learning, Systematic review, TAR, Technology-assisted review, e-Discovery},
	pages = {535--568},
}

@inproceedings{kusa_outcome-based_2023,
	address = {New York, NY, USA},
	series = {{ICTIR} '23},
	title = {Outcome-based {Evaluation} of {Systematic} {Review} {Automation}},
	isbn = {979-8-4007-0073-6},
	url = {https://doi.org/10.1145/3578337.3605135},
	doi = {10.1145/3578337.3605135},
	abstract = {Current methods of evaluating search strategies and automated citation screening for systematic literature reviews typically rely on counting the number of relevant publications (i.e. those to be included in the review) and not relevant publications (i.e. those to be excluded). Significant importance is put into promoting the retrieval of all relevant publications through great attention to recall-oriented measures, and demoting the retrieval of non-relevant publications through precision-oriented or cost metrics. This established practice, however, does not accurately reflect the reality of conducting a systematic review, because not all included publications have the same influence on the final outcome of the systematic review. More specifically, if an important publication gets excluded or included, this might significantly change the overall review outcome, while not including or excluding less influential studies may only have a limited impact. However, in terms of evaluation measures, all inclusion and exclusion decisions are treated equally and, therefore, failing to retrieve publications with little to no impact on the review outcome leads to the same decrease in recall as failing to retrieve crucial publications.We propose a new evaluation framework that takes into account the impact of the reported study on the overall systematic review outcome. We demonstrate the framework by extracting review meta-analysis data and estimating outcome effects using predictions from ranking runs on systematic reviews of interventions from CLEF TAR 2019 shared task. We further measure how closely the obtained outcomes are to the outcomes of the original review if the arbitrary rankings were used. We evaluate 74 runs using the proposed framework and compare the results with those obtained using standard IR measures. We find that accounting for the difference in review outcomes leads to a different assessment of the quality of a system than if traditional evaluation measures were used. Our analysis provides new insights into the evaluation of retrieval results in the context of systematic review automation, emphasising the importance of assessing the usefulness of each document beyond binary relevance.},
	urldate = {2024-06-27},
	booktitle = {Proceedings of the 2023 {ACM} {SIGIR} {International} {Conference} on {Theory} of {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Kusa, Wojciech and Zuccon, Guido and Knoth, Petr and Hanbury, Allan},
	month = aug,
	year = {2023},
	pages = {125--133},
}

@article{norman_measuring_2019,
	title = {Measuring the impact of screening automation on meta-analyses of diagnostic test accuracy},
	volume = {8},
	issn = {2046-4053},
	url = {https://doi.org/10.1186/s13643-019-1162-x},
	doi = {10.1186/s13643-019-1162-x},
	abstract = {The large and increasing number of new studies published each year is making literature identification in systematic reviews ever more time-consuming and costly. Technological assistance has been suggested as an alternative to the conventional, manual study identification to mitigate the cost, but previous literature has mainly evaluated methods in terms of recall (search sensitivity) and workload reduction. There is a need to also evaluate whether screening prioritization methods leads to the same results and conclusions as exhaustive manual screening. In this study, we examined the impact of one screening prioritization method based on active learning on sensitivity and specificity estimates in systematic reviews of diagnostic test accuracy.},
	number = {1},
	urldate = {2024-06-27},
	journal = {Systematic Reviews},
	author = {Norman, Christopher R. and Leeflang, Mariska M. G. and Porcher, Raphaël and Névéol, Aurélie},
	month = oct,
	year = {2019},
	keywords = {*Machine learning, *Systematic review as topic, Evidence based medicine, Natural language processing/*methods},
	pages = {243},
}

@article{tawfik_step_2019,
	title = {A step by step guide for conducting a systematic review and meta-analysis with simulation data},
	volume = {47},
	issn = {1349-4147},
	url = {https://doi.org/10.1186/s41182-019-0165-6},
	doi = {10.1186/s41182-019-0165-6},
	abstract = {The massive abundance of studies relating to tropical medicine and health has increased strikingly over the last few decades. In the field of tropical medicine and health, a well-conducted systematic review and meta-analysis (SR/MA) is considered a feasible solution for keeping clinicians abreast of current evidence-based medicine. Understanding of SR/MA steps is of paramount importance for its conduction. It is not easy to be done as there are obstacles that could face the researcher. To solve those hindrances, this methodology study aimed to provide a step-by-step approach mainly for beginners and junior researchers, in the field of tropical medicine and other health care fields, on how to properly conduct a SR/MA, in which all the steps here depicts our experience and expertise combined with the already well-known and accepted international guidance.},
	number = {1},
	urldate = {2024-07-29},
	journal = {Tropical Medicine and Health},
	author = {Tawfik, Gehad Mohamed and Dila, Kadek Agus Surya and Mohamed, Muawia Yousif Fadlelmola and Tam, Dao Ngoc Hien and Kien, Nguyen Dang and Ahmed, Ali Mahmoud and Huy, Nguyen Tien},
	month = aug,
	year = {2019},
	keywords = {Analysis, Data, Extraction, Results, Search, Study},
	pages = {46},
}

@inproceedings{althammer_annotating_2023,
	address = {New York, NY, USA},
	series = {{SIGIR}-{AP} '23},
	title = {Annotating {Data} for {Fine}-{Tuning} a {Neural} {Ranker}? {Current} {Active} {Learning} {Strategies} are not {Better} than {Random} {Selection}},
	isbn = {979-8-4007-0408-6},
	shorttitle = {Annotating {Data} for {Fine}-{Tuning} a {Neural} {Ranker}?},
	url = {https://doi.org/10.1145/3624918.3625333},
	doi = {10.1145/3624918.3625333},
	abstract = {Search methods based on Pretrained Language Models (PLM) have demonstrated great effectiveness gains compared to statistical and early neural ranking models. However, fine-tuning PLM-based rankers requires a great amount of annotated training data. Annotating data involves a large manual effort and thus is expensive, especially in domain specific tasks. In this paper we investigate fine-tuning PLM-based rankers under limited training data and budget. We investigate two scenarios: fine-tuning a ranker from scratch, and domain adaptation starting with a ranker already fine-tuned on general data, and continuing fine-tuning on a target dataset. We observe a great variability in effectiveness when fine-tuning on different randomly selected subsets of training data. This suggests that it is possible to achieve effectiveness gains by actively selecting a subset of the training data that has the most positive effect on the rankers. This way, it would be possible to fine-tune effective PLM rankers at a reduced annotation budget. To investigate this, we adapt existing Active Learning (AL) strategies to the task of fine-tuning PLM rankers and investigate their effectiveness, also considering annotation and computational costs. Our extensive analysis shows that AL strategies do not significantly outperform random selection of training subsets in terms of effectiveness. We further find that gains provided by AL strategies come at the expense of more assessments (thus higher annotation costs) and AL strategies underperform random selection when comparing effectiveness given a fixed annotation cost. Our results highlight that “optimal” subsets of training data that provide high effectiveness at low annotation cost do exist, but current mainstream AL strategies applied to PLM rankers are not capable of identifying them.},
	urldate = {2024-06-27},
	booktitle = {Proceedings of the {Annual} {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval} in the {Asia} {Pacific} {Region}},
	publisher = {Association for Computing Machinery},
	author = {Althammer, Sophia and Zuccon, Guido and Hofstätter, Sebastian and Verberne, Suzan and Hanbury, Allan},
	month = nov,
	year = {2023},
	pages = {139--149},
}

@article{carvallo_automatic_2020,
	title = {Automatic document screening of medical literature using word and text embeddings in an active learning setting},
	volume = {125},
	issn = {1588-2861},
	url = {https://doi.org/10.1007/s11192-020-03648-6},
	doi = {10.1007/s11192-020-03648-6},
	abstract = {Document screening is a fundamental task within Evidence-based Medicine (EBM), a practice that provides scientific evidence to support medical decisions. Several approaches have tried to reduce physicians’ workload of screening and labeling vast amounts of documents to answer clinical questions. Previous works tried to semi-automate document screening, reporting promising results, but their evaluation was conducted on small datasets, which hinders generalization. Moreover, recent works in natural language processing have introduced neural language models, but none have compared their performance in EBM. In this paper, we evaluate the impact of several document representations such as TF-IDF along with neural language models (BioBERT, BERT, Word2Vec, and GloVe) on an active learning-based setting for document screening in EBM. Our goal is to reduce the number of documents that physicians need to label to answer clinical questions. We evaluate these methods using both a small challenging dataset (CLEF eHealth 2017) as well as a larger one but easier to rank (Epistemonikos). Our results indicate that word as well as textual neural embeddings always outperform the traditional TF-IDF representation. When comparing among neural and textual embeddings, in the CLEF eHealth dataset the models BERT and BioBERT yielded the best results. On the larger dataset, Epistemonikos, Word2Vec and BERT were the most competitive, showing that BERT was the most consistent model across different corpuses. In terms of active learning, an uncertainty sampling strategy combined with a logistic regression achieved the best performance overall, above other methods under evaluation, and in fewer iterations. Finally, we compared the results of evaluating our best models, trained using active learning, with other authors methods from CLEF eHealth, showing better results in terms of work saved for physicians in the document-screening task.},
	language = {en},
	number = {3},
	urldate = {2024-06-27},
	journal = {Scientometrics},
	author = {Carvallo, Andres and Parra, Denis and Lobel, Hans and Soto, Alvaro},
	month = dec,
	year = {2020},
	keywords = {Active learning, Document screening, Natural language processing},
	pages = {3047--3084},
}

@inproceedings{hoi_batch_2006,
	address = {New York, NY, USA},
	series = {{ICML} '06},
	title = {Batch mode active learning and its application to medical image classification},
	isbn = {978-1-59593-383-6},
	url = {https://doi.org/10.1145/1143844.1143897},
	doi = {10.1145/1143844.1143897},
	abstract = {The goal of active learning is to select the most informative examples for manual labeling. Most of the previous studies in active learning have focused on selecting a single unlabeled example in each iteration. This could be inefficient since the classification model has to be retrained for every labeled example. In this paper, we present a framework for "batch mode active learning" that applies the Fisher information matrix to select a number of informative examples simultaneously. The key computational challenge is how to efficiently identify the subset of unlabeled examples that can result in the largest reduction in the Fisher information. To resolve this challenge, we propose an efficient greedy algorithm that is based on the property of submodular functions. Our empirical studies with five UCI datasets and one real-world medical image classification show that the proposed batch mode active learning algorithm is more effective than the state-of-the-art algorithms for active learning.},
	urldate = {2024-07-30},
	booktitle = {Proceedings of the 23rd international conference on {Machine} learning},
	publisher = {Association for Computing Machinery},
	author = {Hoi, Steven C. H. and Jin, Rong and Zhu, Jianke and Lyu, Michael R.},
	month = jun,
	year = {2006},
	pages = {417--424},
}

@article{kranke_evidence-based_2010,
	title = {Evidence-based practice: how to perform and use systematic reviews for clinical decision-making},
	volume = {27},
	issn = {1365-2346},
	shorttitle = {Evidence-based practice},
	doi = {10.1097/EJA.0b013e32833a560a},
	abstract = {One approach to clinical decision-making requires the integration of the best available research evidence with individual clinical expertise and patient values, and is known as evidence-based medicine (EBM). In clinical decision-making with the current best evidence, systematic reviews have an important role. This review article covers the basic principles of systematic reviews and meta-analyses, and their role in the process of evidence-based decision-making. The problems associated with traditional narrative reviews are discussed, as well as the way systematic reviews limit bias associated with the assembly, critical appraisal and synthesis of studies addressing specific clinical questions. The relevant steps in writing a systematic review from the formulation of an initial research question to sensitivity analyses in conjunction with the combined analysis of the pooled data are described. Important issues that need to be considered when appraising a systematic review or meta-analysis are outlined. Some of the terms that are used in the reporting of systematic reviews and meta-analyses, such as relative risk, confidence interval, Forest plot or L'Abbé plot, will be introduced and explained.},
	language = {eng},
	number = {9},
	journal = {European Journal of Anaesthesiology},
	author = {Kranke, Peter},
	month = sep,
	year = {2010},
	pmid = {20523217},
	keywords = {Clinical Trials as Topic, Data Interpretation, Decision Making, Decision Support Techniques, Evidence-Based Medicine, Humans, Meta-Analysis as Topic, Review Literature as Topic, Risk, Statistical, Statistics as Topic},
	pages = {763--772},
}

@article{giummarra_evaluation_2020,
	title = {Evaluation of text mining to reduce screening workload for injury-focused systematic reviews},
	volume = {26},
	issn = {1353-8047, 1475-5785},
	url = {https://injuryprevention.bmj.com/content/26/1/55},
	doi = {10.1136/injuryprev-2019-043247},
	abstract = {Introduction Text mining to support screening in large-scale systematic reviews has been recommended; however, their suitability for reviews in injury research is not known. We examined the performance of text mining in supporting the second reviewer in a systematic review examining associations between fault attribution and health and work-related outcomes after transport injury. Methods Citations were independently screened in Abstrackr in full (reviewer 1; 10 559 citations), and until no more citations were predicted to be relevant (reviewer 2; 1809 citations, 17.1\%). All potentially relevant full-text articles were assessed by reviewer 1 (555 articles). Reviewer 2 used text mining (Wordstat, QDA Miner) to reduce assessment to full-text articles containing ≥1 fault-related exposure term (367 articles, 66.1\%). Results Abstrackr offered excellent workload savings: 82.7\% of citations did not require screening by reviewer 2, and total screening time was reduced by 36.6\% compared with traditional dual screening of all citations. Abstrackr predictions had high specificity (83.7\%), and low false negatives (0.3\%), but overestimated citation relevance, probably due to the complexity of the review with multiple outcomes and high imbalance of relevant to irrelevant records, giving low sensitivity (29.7\%) and precision (14.5\%). Text mining of full-text articles reduced the number needing to be screened by 33.9\%, and reduced total full-text screening time by 38.7\% compared with traditional dual screening. Conclusions Overall, text mining offered important benefits to systematic review workflow, but should not replace full screening by one reviewer, especially for complex reviews examining multiple health or injury outcomes. Trial registration number CRD42018084123.},
	language = {en},
	number = {1},
	urldate = {2024-07-29},
	journal = {Injury Prevention},
	author = {Giummarra, Melita J. and Lau, Georgina and Gabbe, Belinda J.},
	month = feb,
	year = {2020},
	pmid = {31451565},
	note = {tex.copyright: © Author(s) (or their employer(s)) 2020. No commercial re-use. See rights and permissions. Published by BMJ.},
	keywords = {injury, research methods, road trauma, systematic reviews, text mining, transport injury},
	pages = {55--60},
}

@unpublished{fletcher_predicting_2024,
	title = {Predicting {Retracted} {Research}},
	abstract = {Retracting published research is an important safeguard against the dissemination of flawed or fraudulent scientific information. However, detecting problematic research prior to publication remains a challenge. This paper describes the creation of a novel dataset and machine learning models to predict retracted articles. The data set combines information from the Retraction Watch database and the OpenAlex API, including article metadata, abstracts, and citation metrics. A total of 16,224 articles (8,112 retracted and 8,112 nonretracted) published between 2000-2020 were included. Several machine learning models were trained on this data, with a gradient boosting approach achieving the best precision (0.691). An ablation study revealed that the abstract of the article was the most important feature for classification for the accuracy, recall, and F1 score metric. First Author Countries was the more important feature for feature-based classifers with the Precision Metric. This work demonstrates the potential for using machine learning to assist in identifying problematic research during the peer review process,though further improvements in model performance are needed before practi- cal application. The data set and code are made publicly available to support future work in this area.},
	author = {Fletcher, Aaron HA and Stevenson, Mark},
	month = jul,
	year = {2024},
}

@article{knoblock_active_2006,
	title = {Active {Learning} with {Multiple} {Views}},
	volume = {27},
	issn = {1076-9757},
	url = {http://arxiv.org/abs/1110.1073},
	doi = {10.1613/jair.2005},
	abstract = {Active learners alleviate the burden of labeling large amounts of data by detecting and asking the user to label only the most informative examples in the domain. We focus here on active learning for multi-view domains, in which there are several disjoint subsets of features (views), each of which is sufficient to learn the target concept. In this paper we make several contributions. First, we introduce Co-Testing, which is the first approach to multi-view active learning. Second, we extend the multi-view learning framework by also exploiting weak views, which are adequate only for learning a concept that is more general/specific than the target concept. Finally, we empirically show that Co-Testing outperforms existing active learners on a variety of real world domains such as wrapper induction, Web page classification, advertisement removal, and discourse tree parsing.},
	urldate = {2024-06-27},
	journal = {Journal of Artificial Intelligence Research},
	author = {Knoblock, C. A. and Minton, S. and Muslea, I.},
	month = oct,
	year = {2006},
	keywords = {Computer Science - Machine Learning},
	pages = {203--233},
}

@article{teijema_active_2023,
	title = {Active learning-based systematic reviewing using switching classification models: the case of the onset, maintenance, and relapse of depressive disorders},
	volume = {8},
	issn = {2504-0537},
	shorttitle = {Active learning-based systematic reviewing using switching classification models},
	doi = {10.3389/frma.2023.1178181},
	abstract = {INTRODUCTION: This study examines the performance of active learning-aided systematic reviews using a deep learning-based model compared to traditional machine learning approaches, and explores the potential benefits of model-switching strategies. METHODS: Comprising four parts, the study: 1) analyzes the performance and stability of active learning-aided systematic review; 2) implements a convolutional neural network classifier; 3) compares classifier and feature extractor performance; and 4) investigates the impact of model-switching strategies on review performance. RESULTS: Lighter models perform well in early simulation stages, while other models show increased performance in later stages. Model-switching strategies generally improve performance compared to using the default classification model alone. DISCUSSION: The study's findings support the use of model-switching strategies in active learning-based systematic review workflows. It is advised to begin the review with a light model, such as Naïve Bayes or logistic regression, and switch to a heavier classification model based on a heuristic rule when needed.},
	language = {eng},
	journal = {Frontiers in Research Metrics and Analytics},
	author = {Teijema, Jelle Jasper and Hofstee, Laura and Brouwer, Marlies and de Bruin, Jonathan and Ferdinands, Gerbrich and de Boer, Jan and Vizan, Pablo and van den Brand, Sofie and Bockting, Claudi and van de Schoot, Rens and Bagheri, Ayoub},
	year = {2023},
	pmid = {37260784},
	note = {tex.pmcid: PMC10227618},
	keywords = {active learning, convolutional neural network, model switching, simulations, systematic review, work saved over sampling},
	pages = {1178181},
}

@inproceedings{huang_active_2010,
	title = {Active {Learning} by {Querying} {Informative} and {Representative} {Examples}},
	volume = {23},
	url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/5487315b1286f907165907aa8fc96619-Abstract.html},
	abstract = {Most active learning approaches select either informative or representative unlabeled instances to query their labels. Although several active learning algorithms have been proposed to combine the two criterions for query selection, they are usually ad hoc in finding unlabeled instances that are both informative and representative. We address this challenge by a principled approach, termed QUIRE, based on the min-max view of active learning. The proposed approach provides a systematic way for measuring and combining the informativeness and representativeness of an instance. Extensive experimental results show that the proposed QUIRE approach outperforms several state-of -the-art active learning approaches.},
	urldate = {2024-06-27},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Huang, Sheng-jun and Jin, Rong and Zhou, Zhi-Hua},
	year = {2010},
}

@inproceedings{sharma_active_2015,
	address = {Denver, Colorado},
	title = {Active {Learning} with {Rationales} for {Text} {Classification}},
	url = {https://aclanthology.org/N15-1047},
	doi = {10.3115/v1/N15-1047},
	urldate = {2024-06-27},
	booktitle = {Proceedings of the 2015 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Sharma, Manali and Zhuang, Di and Bilgic, Mustafa},
	editor = {Mihalcea, Rada and Chai, Joyce and Sarkar, Anoop},
	month = may,
	year = {2015},
	pages = {441--451},
}

@article{felizardo_visual_2012,
	title = {A visual analysis approach to validate the selection review of primary studies in systematic reviews},
	volume = {54},
	issn = {0950-5849},
	url = {https://www.sciencedirect.com/science/article/pii/S0950584912000742},
	doi = {10.1016/j.infsof.2012.04.003},
	abstract = {Context Systematic Literature Reviews (SLRs) are an important component to identify and aggregate research evidence from different empirical studies. Despite its relevance, most of the process is conducted manually, implying additional effort when the Selection Review task is performed and leading to reading all studies under analysis more than once. Objective We propose an approach based on Visual Text Mining (VTM) techniques to assist the Selection Review task in SLR. It is implemented into a VTM tool (Revis), which is freely available for use. Method We have selected and implemented appropriate visualization techniques into our approach and validated and demonstrated its usefulness in performing real SLRs. Results The results have shown that employment of VTM techniques can successfully assist in the Selection Review task, speeding up the entire SLR process in comparison to the conventional approach. Conclusion VTM techniques are valuable tools to be used in the context of selecting studies in the SLR process, prone to speed up some stages of SLRs.},
	number = {10},
	urldate = {2024-07-29},
	journal = {Information and Software Technology},
	author = {Felizardo, Katia R. and Andery, Gabriel F. and Paulovich, Fernando V. and Minghim, Rosane and Maldonado, José C.},
	month = oct,
	year = {2012},
	keywords = {Citation document map, Content document map, Information visualization, Systematic Literature Review (SLR), Visual Text Mining (VTM)},
	pages = {1079--1091},
}

@article{nussbaumer-streit_resource_2021,
	title = {Resource use during systematic review production varies widely: a scoping review},
	volume = {139},
	issn = {0895-4356},
	shorttitle = {Resource use during systematic review production varies widely},
	url = {https://www.sciencedirect.com/science/article/pii/S0895435621001712},
	doi = {10.1016/j.jclinepi.2021.05.019},
	abstract = {Objective We aimed to map the resource use during systematic review (SR) production and reasons why steps of the SR production are resource intensive to discover where the largest gain in improving efficiency might be possible. Study design and setting We conducted a scoping review. An information specialist searched multiple databases (e.g., Ovid MEDLINE, Scopus) and implemented citation-based and grey literature searching. We employed dual and independent screenings of records at the title/abstract and full-text levels and data extraction. Results We included 34 studies. Thirty-two reported on the resource use—mostly time; four described reasons why steps of the review process are resource intensive. Study selection, data extraction, and critical appraisal seem to be very resource intensive, while protocol development, literature search, or study retrieval take less time. Project management and administration required a large proportion of SR production time. Lack of experience, domain knowledge, use of collaborative and SR-tailored software, and good communication and management can be reasons why SR steps are resource intensive. Conclusion Resource use during SR production varies widely. Areas with the largest resource use are administration and project management, study selection, data extraction, and critical appraisal of studies.},
	urldate = {2024-07-29},
	journal = {Journal of Clinical Epidemiology},
	author = {Nussbaumer-Streit, B. and Ellen, M. and Klerings, I. and Sfetcu, R. and Riva, N. and Mahmić-Kaknjo, M. and Poulentzas, G. and Martinez, P. and Baladia, E. and Ziganshina, L. E. and Marqués, M. E. and Aguilar, L. and Kassianos, A. P. and Frampton, G. and Silva, A. G. and Affengruber, L. and Spjker, R. and Thomas, J. and Berg, R. C. and Kontogiani, M. and Sousa, M. and Kontogiorgis, C. and Gartlehner, G.},
	month = nov,
	year = {2021},
	keywords = {Costs, Efficient, Evidence synthesis, Personnel, Resources, Time},
	pages = {287--296},
}

@article{ghasemi_scientific_2022,
	title = {Scientific {Publishing} in {Biomedicine}: {A} {Brief} {History} of {Scientific} {Journals}},
	volume = {21},
	issn = {1726-913X},
	shorttitle = {Scientific {Publishing} in {Biomedicine}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10024814/},
	doi = {10.5812/ijem-131812},
	abstract = {Scientific publishing, with about 350-year historical background, has played a central role in advancing science by disseminating new findings, generalizing accepted theories, and sharing novel ideas. The number of scientific journals has exponentially grown from 10 at the end of the 17th century to 100,000 at the end of the 20th century. The publishing landscape has dramatically changed over time from printed journals to online publishing. Although scientific publishing was initially non-commercial, it has become a profitable industry with a significant global financial turnover, reaching \$28 billion in annual revenue before the COVID-19 pandemic. However, scientific publishing has encountered several challenges and is suffering from unethical practices and some negative phenomena, like publish-or-perish, driven by the need to survive or get a promotion in academia. Developing a global landscape with collaborative non-commercial journals and platforms is a primary proposed model for the future of scientific publishing. Here, we provide a brief history of the foundation and development of scientific journals and their evolution over time. Furthermore, current challenges and future perspectives of scientific publishing are discussed.},
	number = {1},
	urldate = {2024-07-29},
	journal = {International Journal of Endocrinology and Metabolism},
	author = {Ghasemi, Asghar and Mirmiran, Parvin and Kashfi, Khosrow and Bahadoran, Zahra},
	month = dec,
	year = {2022},
	pmid = {36945344},
	note = {tex.pmcid: PMC10024814},
	pages = {e131812},
}

@misc{vladika_scientific_2023,
	title = {Scientific {Fact}-{Checking}: {A} {Survey} of {Resources} and {Approaches}},
	shorttitle = {Scientific {Fact}-{Checking}},
	url = {http://arxiv.org/abs/2305.16859},
	abstract = {The task of fact-checking deals with assessing the veracity of factual claims based on credible evidence and background knowledge. In particular, scientific fact-checking is the variation of the task concerned with verifying claims rooted in scientific knowledge. This task has received significant attention due to the growing importance of scientific and health discussions on online platforms. Automated scientific fact-checking methods based on NLP can help combat the spread of misinformation, assist researchers in knowledge discovery, and help individuals understand new scientific breakthroughs. In this paper, we present a comprehensive survey of existing research in this emerging field and its related tasks. We provide a task description, discuss the construction process of existing datasets, and analyze proposed models and approaches. Based on our findings, we identify intriguing challenges and outline potential future directions to advance the field.},
	urldate = {2024-06-27},
	publisher = {arXiv},
	author = {Vladika, Juraj and Matthes, Florian},
	month = may,
	year = {2023},
	doi = {10.48550/arXiv.2305.16859},
	keywords = {Computer Science - Computation and Language},
}

@misc{noauthor_oxford_nodate,
	title = {Oxford {Centre} for {Evidence}-{Based} {Medicine}: {Levels} of {Evidence} ({March} 2009)},
	shorttitle = {Oxford {Centre} for {Evidence}-{Based} {Medicine}},
	url = {https://www.cebm.ox.ac.uk/resources/levels-of-evidence/oxford-centre-for-evidence-based-medicine-levels-of-evidence-march-2009},
	language = {en},
	urldate = {2024-07-29},
	note = {Type: Web Page},
}

@misc{sadri_continuous_2022,
	title = {Continuous {Active} {Learning} {Using} {Pretrained} {Transformers}},
	url = {http://arxiv.org/abs/2208.06955},
	abstract = {Pre-trained and fine-tuned transformer models like BERT and T5 have improved the state of the art in ad-hoc retrieval and question-answering, but not as yet in high-recall information retrieval, where the objective is to retrieve substantially all relevant documents. We investigate whether the use of transformer-based models for reranking and/or featurization can improve the Baseline Model Implementation of the TREC Total Recall Track, which represents the current state of the art for high-recall information retrieval. We also introduce CALBERT, a model that can be used to continuously fine-tune a BERT-based model based on relevance feedback.},
	urldate = {2024-06-27},
	publisher = {arXiv},
	author = {Sadri, Nima and Cormack, Gordon V.},
	month = aug,
	year = {2022},
	doi = {10.48550/arXiv.2208.06955},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Information Retrieval, Computer Science - Machine Learning},
}

@article{swanson_how_2010,
	title = {How to {Practice} {Evidence}-{Based} {Medicine}},
	volume = {126},
	issn = {0032-1052},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4389891/},
	doi = {10.1097/PRS.0b013e3181dc54ee},
	abstract = {Evidence-based medicine (EBM) is defined as the conscientious, explicit and judicious use of current best evidence, combined with individual clinical expertise and patient preferences and values, in making decisions about the care of individual patients. In an effort to emphasize the importance of EBM in plastic surgery, ASPS and PRS have launched an initiative to improve the understanding of EBM concepts and provide tools for implementing EBM in practice. Through a series of special articles aimed at educating plastic surgeons, our hope is that readers will be compelled to learn more about EBM and incorporate its principles into their own practices. As the first of the series, this article provides a brief overview of the evolution, current application, and practice of EBM.},
	number = {1},
	urldate = {2024-07-29},
	journal = {Plastic and reconstructive surgery},
	author = {Swanson, Jennifer A. and Schmitz, DeLaine and Chung, Kevin C.},
	month = jul,
	year = {2010},
	pmid = {20224459},
	note = {tex.pmcid: PMC4389891},
	pages = {286--294},
}

@article{omara-eves_using_2015-1,
	title = {Using text mining for study identification in systematic reviews: a systematic review of current approaches},
	volume = {4},
	issn = {2046-4053},
	shorttitle = {Using text mining for study identification in systematic reviews},
	url = {https://doi.org/10.1186/2046-4053-4-5},
	doi = {10.1186/2046-4053-4-5},
	abstract = {The large and growing number of published studies, and their increasing rate of publication, makes the task of identifying relevant studies in an unbiased way for inclusion in systematic reviews both complex and time consuming. Text mining has been offered as a potential solution: through automating some of the screening process, reviewer time can be saved. The evidence base around the use of text mining for screening has not yet been pulled together systematically; this systematic review fills that research gap. Focusing mainly on non-technical issues, the review aims to increase awareness of the potential of these technologies and promote further collaborative research between the computer science and systematic review communities.},
	number = {1},
	urldate = {2024-06-27},
	journal = {Systematic Reviews},
	author = {O’Mara-Eves, Alison and Thomas, James and McNaught, John and Miwa, Makoto and Ananiadou, Sophia},
	month = jan,
	year = {2015},
	keywords = {Automation, Review efficiency, Screening, Study selection, Text mining},
	pages = {5},
}

@article{felizardo_visual_2013,
	title = {A visual approach to validate the selection review of primary studies in systematic reviews: {A} replication study},
	volume = {2013},
	shorttitle = {A visual approach to validate the selection review of primary studies in systematic reviews},
	abstract = {One of the activities associated with the systematic literature review (SLR) process is the selection of primary studies. When the researcher faces large volumes of primary studies to be analysed, the process used to select studies can be arduous, specially when the selection review activity is performed and all studies under analysis are read more than once. An experiment was conducted as a pilot test to compare the performance and accuracy of graduate students in conducting the selection review activity manually and using visual text mining (VTM) techniques. This paper describes a replication study that used the same experimental design and materials of the original experiment. The results have confirmed the outcomes of the original experiment, i.e., VTM is promising and can improve the performance of the selection review of primary studies. There is a positive relationship between the use of VTM techniques and the time spent to conduct the selection review activity.},
	journal = {Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE},
	author = {Felizardo, Katia and Barbosa, Ellen and Maldonado, José},
	month = jan,
	year = {2013},
	pages = {141--146},
}

@misc{mao_reproducibility_2024,
	title = {A {Reproducibility} {Study} of {Goldilocks}: {Just}-{Right} {Tuning} of {BERT} for {TAR}},
	shorttitle = {A {Reproducibility} {Study} of {Goldilocks}},
	url = {http://arxiv.org/abs/2401.08104},
	abstract = {Screening documents is a tedious and time-consuming aspect of high-recall retrieval tasks, such as compiling a systematic literature review, where the goal is to identify all relevant documents for a topic. To help streamline this process, many Technology-Assisted Review (TAR) methods leverage active learning techniques to reduce the number of documents requiring review. BERT-based models have shown high effectiveness in text classification, leading to interest in their potential use in TAR workflows. In this paper, we investigate recent work that examined the impact of further pre-training epochs on the effectiveness and efficiency of a BERT-based active learning pipeline. We first report that we could replicate the original experiments on two specific TAR datasets, confirming some of the findings: importantly, that further pre-training is critical to high effectiveness, but requires attention in terms of selecting the correct training epoch. We then investigate the generalisability of the pipeline on a different TAR task, that of medical systematic reviews. In this context, we show that there is no need for further pre-training if a domain-specific BERT backbone is used within the active learning pipeline. This finding provides practical implications for using the studied active learning pipeline within domain-specific TAR tasks.},
	urldate = {2024-06-27},
	publisher = {arXiv},
	author = {Mao, Xinyu and Koopman, Bevan and Zuccon, Guido},
	month = jan,
	year = {2024},
	doi = {10.48550/arXiv.2401.08104},
	keywords = {Computer Science - Information Retrieval},
}

@book{goeuriot_clef_2017,
	title = {{CLEF} 2017 {eHealth} {Evaluation} {Lab} {Overview}},
	isbn = {978-3-319-65812-4},
	abstract = {In this paper we provide an overview of the fifth edition of the CLEF eHealth evaluation lab. CLEF eHealth 2017 continues our evaluation resource building efforts around the easing and support of patients, their next-of-kins, clinical staff, and health scientists in understanding, accessing, and authoring eHealth information in a multilingual setting. This year’s lab offered three tasks: Task 1 on multilingual information extraction to extend from last year’s task on French corpora, Task 2 on technologically assisted reviews in empirical medicine as a new pilot task, and Task 3 on patient-centered information retrieval (IR) building on the 2013-16 IR tasks. In total 32 teams took part in these tasks (11 in Task 1, 14 in Task 2, and 7 in Task 3). We also continued the replication track from 2016. Herein, we describe the resources created for these tasks, evaluation methodology adopted and provide a brief summary of participants of this year’s challenges and results obtained. As in previous years, the organizers have made data and tools associated with the lab tasks available for future research and development.},
	author = {Goeuriot, Lorraine and Kelly, Liadh and Suominen, Hanna and Névéol, Aurélie and Robert, Aude and Kanoulas, Evangelos and Spijker, René and Palotti, João and Zuccon, Guido},
	month = aug,
	year = {2017},
	doi = {10.1007/978-3-319-65813-1_26},
}

@misc{wolff_enriched_2024,
	title = {Enriched {BERT} {Embeddings} for {Scholarly} {Publication} {Classification}},
	url = {http://arxiv.org/abs/2405.04136},
	abstract = {With the rapid expansion of academic literature and the proliferation of preprints, researchers face growing challenges in manually organizing and labeling large volumes of articles. The NSLP 2024 FoRC Shared Task I addresses this challenge organized as a competition. The goal is to develop a classifier capable of predicting one of 123 predefined classes from the Open Research Knowledge Graph (ORKG) taxonomy of research fields for a given article.This paper presents our results. Initially, we enrich the dataset (containing English scholarly articles sourced from ORKG and arXiv), then leverage different pre-trained language Models (PLMs), specifically BERT, and explore their efficacy in transfer learning for this downstream task. Our experiments encompass feature-based and fine-tuned transfer learning approaches using diverse PLMs, optimized for scientific tasks, including SciBERT, SciNCL, and SPECTER2. We conduct hyperparameter tuning and investigate the impact of data augmentation from bibliographic databases such as OpenAlex, Semantic Scholar, and Crossref. Our results demonstrate that fine-tuning pre-trained models substantially enhances classification performance, with SPECTER2 emerging as the most accurate model. Moreover, enriching the dataset with additional metadata improves classification outcomes significantly, especially when integrating information from S2AG, OpenAlex and Crossref. Our best-performing approach achieves a weighted F1-score of 0.7415. Overall, our study contributes to the advancement of reliable automated systems for scholarly publication categorization, offering a potential solution to the laborious manual curation process, thereby facilitating researchers in efficiently locating relevant resources.},
	urldate = {2024-08-06},
	publisher = {arXiv},
	author = {Wolff, Benjamin and Seidlmayer, Eva and Förstner, Konrad U.},
	month = may,
	year = {2024},
	doi = {10.48550/arXiv.2405.04136},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@inproceedings{wang_new_2014,
	title = {A new active labeling method for deep learning},
	url = {https://ieeexplore.ieee.org/document/6889457/?arnumber=6889457},
	doi = {10.1109/IJCNN.2014.6889457},
	abstract = {Deep learning has been shown to achieve outstanding performance in a number of challenging real-world applications. However, most of the existing works assume a fixed set of labeled data, which is not necessarily true in real-world applications. Getting labeled data is usually expensive and time consuming. Active labelling in deep learning aims at achieving the best learning result with a limited labeled data set, i.e., choosing the most appropriate unlabeled data to get labeled. This paper presents a new active labeling method, AL-DL, for cost-effective selection of data to be labeled. AL-DL uses one of three metrics for data selection: least confidence, margin sampling, and entropy. The method is applied to deep learning networks based on stacked restricted Boltzmann machines, as well as stacked autoencoders. In experiments on the MNIST benchmark dataset, the method outperforms random labeling consistently by a significant margin.},
	urldate = {2024-07-31},
	booktitle = {2014 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Wang, Dan and Shang, Yi},
	month = jul,
	year = {2014},
	keywords = {Classification algorithms, Entropy, Labeling, Measurement, Neural networks, Training, Uncertainty},
	pages = {112--119},
}

@article{argamon-engelson_committee-based_1999,
	title = {Committee-{Based} {Sample} {Selection} for {Probabilistic} {Classifiers}},
	volume = {11},
	issn = {1076-9757},
	url = {http://arxiv.org/abs/1106.0220},
	doi = {10.1613/jair.612},
	abstract = {In many real-world learning tasks, it is expensive to acquire a sufficient number of labeled examples for training. This paper investigates methods for reducing annotation cost by `sample selection'. In this approach, during training the learning program examines many unlabeled examples and selects for labeling only those that are most informative at each stage. This avoids redundantly labeling examples that contribute little new information. Our work follows on previous research on Query By Committee, extending the committee-based paradigm to the context of probabilistic classification. We describe a family of empirical methods for committee-based sample selection in probabilistic classification models, which evaluate the informativeness of an example by measuring the degree of disagreement between several model variants. These variants (the committee) are drawn randomly from a probability distribution conditioned by the training set labeled so far. The method was applied to the real-world natural language processing task of stochastic part-of-speech tagging. We find that all variants of the method achieve a significant reduction in annotation cost, although their computational efficiency differs. In particular, the simplest variant, a two member committee with no parameters to tune, gives excellent results. We also show that sample selection yields a significant reduction in the size of the model used by the tagger.},
	urldate = {2024-07-31},
	journal = {Journal of Artificial Intelligence Research},
	author = {Argamon-Engelson, S. and Dagan, I.},
	month = nov,
	year = {1999},
	keywords = {Computer Science - Artificial Intelligence},
	pages = {335--360},
}

@article{ferdinands_performance_2023,
	title = {Performance of active learning models for screening prioritization in systematic reviews: a simulation study into the {Average} {Time} to {Discover} relevant records},
	volume = {12},
	shorttitle = {Performance of active learning models for screening prioritization in systematic reviews},
	doi = {10.1186/s13643-023-02257-7},
	abstract = {Background Conducting a systematic review demands a significant amount of effort in screening titles and abstracts. To accelerate this process, various tools that utilize active learning have been proposed. These tools allow the reviewer to interact with machine learning software to identify relevant publications as early as possible. The goal of this study is to gain a comprehensive understanding of active learning models for reducing the workload in systematic reviews through a simulation study. Methods The simulation study mimics the process of a human reviewer screening records while interacting with an active learning model. Different active learning models were compared based on four classification techniques (naive Bayes, logistic regression, support vector machines, and random forest) and two feature extraction strategies (TF-IDF and doc2vec). The performance of the models was compared for six systematic review datasets from different research areas. The evaluation of the models was based on the Work Saved over Sampling (WSS) and recall. Additionally, this study introduces two new statistics, Time to Discovery (TD) and Average Time to Discovery (ATD). Results The models reduce the number of publications needed to screen by 91.7 to 63.9\% while still finding 95\% of all relevant records (WSS@95). Recall of the models was defined as the proportion of relevant records found after screening 10\% of of all records and ranges from 53.6 to 99.8\%. The ATD values range from 1.4\% till 11.7\%, which indicate the average proportion of labeling decisions the researcher needs to make to detect a relevant record. The ATD values display a similar ranking across the simulations as the recall and WSS values. Conclusions Active learning models for screening prioritization demonstrate significant potential for reducing the workload in systematic reviews. The Naive Bayes + TF-IDF model yielded the best results overall. The Average Time to Discovery (ATD) measures performance of active learning models throughout the entire screening process without the need for an arbitrary cut-off point. This makes the ATD a promising metric for comparing the performance of different models across different datasets.},
	journal = {Systematic Reviews},
	author = {Ferdinands, Gerbrich and Schram, Raoul and de Bruin, Jonathan and Bagheri, Ayoub and Oberski, Daniel and Tummers, Lars and Teijema, Jelle and Schoot, Rens},
	month = jun,
	year = {2023},
}

@article{angluin_queries_1988,
	title = {Queries and {Concept} {Learning}},
	volume = {2},
	issn = {1573-0565},
	url = {https://doi.org/10.1023/A:1022821128753},
	doi = {10.1023/A:1022821128753},
	abstract = {We consider the problem of using queries to learn an unknown concept. Several types of queries are described and studied: membership, equivalence, subset, superset, disjointness, and exhaustiveness queries. Examples are given of efficient learning methods using various subsets of these queries for formal domains, including the regular languages, restricted classes of context-free languages, the pattern languages, and restricted types of prepositional formulas. Some general lower bound techniques are given. Equivalence queries are compared with Valiant's criterion of probably approximately correct identification under random sampling.},
	language = {en},
	number = {4},
	urldate = {2024-07-31},
	journal = {Machine Learning},
	author = {Angluin, Dana},
	month = apr,
	year = {1988},
	keywords = {Concept learning, queries, supervised learning},
	pages = {319--342},
}

@misc{xu_forget_2020,
	title = {Forget {Me} {Not}: {Reducing} {Catastrophic} {Forgetting} for {Domain} {Adaptation} in {Reading} {Comprehension}},
	shorttitle = {Forget {Me} {Not}},
	url = {http://arxiv.org/abs/1911.00202},
	abstract = {The creation of large-scale open domain reading comprehension data sets in recent years has enabled the development of end-to-end neural comprehension models with promising results. To use these models for domains with limited training data, one of the most effective approach is to first pretrain them on large out-of-domain source data and then fine-tune them with the limited target data. The caveat of this is that after fine-tuning the comprehension models tend to perform poorly in the source domain, a phenomenon known as catastrophic forgetting. In this paper, we explore methods that overcome catastrophic forgetting during fine-tuning without assuming access to data from the source domain. We introduce new auxiliary penalty terms and observe the best performance when a combination of auxiliary penalty terms is used to regularise the fine-tuning process for adapting comprehension models. To test our methods, we develop and release 6 narrow domain data sets that could potentially be used as reading comprehension benchmarks.},
	urldate = {2024-08-04},
	publisher = {arXiv},
	author = {Xu, Y. and Zhong, X. and Yepes, A. J. J. and Lau, J. H.},
	month = nov,
	year = {2020},
	doi = {10.48550/arXiv.1911.00202},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{cormack_evaluation_2014,
	address = {Gold Coast Queensland Australia},
	title = {Evaluation of machine-learning protocols for technology-assisted review in electronic discovery},
	isbn = {978-1-4503-2257-7},
	url = {https://dl.acm.org/doi/10.1145/2600428.2609601},
	doi = {10.1145/2600428.2609601},
	language = {en},
	urldate = {2024-07-31},
	booktitle = {Proceedings of the 37th international {ACM} {SIGIR} conference on {Research} \& development in information retrieval},
	publisher = {ACM},
	author = {Cormack, Gordon V. and Grossman, Maura R.},
	month = jul,
	year = {2014},
	pages = {153--162},
}

@article{grossman_technology-assisted_2010,
	title = {Technology-{Assisted} {Review} in {E}-{Discovery} {Can} {Be} {More} {Effective} and {More} {Efficient} than {Exhaustive} {Manual} {Review} {Annual} {Survey}},
	volume = {17},
	url = {https://heinonline.org/HOL/P?h=hein.journals/jolt17&i=471},
	language = {eng},
	number = {3},
	urldate = {2024-07-31},
	journal = {Richmond Journal of Law and Technology},
	author = {Grossman, Maura R. and Cormack, Gordon V.},
	year = {2010},
	pages = {[i]--48},
}

@techreport{settles_active_2009,
	type = {Technical {Report}},
	title = {Active {Learning} {Literature} {Survey}},
	url = {https://minds.wisconsin.edu/handle/1793/60660},
	abstract = {The key idea behind active learning is that a machine learning algorithm can achieve greater accuracy with fewer labeled training instances if it is allowed to choose the training data from which is learns. An active learner may ask queries in the form of unlabeled instances to be labeled by an oracle (e.g., a human annotator). Active learning is well-motivated in many modern machine learning problems, where unlabeled data may be abundant but labels are difficult, time-consuming, or expensive to obtain. This report provides a general introduction to active learning and a survey of the literature. This includes a discussion of the scenarios in which queries can be formulated, and an overview of the query strategy frameworks proposed in the literature to date. An analysis of the empirical and theoretical evidence for active learning, a summary of several problem setting variants, and a discussion of related topics in machine learning research are also presented.},
	language = {en},
	urldate = {2024-07-31},
	institution = {University of Wisconsin-Madison Department of Computer Sciences},
	author = {Settles, Burr},
	year = {2009},
}

@misc{ostendorff_enriching_2019,
	title = {Enriching {BERT} with {Knowledge} {Graph} {Embeddings} for {Document} {Classification}},
	url = {http://arxiv.org/abs/1909.08402},
	abstract = {In this paper, we focus on the classification of books using short descriptive texts (cover blurbs) and additional metadata. Building upon BERT, a deep neural language model, we demonstrate how to combine text representations with metadata and knowledge graph embeddings, which encode author information. Compared to the standard BERT approach we achieve considerably better results for the classification task. For a more coarse-grained classification using eight labels we achieve an F1- score of 87.20, while a detailed classification using 343 labels yields an F1-score of 64.70. We make the source code and trained models of our experiments publicly available},
	urldate = {2024-08-06},
	publisher = {arXiv},
	author = {Ostendorff, Malte and Bourgonje, Peter and Berger, Maria and Moreno-Schneider, Julian and Rehm, Georg and Gipp, Bela},
	month = sep,
	year = {2019},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval, Computer Science - Machine Learning},
}

@article{cohen_reducing_2006,
	title = {Reducing {Workload} in {Systematic} {Review} {Preparation} {Using} {Automated} {Citation} {Classification}},
	volume = {13},
	issn = {1067-5027},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1447545/},
	doi = {10.1197/jamia.M1929},
	abstract = {Objective: To determine whether automated classification of document citations can be useful in reducing the time spent by experts reviewing journal articles for inclusion in updating systematic reviews of drug class efficacy for treatment of disease., Design: A test collection was built using the annotated reference files from 15 systematic drug class reviews. A voting perceptron-based automated citation classification system was constructed to classify each article as containing high-quality, drug class–specific evidence or not. Cross-validation experiments were performed to evaluate performance., Measurements: Precision, recall, and F-measure were evaluated at a range of sample weightings. Work saved over sampling at 95\% recall was used as the measure of value to the review process., Results: A reduction in the number of articles needing manual review was found for 11 of the 15 drug review topics studied. For three of the topics, the reduction was 50\% or greater., Conclusion: Automated document citation classification could be a useful tool in maintaining systematic reviews of the efficacy of drug therapy. Further work is needed to refine the classification system and determine the best manner to integrate the system into the production of systematic reviews.},
	number = {2},
	urldate = {2024-08-01},
	journal = {Journal of the American Medical Informatics Association : JAMIA},
	author = {Cohen, A.M. and Hersh, W.R. and Peterson, K. and Yen, Po-Yin},
	year = {2006},
	pmid = {16357352},
	note = {tex.pmcid: PMC1447545},
	pages = {206--219},
}

@article{akers2009systematic,
	title = {Systematic reviews: {CRD}’s guidance for undertaking reviews in health care},
	journal = {University of York},
	author = {Akers, Jo and Aguiar-Ibáñez, R and Baba-Akbari, A},
	year = {2009},
}

@article{tsafnat_systematic_2014,
	title = {Systematic review automation technologies},
	volume = {3},
	issn = {2046-4053},
	url = {https://doi.org/10.1186/2046-4053-3-74},
	doi = {10.1186/2046-4053-3-74},
	abstract = {Systematic reviews, a cornerstone of evidence-based medicine, are not produced quickly enough to support clinical practice. The cost of production, availability of the requisite expertise and timeliness are often quoted as major contributors for the delay. This detailed survey of the state of the art of information systems designed to support or automate individual tasks in the systematic review, and in particular systematic reviews of randomized controlled clinical trials, reveals trends that see the convergence of several parallel research projects.},
	number = {1},
	urldate = {2024-08-12},
	journal = {Systematic Reviews},
	author = {Tsafnat, Guy and Glasziou, Paul and Choong, Miew Keen and Dunn, Adam and Galgani, Filippo and Coiera, Enrico},
	month = jul,
	year = {2014},
	keywords = {Information extraction, Information retrieval, Process automation, Systematic reviews},
	pages = {74},
}

@article{lewis_rcv1_2004,
	title = {{RCV1}: {A} {New} {Benchmark} {Collection} for {Text} {Categorization} {Research}},
	volume = {5},
	issn = {ISSN 1533-7928},
	shorttitle = {{RCV1}},
	url = {https://www.jmlr.org/papers/v5/lewis04a.html},
	abstract = {Reuters Corpus Volume I (RCV1) is an archive of over 800,000 manually categorized newswire stories recently made available by Reuters, Ltd. for research purposes. Use of this data for research on text categorization requires a detailed understanding of the real world constraints under which the data was produced. Drawing on interviews with Reuters personnel and access to Reuters documentation, we describe the coding policy and quality control procedures used in producing the RCV1 data, the intended semantics of the hierarchical category taxonomies, and the corrections necessary to remove errorful data. We refer to the original data as RCV1-v1, and the corrected data as RCV1-v2. We benchmark several widely used supervised learning methods on RCV1-v2, illustrating the collection's properties, suggesting new directions for research, and providing baseline results for future studies. We make available detailed, per-category experimental results, as well as corrected versions of the category assignments and taxonomy structures, via online appendices.},
	number = {Apr},
	urldate = {2024-07-31},
	journal = {Journal of Machine Learning Research},
	author = {Lewis, David D. and Yang, Yiming and Rose, Tony G. and Li, Fan},
	year = {2004},
	pages = {361--397},
}

@inproceedings{roegiest_trec_2015,
	series = {{NIST} {Special} {Publication}},
	title = {{TREC} 2015 {Total} {Recall} {Track} {Overview}},
	volume = {500-319},
	url = {https://trec.nist.gov/pubs/trec24/papers/Overview-TR.pdf},
	urldate = {2024-07-31},
	booktitle = {Proceedings of {The} {Twenty}-{Fourth} {Text} {REtrieval} {Conference}, {TREC} 2015, {Gaithersburg}, {Maryland}, {USA}, {November} 17-20, 2015},
	publisher = {National Institute of Standards and Technology (NIST)},
	author = {Roegiest, Adam and Cormack, Gordon V. and Clarke, Charles L. A. and Grossman, Maura R.},
	editor = {Voorhees, Ellen M. and Ellis, Angela},
	year = {2015},
}

@inproceedings{grossman_trec_2016,
	title = {{TREC} 2016 {Total} {Recall} {Track} {Overview}},
	url = {https://www.semanticscholar.org/paper/TREC-2016-Total-Recall-Track-Overview-Grossman-Cormack/126240dedd75626fd736f0485d06f1f516517e54},
	abstract = {The primary purpose of the Total Recall Track is to evaluate, through controlled simulation, methods designed to achieve very high recall – as close as practicable to 100\% – with a human assessor in the loop. Motivating applications include, among others, electronic discovery in legal proceedings [3], systematic review in evidencebased medicine [6], and the creation of fully labeled test collections for information retrieval (“IR”) evaluation [5]. A secondary, but no less important, purpose is to develop a sandboxed virtual test environment within which IR systems may be tested, while preventing the disclosure of sensitive test data to participants. At the same time, the test environment also operates as a “black box,” affording participants confidence that their proprietary systems cannot easily be reverse engineered. The task to be solved in the Total Recall Track is the following:},
	urldate = {2024-07-31},
	author = {Grossman, Maura R. and Cormack, G. and Roegiest, Adam},
	year = {2016},
}

@misc{noauthor_bmc_nodate,
	title = {{BMC} {Medical} {Research} {Methodology}},
	url = {https://bmcmedresmethodol.biomedcentral.com/submission-guidelines/preparing-your-manuscript/research-article},
	abstract = {Publish your healthcare research with BMC Medical Research Methodology, with 3.9 Impact Factor and 18 days to first decision.



Focusing on manuscripts ...},
	language = {en},
	urldate = {2024-11-13},
	journal = {BioMed Central},
}

@misc{wang_zero-shot_2024,
	title = {Zero-shot {Generative} {Large} {Language} {Models} for {Systematic} {Review} {Screening} {Automation}},
	url = {http://arxiv.org/abs/2401.06320},
	doi = {10.48550/arXiv.2401.06320},
	abstract = {Systematic reviews are crucial for evidence-based medicine as they comprehensively analyse published research findings on specific questions. Conducting such reviews is often resource- and time-intensive, especially in the screening phase, where abstracts of publications are assessed for inclusion in a review. This study investigates the effectiveness of using zero-shot large language models{\textasciitilde}(LLMs) for automatic screening. We evaluate the effectiveness of eight different LLMs and investigate a calibration technique that uses a predefined recall threshold to determine whether a publication should be included in a systematic review. Our comprehensive evaluation using five standard test collections shows that instruction fine-tuning plays an important role in screening, that calibration renders LLMs practical for achieving a targeted recall, and that combining both with an ensemble of zero-shot models saves significant screening time compared to state-of-the-art approaches.},
	urldate = {2024-11-10},
	publisher = {arXiv},
	author = {Wang, Shuai and Scells, Harrisen and Zhuang, Shengyao and Potthast, Martin and Koopman, Bevan and Zuccon, Guido},
	month = feb,
	year = {2024},
	note = {arXiv:2401.06320},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
}

@article{rozanec_active_2024,
	title = {Active learning and novel model calibration measurements for automated visual inspection in manufacturing},
	volume = {35},
	issn = {1572-8145},
	url = {https://doi.org/10.1007/s10845-023-02098-0},
	doi = {10.1007/s10845-023-02098-0},
	abstract = {Quality control is a crucial activity performed by manufacturing enterprises to ensure that their products meet quality standards and avoid potential damage to the brand’s reputation. The decreased cost of sensors and connectivity enabled increasing digitalization of manufacturing. In addition, artificial intelligence enables higher degrees of automation, reducing overall costs and time required for defect inspection. This research compares three active learning approaches, having single and multiple oracles, to visual inspection. Six new metrics are proposed to assess the quality of calibration without the need for ground truth. Furthermore, this research explores whether existing calibrators can improve performance by leveraging an approximate ground truth to enlarge the calibration set. The experiments were performed on real-world data provided by Philips Consumer Lifestyle BV. Our results show that the explored active learning settings can reduce the data labeling effort by between three and four percent without detriment to the overall quality goals, considering a threshold of p = 0.95. Furthermore, the results show that the proposed calibration metrics successfully capture relevant information otherwise available to metrics used up to date only through ground truth data. Therefore, the proposed metrics can be used to estimate the quality of models’ probability calibration without committing to a labeling effort to obtain ground truth data.},
	language = {en},
	number = {5},
	urldate = {2024-11-10},
	journal = {Journal of Intelligent Manufacturing},
	author = {Rožanec, Jože M. and Bizjak, Luka and Trajkova, Elena and Zajec, Patrik and Keizer, Jelle and Fortuna, Blaž and Mladenić, Dunja},
	month = jun,
	year = {2024},
	keywords = {Active learning, Artificial intelligence, Automated visual inspection, Machine learning, Probability calibration, Smart manufacturing},
	pages = {1963--1984},
}

@misc{cormack_autonomy_2015,
	title = {Autonomy and {Reliability} of {Continuous} {Active} {Learning} for {Technology}-{Assisted} {Review}},
	url = {http://arxiv.org/abs/1504.06868},
	doi = {10.48550/arXiv.1504.06868},
	abstract = {We enhance the autonomy of the continuous active learning method shown by Cormack and Grossman (SIGIR 2014) to be effective for technology-assisted review, in which documents from a collection are retrieved and reviewed, using relevance feedback, until substantially all of the relevant documents have been reviewed. Autonomy is enhanced through the elimination of topic-specific and dataset-specific tuning parameters, so that the sole input required by the user is, at the outset, a short query, topic description, or single relevant document; and, throughout the review, ongoing relevance assessments of the retrieved documents. We show that our enhancements consistently yield superior results to Cormack and Grossman's version of continuous active learning, and other methods, not only on average, but on the vast majority of topics from four separate sets of tasks: the legal datasets examined by Cormack and Grossman, the Reuters RCV1-v2 subject categories, the TREC 6 AdHoc task, and the construction of the TREC 2002 filtering test collection.},
	urldate = {2024-11-10},
	publisher = {arXiv},
	author = {Cormack, Gordon V. and Grossman, Maura R.},
	month = apr,
	year = {2015},
	note = {arXiv:1504.06868},
	keywords = {Computer Science - Information Retrieval, Computer Science - Machine Learning},
}

@misc{sadri_continuous_2022,
	title = {Continuous {Active} {Learning} {Using} {Pretrained} {Transformers}},
	url = {http://arxiv.org/abs/2208.06955},
	doi = {10.48550/arXiv.2208.06955},
	abstract = {Pre-trained and fine-tuned transformer models like BERT and T5 have improved the state of the art in ad-hoc retrieval and question-answering, but not as yet in high-recall information retrieval, where the objective is to retrieve substantially all relevant documents. We investigate whether the use of transformer-based models for reranking and/or featurization can improve the Baseline Model Implementation of the TREC Total Recall Track, which represents the current state of the art for high-recall information retrieval. We also introduce CALBERT, a model that can be used to continuously fine-tune a BERT-based model based on relevance feedback.},
	urldate = {2024-11-10},
	publisher = {arXiv},
	author = {Sadri, Nima and Cormack, Gordon V.},
	month = aug,
	year = {2022},
	note = {arXiv:2208.06955},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Information Retrieval, Computer Science - Machine Learning},
}

@article{sparck_jones_statistical_1972,
	title = {A {STATISTICAL} {INTERPRETATION} {OF} {TERM} {SPECIFICITY} {AND} {ITS} {APPLICATION} {IN} {RETRIEVAL}},
	volume = {28},
	issn = {0022-0418},
	url = {https://doi.org/10.1108/eb026526},
	doi = {10.1108/eb026526},
	abstract = {The exhaustivity of document descriptions and the specificity of index terms are usually regarded as independent. It is suggested that specificity should be interpreted statistically, as a function of term use rather than of term meaning. The effects on retrieval of variations in term specificity are examined, experiments with three test collections showing in particular that frequently‐occurring terms are required for good overall performance. It is argued that terms should be weighted according to collection frequency, so that matches on less frequent, more specific, terms are of greater value than matches on frequent terms. Results for the test collections show that considerable improvements in performance are obtained with this very simple procedure.},
	number = {1},
	urldate = {2024-11-10},
	journal = {Journal of Documentation},
	author = {SPARCK JONES, KAREN},
	month = jan,
	year = {1972},
	note = {Publisher: MCB UP Ltd},
	pages = {11--21},
}

@article{ostendorff_enriching_nodate,
	title = {Enriching {BERT} with {Knowledge} {Graph} {Embeddings} for {Document} {Classiﬁcation}},
	abstract = {In this paper, we focus on the classiﬁcation of books using short descriptive texts (cover blurbs) and additional metadata. Building upon BERT, a deep neural language model, we demonstrate how to combine text representations with metadata and knowledge graph embeddings, which encode author information. Compared to the standard BERT approach we achieve considerably better results for the classiﬁcation task. For a more coarse-grained classiﬁcation using eight labels we achieve an F1score of 87.20, while a detailed classiﬁcation using 343 labels yields an F1-score of 64.70. We make the source code and trained models of our experiments publicly available.},
	language = {en},
	author = {Ostendorff, Malte and Bourgonje, Peter and Berger, Maria and Moreno-Schneider, Julian and Rehm, Georg and Gipp, Bela},
}

@misc{vaswani_attention_2023,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2024-08-04},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = aug,
	year = {2023},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{nunzio_special_2023,
	title = {Special issue on technology assisted review systems},
	volume = {20},
	issn = {2667-3053},
	url = {https://www.sciencedirect.com/science/article/pii/S2667305323000856},
	doi = {10.1016/j.iswa.2023.200260},
	abstract = {When faced with an information need, an exhaustive search aim to retrieve all the relevant information. The need for comprehensive research is fundamental electronic discovery, systematic reviews, investigation, research in general, and also in the creation of evaluation collections in information retrieval. To support these high-recall retrieval tasks, technology relies on machine learning to develop systems that rank and classify documents with greater efficiency than traditional Boolean keyword searches. Technology-Assisted Review (TAR) is one of the key technologies which can help to improve this process. The aim of this special issue is to investigate the most recent approaches of TAR systems applied in a wide variety of contexts where high recall is necessary: from the classification of emails to the compilation of systematic reviews up to the evaluation of the quality of the assessments.},
	urldate = {2024-07-31},
	journal = {Intelligent Systems with Applications},
	author = {Nunzio, Giorgio Maria Di and Kanoulas, Evangelos},
	month = nov,
	year = {2023},
	pages = {200260},
}

@article{guyatt_grade_2008,
	title = {{GRADE}: an emerging consensus on rating quality of evidence and strength of recommendations},
	volume = {336},
	copyright = {© BMJ Publishing Group Ltd 2008},
	issn = {0959-8138, 1756-1833},
	shorttitle = {{GRADE}},
	url = {https://www.bmj.com/content/336/7650/924},
	doi = {10.1136/bmj.39489.470347.AD},
	abstract = {{\textless}p{\textgreater}Guidelines are inconsistent in how they rate the quality of evidence and the strength of recommendations. This article explores the advantages of the GRADE system, which is increasingly being adopted by organisations worldwide {\textless}/p{\textgreater}},
	language = {en},
	number = {7650},
	urldate = {2024-07-29},
	journal = {BMJ},
	author = {Guyatt, Gordon H. and Oxman, Andrew D. and Vist, Gunn E. and Kunz, Regina and Falck-Ytter, Yngve and Alonso-Coello, Pablo and Schünemann, Holger J.},
	month = apr,
	year = {2008},
	pmid = {18436948},
	note = {Publisher: British Medical Journal Publishing Group
Section: Analysis},
	pages = {924--926},
}

@misc{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	doi = {10.48550/arXiv.1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2024-07-24},
	publisher = {arXiv},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv:1810.04805 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{noauthor_google-bertbert-base-uncased_2024,
	title = {google-bert/bert-base-uncased · {Hugging} {Face}},
	url = {https://huggingface.co/google-bert/bert-base-uncased},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2024-07-24},
	month = mar,
	year = {2024},
}

@article{van_noorden_more_2023,
	title = {More than 10,000 research papers were retracted in 2023 — a new record},
	volume = {624},
	copyright = {2023 Springer Nature Limited},
	url = {https://www.nature.com/articles/d41586-023-03974-8},
	doi = {10.1038/d41586-023-03974-8},
	abstract = {The number of articles being retracted rose sharply this year. Integrity experts say that this is only the tip of the iceberg.},
	language = {en},
	number = {7992},
	urldate = {2024-07-24},
	journal = {Nature},
	author = {Van Noorden, Richard},
	month = dec,
	year = {2023},
	note = {Bandiera\_abtest: a
Cg\_type: News
Publisher: Nature Publishing Group
Subject\_term: Scientific community, Publishing},
	keywords = {Publishing, Scientific community},
	pages = {479--481},
}

@misc{fletcher_afletcher53retractionwatch_nodate,
	title = {afletcher53/{RetractionWatch}},
	url = {https://github.com/afletcher53/RetractionWatch},
	abstract = {Contribute to afletcher53/RetractionWatch development by creating an account on GitHub.},
	urldate = {2024-07-20},
	author = {Fletcher, Aaron},
}

@article{pedregosa_scikit-learn_2011,
	title = {Scikit-learn: {Machine} {Learning} in {Python}},
	volume = {12},
	issn = {1533-7928},
	shorttitle = {Scikit-learn},
	url = {http://jmlr.org/papers/v12/pedregosa11a.html},
	abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language.  Emphasis is put on ease of use, performance, documentation, and API consistency.  It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings.  Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
	number = {85},
	urldate = {2024-07-20},
	journal = {Journal of Machine Learning Research},
	author = {Pedregosa, Fabian and Varoquaux, Gaël and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, Édouard},
	year = {2011},
	pages = {2825--2830},
}

@article{gaudino_trends_2021,
	title = {Trends and {Characteristics} of {Retracted} {Articles} in the {Biomedical} {Literature}, 1971 to 2020},
	volume = {181},
	issn = {2168-6106},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8111562/},
	doi = {10.1001/jamainternmed.2021.1807},
	abstract = {This cross-sectional study describes trends and characteristics of retracted articles in the biomedical literature from 1971 to August 2020.},
	number = {8},
	urldate = {2024-07-20},
	journal = {JAMA Internal Medicine},
	author = {Gaudino, Mario and Robinson, N. Bryce and Audisio, Katia and Rahouma, Mohamed and Benedetto, Umberto and Kurlansky, Paul and Fremes, Stephen E.},
	month = aug,
	year = {2021},
	pmid = {33970185},
	pmcid = {PMC8111562},
	pages = {1118--1121},
}

@article{teijema_active_2023,
	title = {Active learning-based systematic reviewing using switching classification models: the case of the onset, maintenance, and relapse of depressive disorders},
	volume = {8},
	shorttitle = {Active learning-based systematic reviewing using switching classification models},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10227618/},
	doi = {10.3389/frma.2023.1178181},
	abstract = {This study examines the performance of active learning-aided systematic reviews using a deep learning-based model compared to traditional machine learning approaches, and explores the potential benefits of model-switching strategies.Comprising four parts, ...},
	language = {en},
	urldate = {2024-06-18},
	journal = {Frontiers in Research Metrics and Analytics},
	author = {Teijema, Jelle Jasper and Hofstee, Laura and Brouwer, Marlies and Bruin, Jonathan de and Ferdinands, Gerbrich and Boer, Jan de and Vizan, Pablo and Brand, Sofie van den and Bockting, Claudi and Schoot, Rens van de and Bagheri, Ayoub},
	year = {2023},
	pmid = {37260784},
	note = {Publisher: Frontiers Media SA},
}

@inproceedings{patil_inverted_2011,
	address = {New York, NY, USA},
	series = {{SIGIR} '11},
	title = {Inverted indexes for phrases and strings},
	isbn = {978-1-4503-0757-4},
	url = {https://doi.org/10.1145/2009916.2009992},
	doi = {10.1145/2009916.2009992},
	abstract = {Inverted indexes are the most fundamental and widely used data structures in information retrieval. For each unique word occurring in a document collection, the inverted index stores a list of the documents in which this word occurs. Compression techniques are often applied to further reduce the space requirement of these lists. However, the index has a shortcoming, in that only predefined pattern queries can be supported efficiently. In terms of string documents where word boundaries are undefined, if we have to index all the substrings of a given document, then the storage quickly becomes quadratic in the data size. Also, if we want to apply the same type of indexes for querying phrases or sequence of words, then the inverted index will end up storing redundant information. In this paper, we show the first set of inverted indexes which work naturally for strings as well as phrase searching. The central idea is to exclude document d in the inverted list of a string P if every occurrence of P in d is subsumed by another string of which P is a prefix. With this we show that our space utilization is close to the optimal. Techniques from succinct data structures are deployed to achieve compression while allowing fast access in terms of frequency and document id based retrieval. Compression and speed trade-offs are evaluated for different variants of the proposed index. For phrase searching, we show that our indexes compare favorably against a typical inverted index deploying position-wise intersections. We also show efficient top-k based retrieval under relevance metrics like frequency and tf-idf.},
	urldate = {2024-06-17},
	booktitle = {Proceedings of the 34th international {ACM} {SIGIR} conference on {Research} and development in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Patil, Manish and Thankachan, Sharma V. and Shah, Rahul and Hon, Wing-Kai and Vitter, Jeffrey Scott and Chandrasekaran, Sabrina},
	month = jul,
	year = {2011},
	keywords = {compressed data structures, inverted indexes, phrase searching, top-k queries},
	pages = {555--564},
}

@misc{noauthor_inverted_2023,
	title = {Inverted index},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Inverted_index&oldid=1170015016},
	abstract = {In computer science, an inverted index (also referred to as a postings list, postings file, or inverted file) is a database index storing a mapping from content, such as words or numbers, to its locations in a table, or in a document or a set of documents (named in contrast to a forward index, which maps from documents to content). The purpose of an inverted index is to allow fast full-text searches, at a cost of increased processing when a document is added to the database. The inverted file may be the database file itself, rather than its index. It is the most popular data structure used in document retrieval systems, used on a large scale for example in search engines. Additionally, several significant general-purpose mainframe-based database management systems have used inverted list architectures, including ADABAS, DATACOM/DB, and Model 204.
There are two main variants of inverted indexes: A record-level inverted index (or inverted file index or just inverted file) contains a list of references to documents for each word. A word-level inverted index (or full inverted index or inverted list) additionally contains the positions of each word within a document. The latter form offers more functionality (like phrase searches), but needs more processing power and space to be created.},
	language = {en},
	urldate = {2024-06-17},
	journal = {Wikipedia},
	month = aug,
	year = {2023},
	note = {Page Version ID: 1170015016},
}

@misc{noauthor_work_2024,
	title = {Work object {\textbar} {OpenAlex} technical documentation},
	url = {https://docs.openalex.org/api-entities/works/work-object},
	language = {en},
	urldate = {2024-06-17},
	month = may,
	year = {2024},
}

@misc{priem_openalex_2022,
	title = {{OpenAlex}: {A} fully-open index of scholarly works, authors, venues, institutions, and concepts},
	shorttitle = {{OpenAlex}},
	url = {http://arxiv.org/abs/2205.01833},
	doi = {10.48550/arXiv.2205.01833},
	abstract = {OpenAlex is a new, fully-open scientific knowledge graph (SKG), launched to replace the discontinued Microsoft Academic Graph (MAG). It contains metadata for 209M works (journal articles, books, etc); 2013M disambiguated authors; 124k venues (places that host works, such as journals and online repositories); 109k institutions; and 65k Wikidata concepts (linked to works via an automated hierarchical multi-tag classifier). The dataset is fully and freely available via a web-based GUI, a full data dump, and high-volume REST API. The resource is under active development and future work will improve accuracy and coverage of citation information and author/institution parsing and deduplication.},
	urldate = {2024-06-14},
	publisher = {arXiv},
	author = {Priem, Jason and Piwowar, Heather and Orr, Richard},
	month = jun,
	year = {2022},
	note = {arXiv:2205.01833 [cs]},
	keywords = {Computer Science - Digital Libraries},
}

@misc{retraction_watch_retraction_2024,
	type = {Tracking retractions as a window into the scientific process},
	title = {Retraction {Watch}},
	url = {https://retractionwatch.com/},
	abstract = {Tracking retractions as a window into the scientific process},
	language = {en-US},
	urldate = {2024-06-14},
	journal = {Retraction Watch},
	author = {Retraction Watch},
	month = jun,
	year = {2024},
}

@article{teixeira_da_silva_silent_2016,
	title = {Silent or {Stealth} {Retractions}, the {Dangerous} {Voices} of the {Unknown}, {Deleted} {Literature}},
	volume = {32},
	issn = {1936-4792},
	url = {https://doi.org/10.1007/s12109-015-9439-y},
	doi = {10.1007/s12109-015-9439-y},
	abstract = {Retractions serve as one perspective of the publishing process, and can offer vast insight into the problems associated with basic research, with the traditional publishing platform, or with policies. Some established retraction guidelines exist, such as those established by the Committee on Publication Ethics, or COPE. This essay provides a perspective of stealth or silent retractions within the broader concept of retractions, and within the framework of the COPE retraction guidelines. The issue of opaque retraction notices, especially in the case of COPE members, as well as the prominence of questionable retraction policies among select “predatory” open access publishers, is emphasized. Select clear examples are provided.},
	language = {en},
	number = {1},
	urldate = {2024-06-14},
	journal = {Publishing Research Quarterly},
	author = {Teixeira da Silva, Jaime A.},
	month = mar,
	year = {2016},
	keywords = {Accountability, COPE, Errors, Ethics, Literature correction},
	pages = {44--53},
}

@misc{retraction_watch_retraction_2024-1,
	title = {Retraction {Watch} {Database} {User} {Guide}},
	url = {https://retractionwatch.com/wp-content/uploads/2023/12/Building-The-Database.pdf},
	urldate = {2024-06-14},
	journal = {Retraction Watch Database User Guide},
	author = {Retraction Watch},
	month = jun,
	year = {2024},
}

@article{banks_thoughts_2018,
	title = {Thoughts on {Publishing} the {Research} {Article} over the {Centuries}},
	volume = {6},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2304-6775},
	url = {https://www.mdpi.com/2304-6775/6/1/10},
	doi = {10.3390/publications6010010},
	abstract = {The first academic periodical was the Journal des Sçavans, which first appeared in January 1665. It was followed two months later by the Philosophical Transactions. The Journal des Sçavans was sponsored by the state and was made up mainly of book reviews and covered all the known disciplines of the time. The Philosophical Transactions was a private venture based on Oldenburg’s correspondence and was restricted to science and technology. Scientific writers were motivated by personal reputation, the desire to improve the human condition, and, sometimes, priority. The “publish or perish” syndrome is a recent development. Among the factors that have influenced it are the increasing professionalization of science, the development of the peer-review system, and, towards the end of the twentieth century, a desire for rapid publication. The fact that English has (recently) become the lingua franca of scientific publishing creates additional difficulties for non-Anglophone scientists, which their Anglophone colleagues do not have to face. Scientific language, similar to all languages, evolves constantly. One area that seems to be changing at the moment is that of passive use, which is the subject of ongoing research. Cultural differences may also have a role to play. For example, French scientists may have to overcome a basically Cartesian education.},
	language = {en},
	number = {1},
	urldate = {2024-06-14},
	journal = {Publications},
	author = {Banks, David},
	month = mar,
	year = {2018},
	note = {Number: 1
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {\textit{Journal des Sçavans}, \textit{Philosophical Transactions}, \textit{lingua franca}, cultural difference, motivation, peer-review, publish or perish},
	pages = {10},
}

@misc{noauthor_banks_nodate,
	title = {Banks: {Thoughts} on publishing the research article... - {Google} {Scholar}},
	url = {https://scholar.google.com/scholar_lookup?journal=Publications&title=Thoughts+on+publishing+the+research+article+over+the+centuries&author=D+Banks&volume=6&publication_year=2018&pages=10&doi=10.3390/publications6010010&},
	urldate = {2024-06-14},
}

@article{steen_retractions_2011,
	title = {Retractions in the scientific literature: do authors deliberately commit research fraud?},
	volume = {37},
	issn = {1473-4257},
	shorttitle = {Retractions in the scientific literature},
	doi = {10.1136/jme.2010.038125},
	abstract = {BACKGROUND: Papers retracted for fraud (data fabrication or data falsification) may represent a deliberate effort to deceive, a motivation fundamentally different from papers retracted for error. It is hypothesised that fraudulent authors target journals with a high impact factor (IF), have other fraudulent publications, diffuse responsibility across many co-authors, delay retracting fraudulent papers and publish from countries with a weak research infrastructure.
METHODS: All 788 English language research papers retracted from the PubMed database between 2000 and 2010 were evaluated. Data pertinent to each retracted paper were abstracted from the paper and the reasons for retraction were derived from the retraction notice and dichotomised as fraud or error. Data for each retracted article were entered in an Excel spreadsheet for analysis.
RESULTS: Journal IF was higher for fraudulent papers (p{\textless}0.001). Roughly 53\% of fraudulent papers were written by a first author who had written other retracted papers ('repeat offender'), whereas only 18\% of erroneous papers were written by a repeat offender (χ=88.40; p{\textless}0.0001). Fraudulent papers had more authors (p{\textless}0.001) and were retracted more slowly than erroneous papers (p{\textless}0.005). Surprisingly, there was significantly more fraud than error among retracted papers from the USA (χ(2)=8.71; p{\textless}0.05) compared with the rest of the world.
CONCLUSIONS: This study reports evidence consistent with the 'deliberate fraud' hypothesis. The results suggest that papers retracted because of data fabrication or falsification represent a calculated effort to deceive. It is inferred that such behaviour is neither naïve, feckless nor inadvertent.},
	language = {eng},
	number = {2},
	journal = {Journal of Medical Ethics},
	author = {Steen, R. Grant},
	month = feb,
	year = {2011},
	pmid = {21081306},
	keywords = {Authorship, Biomedical Research, Editorial Policies, Fraud, Humans, Journal Impact Factor, Periodicals as Topic, Research Report, Retraction of Publication as Topic, Scientific Misconduct},
	pages = {113--117},
}

@article{stretton_publication_2012,
	title = {Publication misconduct and plagiarism retractions: a systematic, retrospective study},
	volume = {28},
	issn = {1473-4877},
	shorttitle = {Publication misconduct and plagiarism retractions},
	doi = {10.1185/03007995.2012.728131},
	abstract = {OBJECTIVES: To investigate whether plagiarism is more prevalent in publications retracted from the medical literature when first authors are affiliated with lower-income countries versus higher-income countries. Secondary objectives included investigating other factors associated with plagiarism (e.g., national language of the first author's country affiliation, publication type, journal ranking).
DESIGN: Systematic, controlled, retrospective, bibliometric study.
DATA SOURCE: Retracted publications dataset in MEDLINE (search filters: English, human, January 1966-February 2008).
DATA SELECTION: Retracted misconduct publications were classified according to the first author's country affiliation, country income level, and country national language, publication type, and ranking of the publishing journal. Standardised definitions and data collection tools were used; data were analysed (odds ratio [OR], 95\% confidence limits [CL], chi-squared tests) by an independent academic statistician.
RESULTS: Of the 213 retracted misconduct publications, 41.8\% (89/213) were retracted for plagiarism, 52.1\% (111/213) for falsification/fabrication, 2.3\% (5/213) for author disputes, 2.3\% (5/213) for ethical issues, and 1.4\% (3/213) for unknown reasons. The OR (95\% CL) of plagiarism retractions (other misconduct retractions as reference) were higher (P {\textless} 0.001) for first authors affiliated with lower-income versus higher-income countries (15.4 [4.5, 52.9]) and with non-English versus English national language countries (3.2 [1.8, 5.7]), for non-original research versus original research publications (8.4 [3.3, 21.3]), for case reports and series versus other original research types (4.2 [1.4, 13.0]), and for publications in low-ranked versus high-ranked journals (4.9 [2.4, 9.9]). Up until 2012, there were significantly (P {\textless} 0.007) fewer 'serial offenders' (first authors with {\textgreater}1 retraction) with publications retracted for plagiarism (11.5\%, 9/78) than other types of misconduct (28.9\%, 24/83).
CONCLUSIONS: This is the first study to demonstrate that publications retracted for plagiarism are significantly associated with first authors affiliated with lower-income countries. These findings have implications for developing appropriate evidence-based strategies and allocation of resources to help mitigate plagiarism misconduct.},
	language = {eng},
	number = {10},
	journal = {Current Medical Research and Opinion},
	author = {Stretton, Serina and Bramich, Narelle J. and Keys, Janelle R. and Monk, Julie A. and Ely, Julie A. and Haley, Cassandra and Woolley, Mark J. and Woolley, Karen L.},
	month = oct,
	year = {2012},
	pmid = {22978774},
	keywords = {Biomedical Research, Humans, MEDLINE, Periodicals as Topic, Plagiarism, Retraction of Publication as Topic, Scientific Misconduct},
	pages = {1575--1583},
}

@article{steen_retractions_2011-1,
	title = {Retractions in the scientific literature: is the incidence of research fraud increasing?},
	volume = {37},
	issn = {1473-4257},
	shorttitle = {Retractions in the scientific literature},
	doi = {10.1136/jme.2010.040923},
	abstract = {BACKGROUND: Scientific papers are retracted for many reasons including fraud (data fabrication or falsification) or error (plagiarism, scientific mistake, ethical problems). Growing attention to fraud in the lay press suggests that the incidence of fraud is increasing.
METHODS: The reasons for retracting 742 English language research papers retracted from the PubMed database between 2000 and 2010 were evaluated. Reasons for retraction were initially dichotomised as fraud or error and then analysed to determine specific reasons for retraction.
RESULTS: Error was more common than fraud (73.5\% of papers were retracted for error (or an undisclosed reason) vs 26.6\% retracted for fraud). Eight reasons for retraction were identified; the most common reason was scientific mistake in 234 papers (31.5\%), but 134 papers (18.1\%) were retracted for ambiguous reasons. Fabrication (including data plagiarism) was more common than text plagiarism. Total papers retracted per year have increased sharply over the decade (r=0.96; p{\textless}0.001), as have retractions specifically for fraud (r=0.89; p{\textless}0.001). Journals now reach farther back in time to retract, both for fraud (r=0.87; p{\textless}0.001) and for scientific mistakes (r=0.95; p{\textless}0.001). Journals often fail to alert the naïve reader; 31.8\% of retracted papers were not noted as retracted in any way.
CONCLUSIONS: Levels of misconduct appear to be higher than in the past. This may reflect either a real increase in the incidence of fraud or a greater effort on the part of journals to police the literature. However, research bias is rarely cited as a reason for retraction.},
	language = {eng},
	number = {4},
	journal = {Journal of Medical Ethics},
	author = {Steen, R. Grant},
	month = apr,
	year = {2011},
	pmid = {21186208},
	keywords = {Authorship, Biomedical Research, Periodicals as Topic, Plagiarism, PubMed, Retraction of Publication as Topic, Scientific Misconduct},
	pages = {249--253},
}

@article{moylan_why_2016,
	title = {Why articles are retracted: a retrospective cross-sectional study of retraction notices at {BioMed} {Central}},
	volume = {6},
	copyright = {Published by the BMJ Publishing Group Limited. For permission to use (where not already granted under a licence) please go to http://www.bmj.com/company/products-services/rights-and-licensing/. This is an Open Access article distributed in accordance with the Creative Commons Attribution Non Commercial (CC BY-NC 4.0) license, which permits others to distribute, remix, adapt, build upon this work non-commercially, and license their derivative works on different terms, provided the original work is properly cited and the use is non-commercial. See: http://creativecommons.org/licenses/by-nc/4.0/},
	issn = {2044-6055, 2044-6055},
	shorttitle = {Why articles are retracted},
	url = {https://bmjopen.bmj.com/content/6/11/e012047},
	doi = {10.1136/bmjopen-2016-012047},
	abstract = {Objectives To assess why articles are retracted from BioMed Central journals, whether retraction notices adhered to the Committee on Publication Ethics (COPE) guidelines, and are becoming more frequent as a proportion of published articles.
Design/setting Retrospective cross-sectional analysis of 134 retractions from January 2000 to December 2015.
Results 134 retraction notices were published during this timeframe. Although they account for 0.07\% of all articles published (190 514 excluding supplements, corrections, retractions and commissioned content), the rate of retraction is rising. COPE guidelines on retraction were adhered to in that an explicit reason for each retraction was given. However, some notices did not document who retracted the article (eight articles, 6\%) and others were unclear whether the underlying cause was honest error or misconduct (15 articles, 11\%). The largest proportion of notices was issued by the authors (47 articles, 35\%). The majority of retractions were due to some form of misconduct (102 articles, 76\%), that is, compromised peer review (44 articles, 33\%), plagiarism (22 articles, 16\%) and data falsification/fabrication (10 articles, 7\%). Honest error accounted for 17 retractions (13\%) of which 10 articles (7\%) were published in error. The median number of days from publication to retraction was 337.5 days.
Conclusions The most common reason to retract was compromised peer review. However, the majority of these cases date to March 2015 and appear to be the result of a systematic attempt to manipulate peer review across several publishers. Retractions due to plagiarism account for the second largest category and may be reduced by screening manuscripts before publication although this is not guaranteed. Retractions due to problems with the data may be reduced by appropriate data sharing and deposition before publication. Adopting a checklist (linked to COPE guidelines) and templates for various classes of retraction notices would increase transparency of retraction notices in future.},
	language = {en},
	number = {11},
	urldate = {2024-06-13},
	journal = {BMJ Open},
	author = {Moylan, Elizabeth C. and Kowalczuk, Maria K.},
	month = nov,
	year = {2016},
	pmid = {27881524},
	note = {Publisher: British Medical Journal Publishing Group
Section: Ethics},
	keywords = {data, misconduct, peer review, plagiarism, retraction, retraction guidelines},
	pages = {e012047},
}

@article{moylan_why_2016-1,
	title = {Why articles are retracted: a retrospective cross-sectional study of retraction notices at {BioMed} {Central}},
	volume = {6},
	issn = {2044-6055, 2044-6055},
	shorttitle = {Why articles are retracted},
	url = {https://bmjopen.bmj.com/lookup/doi/10.1136/bmjopen-2016-012047},
	doi = {10.1136/bmjopen-2016-012047},
	abstract = {Objectives: To assess why articles are retracted from BioMed Central journals, whether retraction notices adhered to the Committee on Publication Ethics (COPE) guidelines, and are becoming more frequent as a proportion of published articles. Design/setting: Retrospective cross-sectional analysis of 134 retractions from January 2000 to December 2015.
Results: 134 retraction notices were published during this timeframe. Although they account for 0.07\% of all articles published (190 514 excluding supplements, corrections, retractions and commissioned content), the rate of retraction is rising. COPE guidelines on retraction were adhered to in that an explicit reason for each retraction was given. However, some notices did not document who retracted the article (eight articles, 6\%) and others were unclear whether the underlying cause was honest error or misconduct (15 articles, 11\%). The largest proportion of notices was issued by the authors (47 articles, 35\%). The majority of retractions were due to some form of misconduct (102 articles, 76\%), that is, compromised peer review (44 articles, 33\%), plagiarism (22 articles, 16\%) and data falsification/fabrication (10 articles, 7\%). Honest error accounted for 17 retractions (13\%) of which 10 articles (7\%) were published in error. The median number of days from publication to retraction was 337.5 days.
Conclusions: The most common reason to retract was compromised peer review. However, the majority of these cases date to March 2015 and appear to be the result of a systematic attempt to manipulate peer review across several publishers. Retractions due to plagiarism account for the second largest category and may be reduced by screening manuscripts before publication although this is not guaranteed. Retractions due to problems with the data may be reduced by appropriate data sharing and deposition before publication. Adopting a checklist (linked to COPE guidelines) and templates for various classes of retraction notices would increase transparency of retraction notices in future.},
	language = {en},
	number = {11},
	urldate = {2024-06-13},
	journal = {BMJ Open},
	author = {Moylan, Elizabeth C and Kowalczuk, Maria K},
	month = nov,
	year = {2016},
	pages = {e012047},
}

@misc{krishnan_mitigating_2021,
	title = {Mitigating {Sampling} {Bias} and {Improving} {Robustness} in {Active} {Learning}},
	url = {http://arxiv.org/abs/2109.06321},
	doi = {10.48550/arXiv.2109.06321},
	abstract = {This paper presents simple and efficient methods to mitigate sampling bias in active learning while achieving state-of-the-art accuracy and model robustness. We introduce supervised contrastive active learning by leveraging the contrastive loss for active learning under a supervised setting. We propose an unbiased query strategy that selects informative data samples of diverse feature representations with our methods: supervised contrastive active learning (SCAL) and deep feature modeling (DFM). We empirically demonstrate our proposed methods reduce sampling bias, achieve state-of-the-art accuracy and model calibration in an active learning setup with the query computation 26x faster than Bayesian active learning by disagreement and 11x faster than CoreSet. The proposed SCAL method outperforms by a big margin in robustness to dataset shift and out-of-distribution.},
	urldate = {2024-06-13},
	publisher = {arXiv},
	author = {Krishnan, Ranganath and Sinha, Alok and Ahuja, Nilesh and Subedar, Mahesh and Tickoo, Omesh and Iyer, Ravi},
	month = sep,
	year = {2021},
	note = {arXiv:2109.06321 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{dasgupta_hierarchical_2008,
	address = {Helsinki, Finland},
	title = {Hierarchical sampling for active learning},
	isbn = {978-1-60558-205-4},
	url = {http://portal.acm.org/citation.cfm?doid=1390156.1390183},
	doi = {10.1145/1390156.1390183},
	abstract = {We present an active learning scheme that exploits cluster structure in data.},
	language = {en},
	urldate = {2024-06-13},
	booktitle = {Proceedings of the 25th international conference on {Machine} learning - {ICML} '08},
	publisher = {ACM Press},
	author = {Dasgupta, Sanjoy and Hsu, Daniel},
	year = {2008},
	pages = {208--215},
}

@inproceedings{molinari_active_2022,
	title = {Active {Learning} and the {Saerens}-{Latinne}-{Decaestecker} {Algorithm}: {An} {Evaluation}},
	shorttitle = {Active {Learning} and the {Saerens}-{Latinne}-{Decaestecker} {Algorithm}},
	url = {https://openreview.net/forum?id=ITu3RkBqdO},
	language = {en},
	urldate = {2024-06-13},
	author = {Molinari, Alessio and Esuli, Andrea and Sebastiani, Fabrizio},
	month = jan,
	year = {2022},
}

@misc{molinari_active_2022-1,
	title = {Active {Learning} and the {Saerens}-{Latinne}-{Decaestecker} {Algorithm}: {An} {Evaluation}},
	shorttitle = {Active {Learning} and the {Saerens}-{Latinne}-{Decaestecker} {Algorithm}},
	url = {https://zenodo.org/records/7090015},
	abstract = {The Saerens-Latinne-Decaestecker (SLD) algorithm is a method whose goal is improving the quality of the posterior probabilities (or simply “posteriors”) returned by a probabilistic classifier in scenarios characterized by prior probability shift (PPS) between the training set and the unlabelled (“test”) set. This is an important task, (a) because posteriors are of the utmost importance in downstream tasks such as, e.g., multiclass classification and cost-sensitive classification, and (b) because PPS is ubiquitous in many applications. In this paper we explore whether using SLD can indeed improve the quality of posteriors returned by a classifier trained via active learning (AL), a class of machine learning (ML) techniques that indeed tend to generate substantial PPS. Specifically, we target AL via relevance sampling (ALvRS) and AL via uncertainty sampling (ALvUS), two AL techniques that are very well-known especially because, due to their low computational cost, are suitable to being applied in scenarios characterized by large datasets. We present experimental results obtained on the RCV1-v2 dataset, showing that SLD fails to deliver better-quality posteriors with both ALvRS and ALvUS, thus contradicting previous findings in the literature, and that this is due not to the amount of PPS that these techniques generate, but to how the examples they prioritize for annotation are distributed.},
	urldate = {2024-06-13},
	author = {Molinari, Alessio and Esuli, Andrea and Sebastiani, Fabrizio},
	month = jul,
	year = {2022},
	doi = {10.5281/zenodo.7090015},
	note = {Publisher: Zenodo},
}

@article{molinari_active_nodate,
	title = {Active {Learning} {\textbackslash}{\textbackslash} and the {Saerens}-{Latinne}-{Decaestecker} {Algorithm}: {\textbackslash}{\textbackslash} {An} {Evaluation}},
	abstract = {The Saerens-Latinne-Decaestecker (SLD) algorithm is a method whose goal is improving the quality of the posterior probabilities (or simply “posteriors”) returned by a probabilistic classifier in scenarios characterized by prior probability shift (PPS) between the training set and the unlabelled (“test”) set. This is an important task, (a) because posteriors are of the utmost importance in downstream tasks such as, e.g., multiclass classification and cost-sensitive classification, and (b) because PPS is ubiquitous in many applications. In this paper we explore whether using SLD can indeed improve the quality of posteriors returned by a classifier trained via active learning (AL), a class of machine learning (ML) techniques that indeed tend to generate substantial PPS. Specifically, we target AL via relevance sampling (ALvRS) and AL via uncertainty sampling (ALvUS), two AL techniques that are very well-known especially because, due to their low computational cost, are suitable to being applied in scenarios characterized by large datasets. We present experimental results obtained on the RCV1-v2 dataset, showing that SLD fails to deliver better-quality posteriors with both ALvRS and ALvUS, thus contradicting previous findings in the literature, and that this is due not to the amount of PPS that these techniques generate, but to how the examples they prioritize for annotation are distributed.},
	language = {en},
	author = {Molinari, Alessio and Esuli, Andrea and Sebastiani, Fabrizio},
}

@article{molinari_active_nodate-1,
	title = {Active {Learning} {\textbackslash}{\textbackslash} and the {Saerens}-{Latinne}-{Decaestecker} {Algorithm}: {\textbackslash}{\textbackslash} {An} {Evaluation}},
	abstract = {The Saerens-Latinne-Decaestecker (SLD) algorithm is a method whose goal is improving the quality of the posterior probabilities (or simply “posteriors”) returned by a probabilistic classifier in scenarios characterized by prior probability shift (PPS) between the training set and the unlabelled (“test”) set. This is an important task, (a) because posteriors are of the utmost importance in downstream tasks such as, e.g., multiclass classification and cost-sensitive classification, and (b) because PPS is ubiquitous in many applications. In this paper we explore whether using SLD can indeed improve the quality of posteriors returned by a classifier trained via active learning (AL), a class of machine learning (ML) techniques that indeed tend to generate substantial PPS. Specifically, we target AL via relevance sampling (ALvRS) and AL via uncertainty sampling (ALvUS), two AL techniques that are very well-known especially because, due to their low computational cost, are suitable to being applied in scenarios characterized by large datasets. We present experimental results obtained on the RCV1-v2 dataset, showing that SLD fails to deliver better-quality posteriors with both ALvRS and ALvUS, thus contradicting previous findings in the literature, and that this is due not to the amount of PPS that these techniques generate, but to how the examples they prioritize for annotation are distributed.},
	language = {en},
	author = {Molinari, Alessio and Esuli, Andrea and Sebastiani, Fabrizio},
}

@inproceedings{yang_heuristic_2021,
	address = {New York, NY, USA},
	series = {{DocEng} '21},
	title = {Heuristic stopping rules for technology-assisted review},
	isbn = {978-1-4503-8596-1},
	url = {https://dl.acm.org/doi/10.1145/3469096.3469873},
	doi = {10.1145/3469096.3469873},
	abstract = {Technology-assisted review (TAR) refers to human-in-the-loop active learning workflows for finding relevant documents in large collections. These workflows often must meet a target for the proportion of relevant documents found (i.e. recall) while also holding down costs. A variety of heuristic stopping rules have been suggested for striking this tradeoff in particular settings, but none have been tested against a range of recall targets and tasks. We propose two new heuristic stopping rules, Quant and QuantCI based on model-based estimation techniques from survey research. We compare them against a range of proposed heuristics and find they are accurate at hitting a range of recall targets while substantially reducing review costs.},
	urldate = {2024-06-13},
	booktitle = {Proceedings of the 21st {ACM} {Symposium} on {Document} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Yang, Eugene and Lewis, David D. and Frieder, Ophir},
	month = aug,
	year = {2021},
	pages = {1--10},
}

@article{oard_jointly_2018,
	title = {Jointly {Minimizing} the {Expected} {Costs} of {Review} for {Responsiveness} and {Privilege} in {E}-{Discovery}},
	volume = {37},
	issn = {1046-8188},
	url = {https://dl.acm.org/doi/10.1145/3268928},
	doi = {10.1145/3268928},
	abstract = {Discovery is an important aspect of the civil litigation process in the United States of America, in which all parties to a lawsuit are permitted to request relevant evidence from other parties. With the rapid growth of digital content, the emerging need for “e-discovery” has created a strong demand for techniques that can be used to review massive collections both for “responsiveness” (i.e., relevance) to the request and for “privilege” (i.e., presence of legally protected content that the party performing the review may have a right to withhold). In this process, the party performing the review may incur costs of two types, namely, annotation costs (deriving from the fact that human reviewers need to be paid for their work) and misclassification costs (deriving from the fact that failing to correctly determine the responsiveness or privilege of a document may adversely affect the interests of the parties in various ways). Relying exclusively on automatic classification would minimize annotation costs but could result in substantial misclassification costs, while relying exclusively on manual classification could generate the opposite consequences. This article proposes a risk minimization framework (called MINECORE, for “{\textless}underline{\textgreater}min{\textless}/underline{\textgreater}imizing the {\textless}underline{\textgreater}e{\textless}/underline{\textgreater}xpected {\textless}underline{\textgreater}co{\textless}/underline{\textgreater}sts of {\textless}underline{\textgreater}re{\textless}/underline{\textgreater}view”) that seeks to strike an optimal balance between these two extreme stands. In MINECORE (a) the documents are first automatically classified for both responsiveness and privilege, and then (b) some of the automatically classified documents are annotated by human reviewers for responsiveness (typically by junior reviewers) and/or, in cascade, for privilege (typically by senior reviewers), with the overall goal of minimizing the expected cost (i.e., the risk) of the entire process. Risk minimization is achieved by optimizing, for both responsiveness and privilege, the choice of which documents to manually review. We present a simulation study in which classes from a standard text classification test collection (RCV1-v2) are used as surrogates for responsiveness and privilege. The results indicate that MINECORE can yield substantially lower total cost than any of a set of strong baselines.},
	number = {1},
	urldate = {2024-06-13},
	journal = {ACM Transactions on Information Systems},
	author = {Oard, Douglas W. and Sebastiani, Fabrizio and Vinjumur, Jyothi K.},
	month = nov,
	year = {2018},
	keywords = {E-discovery, semi-automated text classification, technology-assisted review, utility theory},
	pages = {11:1--11:35},
}

@article{li_when_2020,
	title = {When to {Stop} {Reviewing} in {Technology}-{Assisted} {Reviews}: {Sampling} from an {Adaptive} {Distribution} to {Estimate} {Residual} {Relevant} {Documents}},
	volume = {38},
	issn = {1046-8188},
	shorttitle = {When to {Stop} {Reviewing} in {Technology}-{Assisted} {Reviews}},
	url = {https://dl.acm.org/doi/10.1145/3411755},
	doi = {10.1145/3411755},
	abstract = {Technology-Assisted Reviews (TAR) aim to expedite document reviewing (e.g., medical articles or legal documents) by iteratively incorporating machine learning algorithms and human feedback on document relevance. Continuous Active Learning (CAL) algorithms have demonstrated superior performance compared to other methods in efficiently identifying relevant documents. One of the key challenges for CAL algorithms is deciding when to stop displaying documents to reviewers. Existing work either lacks transparency—it provides an ad-hoc stopping point, without indicating how many relevant documents are still not found, or lacks efficiency by paying an extra cost to estimate the total number of relevant documents in the collection prior to the actual review. In this article, we handle the problem of deciding the stopping point of TAR under the continuous active learning framework by jointly training a ranking model to rank documents, and by conducting a “greedy” sampling to estimate the total number of relevant documents in the collection. We prove the unbiasedness of the proposed estimators under a with-replacement sampling design, while experimental results demonstrate that the proposed approach, similar to CAL, effectively retrieves relevant documents; but it also provides a transparent, accurate, and effective stopping point.},
	number = {4},
	urldate = {2024-06-13},
	journal = {ACM Transactions on Information Systems},
	author = {Li, Dan and Kanoulas, Evangelos},
	month = sep,
	year = {2020},
	keywords = {Total recall, active sampling, unbiased estimator},
	pages = {41:1--41:36},
}

@misc{noauthor_when_nodate,
	title = {When to {Stop} {Reviewing} in {Technology}-{Assisted} {Reviews}: {Sampling} from an {Adaptive} {Distribution} to {Estimate} {Residual} {Relevant} {Documents}: {ACM} {Transactions} on {Information} {Systems}: {Vol} 38, {No} 4},
	url = {https://dl.acm.org/doi/10.1145/3411755},
	urldate = {2024-06-13},
}

@inproceedings{cormack_engineering_2016,
	address = {New York, NY, USA},
	series = {{SIGIR} '16},
	title = {Engineering {Quality} and {Reliability} in {Technology}-{Assisted} {Review}},
	isbn = {978-1-4503-4069-4},
	url = {https://dl.acm.org/doi/10.1145/2911451.2911510},
	doi = {10.1145/2911451.2911510},
	abstract = {The objective of technology-assisted review ("TAR") is to find as much relevant information as possible with reasonable effort. Quality is a measure of the extent to which a TAR method achieves this objective, while reliability is a measure of how consistently it achieves an acceptable result. We are concerned with how to define, measure, and achieve high quality and high reliability in TAR. When quality is defined using the traditional goal-post method of specifying a minimum acceptable recall threshold, the quality and reliability of a TAR method are both, by definition, equal to the probability of achieving the threshold. Assuming this definition of quality and reliability, we show how to augment any TAR method to achieve guaranteed reliability, for a quantifiable level of additional review effort. We demonstrate this result by augmenting the TAR method supplied as the baseline model implementation for the TREC 2015 Total Recall Track, measuring reliability and effort for 555 topics from eight test collections. While our empirical results corroborate our claim of guaranteed reliability, we observe that the augmentation strategy may entail disproportionate effort, especially when the number of relevant documents is low. To address this limitation, we propose stopping criteria for the model implementation that may be applied with no additional review effort, while achieving empirical reliability that compares favorably to the provably reliable method. We further argue that optimizing reliability according to the traditional goal-post method is inconsistent with certain subjective aspects of quality, and that optimizing a Taguchi quality loss function may be more apt.},
	urldate = {2024-06-13},
	booktitle = {Proceedings of the 39th {International} {ACM} {SIGIR} conference on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Cormack, Gordon V. and Grossman, Maura R.},
	month = jul,
	year = {2016},
	keywords = {continuous active learning, e-discovery, electronic discovery, predictive coding, quality, relevance feedback, reliability, systematic review, technology-assisted review, test collections},
	pages = {75--84},
}

@inproceedings{di_nunzio_study_2018,
	address = {Cham},
	title = {A {Study} of an {Automatic} {Stopping} {Strategy} for {Technologically} {Assisted} {Medical} {Reviews}},
	isbn = {978-3-319-76941-7},
	doi = {10.1007/978-3-319-76941-7_61},
	abstract = {Systematic medical reviews are a method to collect the findings from multiple studies in a reliable way. Given budget and time constraints, limiting the recall of a search may undermine the quality of a review to such a degree that the validity of its findings is questionable. In this paper, we investigate a variable threshold approach to tackle the problem of a total recall task in medical reviews proposed by a Cross-Language Evaluation Forum (CLEF) eHealth lab in 2017. Compared to the official results submitted to the CLEF eHealth task, our approach performed consistently better over all the range of thresholds considered achieving a recall greater than 0.95 with 25,000 documents less than the best performing systems. The runs and the source code to generate the analyses of this paper are available at the following GitHub repository (https://github.com/gmdn/ECIR2018).},
	language = {en},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer International Publishing},
	author = {Di Nunzio, Giorgio Maria},
	editor = {Pasi, Gabriella and Piwowarski, Benjamin and Azzopardi, Leif and Hanbury, Allan},
	year = {2018},
	pages = {672--677},
}

@inproceedings{cormack_scalability_2016,
	address = {Indianapolis Indiana USA},
	title = {Scalability of {Continuous} {Active} {Learning} for {Reliable} {High}-{Recall} {Text} {Classification}},
	isbn = {978-1-4503-4073-1},
	url = {https://dl.acm.org/doi/10.1145/2983323.2983776},
	doi = {10.1145/2983323.2983776},
	language = {en},
	urldate = {2024-06-06},
	booktitle = {Proceedings of the 25th {ACM} {International} on {Conference} on {Information} and {Knowledge} {Management}},
	publisher = {ACM},
	author = {Cormack, Gordon V. and Grossman, Maura R.},
	month = oct,
	year = {2016},
	pages = {1039--1048},
}

@inproceedings{cormack_engineering_2016-1,
	address = {New York, NY, USA},
	series = {{SIGIR} '16},
	title = {Engineering {Quality} and {Reliability} in {Technology}-{Assisted} {Review}},
	isbn = {978-1-4503-4069-4},
	url = {https://dl.acm.org/doi/10.1145/2911451.2911510},
	doi = {10.1145/2911451.2911510},
	abstract = {The objective of technology-assisted review ("TAR") is to find as much relevant information as possible with reasonable effort. Quality is a measure of the extent to which a TAR method achieves this objective, while reliability is a measure of how consistently it achieves an acceptable result. We are concerned with how to define, measure, and achieve high quality and high reliability in TAR. When quality is defined using the traditional goal-post method of specifying a minimum acceptable recall threshold, the quality and reliability of a TAR method are both, by definition, equal to the probability of achieving the threshold. Assuming this definition of quality and reliability, we show how to augment any TAR method to achieve guaranteed reliability, for a quantifiable level of additional review effort. We demonstrate this result by augmenting the TAR method supplied as the baseline model implementation for the TREC 2015 Total Recall Track, measuring reliability and effort for 555 topics from eight test collections. While our empirical results corroborate our claim of guaranteed reliability, we observe that the augmentation strategy may entail disproportionate effort, especially when the number of relevant documents is low. To address this limitation, we propose stopping criteria for the model implementation that may be applied with no additional review effort, while achieving empirical reliability that compares favorably to the provably reliable method. We further argue that optimizing reliability according to the traditional goal-post method is inconsistent with certain subjective aspects of quality, and that optimizing a Taguchi quality loss function may be more apt.},
	urldate = {2024-06-06},
	booktitle = {Proceedings of the 39th {International} {ACM} {SIGIR} conference on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Cormack, Gordon V. and Grossman, Maura R.},
	month = jul,
	year = {2016},
	keywords = {continuous active learning, e-discovery, electronic discovery, predictive coding, quality, relevance feedback, reliability, systematic review, technology-assisted review, test collections},
	pages = {75--84},
}

@misc{noauthor_engineering_nodate,
	title = {Engineering {Quality} and {Reliability} in {Technology}-{Assisted} {Review} {\textbar} {Proceedings} of the 39th {International} {ACM} {SIGIR} conference on {Research} and {Development} in {Information} {Retrieval}},
	url = {https://dl.acm.org/doi/10.1145/2911451.2911510},
	urldate = {2024-06-06},
}

@misc{cormack_autonomy_2015-1,
	title = {Autonomy and {Reliability} of {Continuous} {Active} {Learning} for {Technology}-{Assisted} {Review}},
	url = {http://arxiv.org/abs/1504.06868},
	abstract = {We enhance the autonomy of the continuous active learning method shown by Cormack and Grossman (SIGIR 2014) to be effective for technology-assisted review, in which documents from a collection are retrieved and reviewed, using relevance feedback, until substantially all of the relevant documents have been reviewed. Autonomy is enhanced through the elimination of topic-specific and dataset-specific tuning parameters, so that the sole input required by the user is, at the outset, a short query, topic description, or single relevant document; and, throughout the review, ongoing relevance assessments of the retrieved documents. We show that our enhancements consistently yield superior results to Cormack and Grossman's version of continuous active learning, and other methods, not only on average, but on the vast majority of topics from four separate sets of tasks: the legal datasets examined by Cormack and Grossman, the Reuters RCV1-v2 subject categories, the TREC 6 AdHoc task, and the construction of the TREC 2002 filtering test collection.},
	language = {en},
	urldate = {2024-06-06},
	publisher = {arXiv},
	author = {Cormack, Gordon V. and Grossman, Maura R.},
	month = apr,
	year = {2015},
	note = {arXiv:1504.06868 [cs]},
	keywords = {Computer Science - Information Retrieval, Computer Science - Machine Learning},
}

@inproceedings{cormack_efficient_1998,
	address = {New York, NY, USA},
	series = {{SIGIR} '98},
	title = {Efficient construction of large test collections},
	isbn = {978-1-58113-015-7},
	url = {https://dl.acm.org/doi/10.1145/290941.291009},
	doi = {10.1145/290941.291009},
	urldate = {2024-05-29},
	booktitle = {Proceedings of the 21st annual international {ACM} {SIGIR} conference on {Research} and development in information retrieval},
	publisher = {Association for Computing Machinery},
	author = {Cormack, Gordon V. and Palmer, Christopher R. and Clarke, Charles L. A.},
	month = aug,
	year = {1998},
	pages = {282--289},
}

@misc{rahman_chatgpt_2023,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {{ChatGPT} and {Academic} {Research}: {A} {Review} and {Recommendations} {Based} on {Practical} {Examples}},
	shorttitle = {{ChatGPT} and {Academic} {Research}},
	url = {https://papers.ssrn.com/abstract=4407462},
	abstract = {In the academic world, academicians, researchers, and students have already employed Large Language Models (LLMs) such as ChatGPT to complete their various academic and non-academic tasks, including essay writing, different formal and informal speech writing, summarising literature, and generating ideas. However,  yet, it is a controversial issue to use ChatGPT in academic research. Recently, its impact on academic research and publication has been scrutinized. The fundamental objective of this study is to highlight the application of ChatGPT in academic research by demonstrating a practical example with some recommendations. Data for this study was gathered using published articles, websites, blogs, and visual and numerical artefacts. We have analyzed, synthesized, and described our gathered data using an "introductory literature review." The findings revealed that for the initial idea generation for academic scientific research, ChatGPT could be an effective tool. However, in the case of literature synthesis, citations, problem statements, research gaps, and data analysis, the researchers might encounter some challenges. Therefore, in these cases, researchers must be cautious about using ChatGPT in academic research. Considering the potential applications and consequences of ChatGPT, it is a must for the academic and scientific community to establish the necessary guidelines for the appropriate use of LLMs, especially ChatGPT, in research and publishing.},
	language = {en},
	urldate = {2024-05-28},
	author = {Rahman, Md Mizanur and Terano, Harold Jan and Rahman, Md Nafizur and Salamzadeh, Aidin and Rahaman, Md Saidur},
	month = mar,
	year = {2023},
	keywords = {ChatGPT, Large Language Models, generative AI, publishing, research},
}

@article{mondal_chatgpt_2023,
	title = {{ChatGPT} in academic writing: {Maximizing} its benefits and minimizing the risks},
	volume = {71},
	issn = {0301-4738},
	shorttitle = {{ChatGPT} in academic writing},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10788737/},
	doi = {10.4103/IJO.IJO_718_23},
	abstract = {This review article explores the use of ChatGPT in academic writing and provides insights on how to utilize it judiciously. With the increasing popularity of AI-powered language models, ChatGPT has emerged as a potential tool for assisting writers in the research and writing process. We have provided a list of potential uses of ChatGPT by a novice researcher for getting help during research proposal preparation and manuscript writing. However, there are concerns regarding its reliability and potential risks associated with its use. The review highlights the importance of maintaining human judgment in the writing process and using ChatGPT as a complementary tool rather than a replacement for human effort. The article concludes with recommendations for researchers and writers to ensure responsible and effective use of ChatGPT in academic writing.},
	number = {12},
	urldate = {2024-05-28},
	journal = {Indian Journal of Ophthalmology},
	author = {Mondal, Himel and Mondal, Shaikat},
	month = dec,
	year = {2023},
	pmid = {37991290},
	pmcid = {PMC10788737},
	pages = {3600--3606},
}

@misc{noauthor_editorial_nodate,
	title = {Editorial policies - {Corrections} and {Retractions} {\textbar} {Springer} {\textbar} {Springer} — {International} {Publisher}},
	url = {https://www.springer.com/gp/editorial-policies/corrections-and-retractions},
	urldate = {2024-05-28},
}

@misc{noauthor_editorial_nodate-1,
	title = {Editorial policies - {Corrections} and {Retractions} {\textbar} {Springer} {\textbar} {Springer} — {International} {Publisher}},
	url = {https://www.springer.com/gp/editorial-policies/corrections-and-retractions},
	urldate = {2024-05-28},
}

@article{candal-pedreira_retracted_2022,
	title = {Retracted papers originating from paper mills: cross sectional study},
	volume = {379},
	copyright = {© Author(s) (or their employer(s)) 2019. Re-use permitted under CC BY-NC. No commercial re-use. See rights and permissions. Published by BMJ.. http://creativecommons.org/licenses/by-nc/4.0/This is an Open Access article distributed in accordance with the Creative Commons Attribution Non Commercial (CC BY-NC 4.0) license, which permits others to distribute, remix, adapt, build upon this work non-commercially, and license their derivative works on different terms, provided the original work is properly cited and the use is non-commercial. See: http://creativecommons.org/licenses/by-nc/4.0/.},
	issn = {1756-1833},
	shorttitle = {Retracted papers originating from paper mills},
	url = {https://www.bmj.com/content/379/bmj-2022-071517},
	doi = {10.1136/bmj-2022-071517},
	abstract = {Objectives To describe retracted papers originating from paper mills, including their characteristics, visibility, and impact over time, and the journals in which they were published.
Design Cross sectional study.
Setting The Retraction Watch database was used for identification of retracted papers from paper mills, Web of Science was used for the total number of published papers, and data from Journal Citation Reports were collected to show characteristics of journals.
Participants All paper mill papers retracted from 1 January 2004 to 26 June 2022 were included in the study. Papers bearing an expression of concern were excluded.
Main outcome measures Descriptive statistics were used to characterise the sample and analyse the trend of retracted paper mill papers over time, and to analyse their impact and visibility by reference to the number of citations received.
Results 1182 retracted paper mill papers were identified. The publication of the first paper mill paper was in 2004 and the first retraction was in 2016; by 2021, paper mill retractions accounted for 772 (21.8\%) of the 3544 total retractions. Overall, retracted paper mill papers were mostly published in journals of the second highest Journal Citation Reports quartile for impact factor (n=529 (44.8\%)) and listed four to six authors (n=602 (50.9\%)). Of the 1182 papers, almost all listed authors of 1143 (96.8\%) paper mill retractions came from Chinese institutions and 909 (76.9\%) listed a hospital as a primary affiliation. 15 journals accounted for 812 (68.7\%) of 1182 paper mill retractions, with one journal accounting for 166 (14.0\%). Nearly all (n=1083, 93.8\%) paper mill retractions had received at least one citation since publication, with a median of 11 (interquartile range 5-22) citations received.
Conclusions Papers retracted originating from paper mills are increasing in frequency, posing a problem for the research community. Retracted paper mill papers most commonly originated from China and were published in a small number of journals. Nevertheless, detected paper mill papers might be substantially different from those that are not detected. New mechanisms are needed to identify and avoid this relatively new type of misconduct.},
	language = {en},
	urldate = {2024-05-24},
	journal = {BMJ},
	author = {Candal-Pedreira, Cristina and Ross, Joseph S. and Ruano-Ravina, Alberto and Egilman, David S. and Fernández, Esteve and Pérez-Ríos, Mónica},
	month = nov,
	year = {2022},
	pmid = {36442874},
	note = {Publisher: British Medical Journal Publishing Group
Section: Research},
	pages = {e071517},
}

@misc{services_paper_2021,
	title = {Paper {Retraction}: {Meaning} and {Main} {Reasons} {\textbar} {Elsevier} {Blog}},
	shorttitle = {Paper {Retraction}},
	url = {https://scientific-publishing.webshop.elsevier.com/research-process/paper-retraction-meaning-and-main-reasons/},
	abstract = {What is a Paper Retraction and what are the main reasons for it to happen? Sometimes, with no fault of the researcher, a paper needs to be retracted. Know more.},
	language = {en-US},
	urldate = {2024-05-24},
	journal = {Elsevier Author Services - Articles},
	author = {Services, Elsevier Author},
	month = sep,
	year = {2021},
}

@misc{noauthor_high-performance_nodate,
	title = {High-performance medicine: the convergence of human and artificial intelligence - {PubMed}},
	url = {https://pubmed.ncbi.nlm.nih.gov/30617339/},
	urldate = {2024-05-04},
}

@inproceedings{mao_reproducibility_2024,
	address = {Cham},
	title = {A {Reproducibility} {Study} of {Goldilocks}: {Just}-{Right} {Tuning} of {BERT} for {TAR}},
	isbn = {978-3-031-56066-8},
	shorttitle = {A {Reproducibility} {Study} of {Goldilocks}},
	doi = {10.1007/978-3-031-56066-8_13},
	abstract = {Screening documents is a tedious and time-consuming aspect of high-recall retrieval tasks, such as compiling a systematic literature review, where the goal is to identify all relevant documents for a topic. To help streamline this process, many Technology-Assisted Review (TAR) methods leverage active learning techniques to reduce the number of documents requiring review. BERT-based models have shown high effectiveness in text classification, leading to interest in their potential use in TAR workflows. In this paper, we investigate recent work that examined the impact of further pre-training epochs on the effectiveness and efficiency of a BERT-based active learning pipeline. We first report that we could replicate the original experiments on two specific TAR datasets, confirming some of the findings: importantly, that further pre-training is critical to high effectiveness, but requires attention in terms of selecting the correct training epoch. We then investigate the generalisability of the pipeline on a different TAR task, that of medical systematic reviews. In this context, we show that there is no need for further pre-training if a domain-specific BERT backbone is used within the active learning pipeline. This finding provides practical implications for using the studied active learning pipeline within domain-specific TAR tasks.},
	language = {en},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer Nature Switzerland},
	author = {Mao, Xinyu and Koopman, Bevan and Zuccon, Guido},
	editor = {Goharian, Nazli and Tonellotto, Nicola and He, Yulan and Lipani, Aldo and McDonald, Graham and Macdonald, Craig and Ounis, Iadh},
	year = {2024},
	keywords = {Active Learning, Systematic Reviews, Technology-Assisted Review (TAR)},
	pages = {132--146},
}

@misc{wang_zero-shot_2024,
	title = {Zero-shot {Generative} {Large} {Language} {Models} for {Systematic} {Review} {Screening} {Automation}},
	url = {http://arxiv.org/abs/2401.06320},
	doi = {10.48550/arXiv.2401.06320},
	abstract = {Systematic reviews are crucial for evidence-based medicine as they comprehensively analyse published research findings on specific questions. Conducting such reviews is often resource- and time-intensive, especially in the screening phase, where abstracts of publications are assessed for inclusion in a review. This study investigates the effectiveness of using zero-shot large language models{\textasciitilde}(LLMs) for automatic screening. We evaluate the effectiveness of eight different LLMs and investigate a calibration technique that uses a predefined recall threshold to determine whether a publication should be included in a systematic review. Our comprehensive evaluation using five standard test collections shows that instruction fine-tuning plays an important role in screening, that calibration renders LLMs practical for achieving a targeted recall, and that combining both with an ensemble of zero-shot models saves significant screening time compared to state-of-the-art approaches.},
	urldate = {2024-04-25},
	publisher = {arXiv},
	author = {Wang, Shuai and Scells, Harrisen and Zhuang, Shengyao and Potthast, Martin and Koopman, Bevan and Zuccon, Guido},
	month = jan,
	year = {2024},
	note = {arXiv:2401.06320 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
}

@misc{yang_goldilocks_2022,
	title = {Goldilocks: {Just}-{Right} {Tuning} of {BERT} for {Technology}-{Assisted} {Review}},
	shorttitle = {Goldilocks},
	url = {http://arxiv.org/abs/2105.01044},
	doi = {10.48550/arXiv.2105.01044},
	abstract = {Technology-assisted review (TAR) refers to iterative active learning workflows for document review in high recall retrieval (HRR) tasks. TAR research and most commercial TAR software have applied linear models such as logistic regression to lexical features. Transformer-based models with supervised tuning are known to improve effectiveness on many text classification tasks, suggesting their use in TAR. We indeed find that the pre-trained BERT model reduces review cost by 10\% to 15\% in TAR workflows simulated on the RCV1-v2 newswire collection. In contrast, we likewise determined that linear models outperform BERT for simulated legal discovery topics on the Jeb Bush e-mail collection. This suggests the match between transformer pre-training corpora and the task domain is of greater significance than generally appreciated. Additionally, we show that just-right language model fine-tuning on the task collection before starting active learning is critical. Too little or too much fine-tuning hinders performance, worse than that of linear models, even for a favorable corpus such as RCV1-v2.},
	urldate = {2024-04-25},
	publisher = {arXiv},
	author = {Yang, Eugene and MacAvaney, Sean and Lewis, David D. and Frieder, Ophir},
	month = jan,
	year = {2022},
	note = {arXiv:2105.01044 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
}

@misc{pubmed_medlinepubmed_2023,
	title = {{MEDLINE}/{PubMed} {Baseline} {Repository} ({MBR})},
	copyright = {Courtesy of the U.S. National Library of Medicine},
	url = {https://lhncbc.nlm.nih.gov/ii/information/MBR.html},
	abstract = {The MEDLINE/PubMed Baseline Repository (MBR) provides access to each MEDLINE/PubMed Baseline snapshot starting with the 2002 MEDLINE Baseline. Each baseline contains a snapshot of MEDLINE citations in the state they were at a given moment in time without the MeSH vocabulary updates and other revisions that occur during the year. The baseline snapshot is created at the beginning of each new MeSH Indexing Year. The records included in the MEDLINE/PubMed Baseline databases represent a static view of the data at the time each baseline database was created.},
	urldate = {2024-04-25},
	journal = {MEDLINE/PubMed Baseline Repository (MBR)},
	author = {Pubmed},
	month = jan,
	year = {2023},
	note = {Courtesy of the U.S. National Library of Medicine},
}

@article{kanoulas_clef_2019,
	title = {{CLEF} 2019 technology assisted reviews in empirical medicine overview: 20th {Working} {Notes} of {CLEF} {Conference} and {Labs} of the {Evaluation} {Forum}, {CLEF} 2019},
	volume = {2380},
	issn = {1613-0073},
	shorttitle = {{CLEF} 2019 technology assisted reviews in empirical medicine overview},
	url = {http://ceur-ws.org/Vol-2380/},
	abstract = {Systematic reviews are a widely used method to provide an overview over the current scientific consensus, by bringing together multiple studies in a systematic, reliable, and transparent way. The large and growing number of published studies, and their increasing rate of publication, makes the task of identifying all relevant studies in an unbiased way both complex and time consuming to the extent that jeopardizes the validity of their findings and the ability to inform policy and practice in a timely manner. The CLEF 2019 e-Health TAR Lab accommodated two tasks. Task 1 focused on retrieving relevant studies from PubMed without the use of a Boolean query, while Task 2 focused on the efficient and effective ranking of studies during the abstract and title screening phase of conducting a systematic review. In the 2019 lab we also expanded upon the type of systematics reviews considered. Hence, beyond Diagnostic Test Accuracy reviews, we also included Intervention, Prognosis, and Qualitative systematic reviews. We constructed a benchmark collection of 31 reviews published by Cochrane, and the corresponding relevant and irrelevant articles found by the original Boolean query. Three teams participated in Task 2, submitting automatic and semi-automatic runs, using information retrieval and machine learning algorithms over a variety of text representations, in a batch and iterative manner. This paper reports both the methodology used to construct the benchmark collection, and the results of the evaluation.},
	number = {250},
	urldate = {2024-04-25},
	journal = {CEUR Workshop Proceedings},
	author = {Kanoulas, Evangelos and Li, Dan and Azzopardi, Leif and Spijker, Rene},
	month = sep,
	year = {2019},
	keywords = {Information Retrieval, TAR, active learning, evaluation, systematic reviews, text classification},
}

@misc{hui_rot_2024,
	title = {{RoT}: {Enhancing} {Large} {Language} {Models} with {Reflection} on {Search} {Trees}},
	shorttitle = {{RoT}},
	url = {http://arxiv.org/abs/2404.05449},
	abstract = {Large language models (LLMs) have demonstrated impressive capability in reasoning and planning when integrated with tree-search-based prompting methods. However, since these methods ignore the previous search experiences, they often make the same mistakes in the search process. To address this issue, we introduce Reflection on search Trees (RoT), an LLM reflection framework designed to improve the performance of tree-search-based prompting methods. It uses a strong LLM to summarize guidelines from previous tree search experiences to enhance the ability of a weak LLM. The guidelines are instructions about solving this task through tree search which can prevent the weak LLMs from making similar mistakes in the past search process. In addition, we proposed a novel state selection method, which identifies the critical information from historical search processes to help RoT generate more specific and meaningful guidelines. In our extensive experiments, we find that RoT significantly improves the performance of LLMs in reasoning or planning tasks with various tree-search-based prompting methods (e.g., BFS and MCTS). Non-tree-search-based prompting methods such as Chain-of-Thought (CoT) can also benefit from RoT guidelines since RoT can provide task-specific knowledge collected from the search experience.},
	urldate = {2024-04-25},
	publisher = {arXiv},
	author = {Hui, Wenyang and Jiang, Chengyue and Wang, Yan and Tu, Kewei},
	month = apr,
	year = {2024},
	note = {arXiv:2404.05449 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{sutskever_sequence_2014,
	title = {Sequence to {Sequence} {Learning} with {Neural} {Networks}},
	url = {http://arxiv.org/abs/1409.3215},
	doi = {10.48550/arXiv.1409.3215},
	abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
	urldate = {2024-04-15},
	publisher = {arXiv},
	author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
	month = dec,
	year = {2014},
	note = {arXiv:1409.3215 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{manakul_long-span_2021,
	address = {Online},
	title = {Long-{Span} {Summarization} via {Local} {Attention} and {Content} {Selection}},
	url = {https://aclanthology.org/2021.acl-long.470},
	doi = {10.18653/v1/2021.acl-long.470},
	abstract = {Transformer-based models have achieved state-of-the-art results in a wide range of natural language processing (NLP) tasks including document summarization. Typically these systems are trained by fine-tuning a large pre-trained model to the target task. One issue with these transformer-based models is that they do not scale well in terms of memory and compute requirements as the input length grows. Thus, for long document summarization, it can be challenging to train or fine-tune these models. In this work, we exploit large pre-trained transformer-based models and address long-span dependencies in abstractive summarization using two methods: local self-attention; and explicit content selection. These approaches are compared on a range of network configurations. Experiments are carried out on standard long-span summarization tasks, including Spotify Podcast, arXiv, and PubMed datasets. We demonstrate that by combining these methods, we can achieve state-of-the-art results on all three tasks in the ROUGE scores. Moreover, without a large-scale GPU card, our approach can achieve comparable or better results than existing approaches.},
	urldate = {2024-02-27},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Manakul, Potsawee and Gales, Mark},
	editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
	month = aug,
	year = {2021},
	pages = {6026--6041},
}

@misc{wei_chain--thought_2023,
	title = {Chain-of-{Thought} {Prompting} {Elicits} {Reasoning} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2201.11903},
	abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
	urldate = {2023-12-31},
	publisher = {arXiv},
	author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
	month = jan,
	year = {2023},
	note = {arXiv:2201.11903 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@inproceedings{byrne_taskmaster-1_2019,
	address = {Hong Kong, China},
	title = {Taskmaster-1: {Toward} a {Realistic} and {Diverse} {Dialog} {Dataset}},
	shorttitle = {Taskmaster-1},
	url = {https://aclanthology.org/D19-1459},
	doi = {10.18653/v1/D19-1459},
	abstract = {A significant barrier to progress in data-driven approaches to building dialog systems is the lack of high quality, goal-oriented conversational data. To help satisfy this elementary requirement, we introduce the initial release of the Taskmaster-1 dataset which includes 13,215 task-based dialogs comprising six domains. Two procedures were used to create this collection, each with unique advantages. The first involves a two-person, spoken “Wizard of Oz” (WOz) approach in which trained agents and crowdsourced workers interact to complete the task while the second is “self-dialog” in which crowdsourced workers write the entire dialog themselves. We do not restrict the workers to detailed scripts or to a small knowledge base and hence we observe that our dataset contains more realistic and diverse conversations in comparison to existing datasets. We offer several baseline models including state of the art neural seq2seq architectures with benchmark performance as well as qualitative human evaluations. Dialogs are labeled with API calls and arguments, a simple and cost effective approach which avoids the requirement of complex annotation schema. The layer of abstraction between the dialog model and the service provider API allows for a given model to interact with multiple services that provide similar functionally. Finally, the dataset will evoke interest in written vs. spoken language, discourse patterns, error handling and other linguistic phenomena related to dialog system research, development and design.},
	urldate = {2023-10-26},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Byrne, Bill and Krishnamoorthi, Karthik and Sankar, Chinnadhurai and Neelakantan, Arvind and Goodrich, Ben and Duckworth, Daniel and Yavuz, Semih and Dubey, Amit and Kim, Kyu-Young and Cedilnik, Andy},
	month = nov,
	year = {2019},
	pages = {4516--4525},
}

@inproceedings{viglino_end--end_2019,
	title = {End-to-{End} {Accented} {Speech} {Recognition}},
	url = {https://www.isca-speech.org/archive/interspeech_2019/viglino19_interspeech.html},
	doi = {10.21437/Interspeech.2019-2122},
	abstract = {Correct pronunciation is known to be the most difﬁcult part to acquire for (native or non-native) language learners. The accented speech is thus more variable, and standard Automatic Speech Recognition (ASR) training approaches that rely on intermediate phone alignment might introduce errors during the ASR training. With end-to-end training we could alleviate this problem. In this work, we explore the use of multi-task training and accent embedding in the context of end-to-end ASR trained with the connectionist temporal classiﬁcation loss. Comparing to the baseline developed using conventional ASR framework exploiting time-delay neural networks trained on accented English, we show signiﬁcant relative improvement of about 25\% in word error rate. Additional evaluation on unseen accent data yields relative improvements of of 31\% and 2\% for New Zealand English and Indian English, respectively.},
	language = {en},
	urldate = {2023-10-26},
	booktitle = {Interspeech 2019},
	publisher = {ISCA},
	author = {Viglino, Thibault and Motlicek, Petr and Cernak, Milos},
	month = sep,
	year = {2019},
	pages = {2140--2144},
}

@inproceedings{iseli_age-and_2006,
	title = {Age-and {Gender}-{Dependent} {Analysis} of {Voice} {Source} {Characteristics}},
	volume = {1},
	doi = {10.1109/ICASSP.2006.1660039},
	abstract = {The effects of age, gender, and vocal tract configurations on the glottal excitation signal are still only partially understood. In this paper we examine some of these effects, and show that the voice source parameters, such as fundamental frequency (Fo), open quotient (related to H*1 - H*2), and spectral tilt (related to H*1 - A*3) are not only affected by age and gender but are also intercorrelated (the asterisk superscript denotes correction for the influence of various formants). Recordings of 92 male and female speakers from three age groups (8, 15, 20-39) are analyzed. The main observations are: for low-pitched talkers H*1 - H* 2 (hence, the open quotient) is proportional to Fo, while for high-pitched talkers H*1 \$H*2 is proportional to F1 (high to low vowels) for F1 {\textless} 700 Hz. The parameter H*1 - A*3 showed a strong dependence on F2 and F3 for all talkers and age groups: increasing F2 or F3 yielded an increase in H*1 - A*3. Spectral tilt was seen to be vowel dependent and for male talkers, spectral tilt changed dramatically with age. A better understanding of the dependencies of voice source parameters on age and gender will help improve voice source parameter estimation and analysis for a variety of speech processing and medical applications},
	author = {Iseli, Markus and Shue, Yen-Liang and Alwan, Abeer},
	month = jun,
	year = {2006},
	pages = {I--I},
}

@article{zellou_age-_2021,
	title = {Age- and {Gender}-{Related} {Differences} in {Speech} {Alignment} {Toward} {Humans} and {Voice}-{AI}},
	volume = {5},
	issn = {2297-900X},
	url = {https://www.frontiersin.org/articles/10.3389/fcomm.2020.600361},
	abstract = {Speech alignment is where talkers subconsciously adopt the speech and language patterns of their interlocutor. Nowadays, people of all ages are speaking with voice-activated, artificially-intelligent (voice-AI) digital assistants through phones or smart speakers. This study examines participants’ age (older adults, 53–81 years old vs. younger adults, 18–39 years old) and gender (female and male) on degree of speech alignment during shadowing of (female and male) human and voice-AI (Apple’s Siri) productions. Degree of alignment was assessed holistically via a perceptual ratings AXB task by a separate group of listeners. Results reveal that older and younger adults display distinct patterns of alignment based on humanness and gender of the human model talkers: older adults displayed greater alignment toward the female human and device voices, while younger adults aligned to a greater extent toward the male human voice. Additionally, there were other gender-mediated differences observed, all of which interacted with model talker category (voice-AI vs. human) or shadower age category (OA vs. YA). Taken together, these results suggest a complex interplay of social dynamics in alignment, which can inform models of speech production both in human-human and human-device interaction.},
	urldate = {2023-10-26},
	journal = {Frontiers in Communication},
	author = {Zellou, Georgia and Cohn, Michelle and Ferenc Segedin, Bruno},
	year = {2021},
}

@article{howell_comparison_1991,
	title = {Comparison of prosodic properties between read and spontaneous speech material},
	volume = {10},
	issn = {0167-6393},
	url = {https://www.sciencedirect.com/science/article/pii/016763939190039V},
	doi = {10.1016/0167-6393(91)90039-V},
	abstract = {In the present study, selected prosodic properties of spontaneous material which was later read by the same, and other, speakers are compared. The major differences between spontaneous speech and read speech are (1) readers tend to make the boundaries between tone units at different points, (2) the position of the stresses differs, and (3) there are fewer pauses in read speech, and the location of these differs between readers.
Zusammenfassung
Die prosodischen Einheiten von spontaner Rede werden mit ausgewählten prosodischen Eigenschaften von gelesenen Aufzeichungen desselben Sprachmaterials verglichen. Das Gelesene wird vom ursprünglichen Sprecher und von anderen Sprechern aufgezeichnet. Die Hauptunterschiede die zwischen spontaner und gelesener Rede gefunden wurden, sind folgende: (1) die Grenzen zwischen Toneinheiten werden an unterschiedliche Stellen gesetzt, (2) die Satzakzente werden auf andere Wörter gesetzt, und (3) es werden viel weniger Pausen in gelesener Rede gemacht, und die Stellen an denen sie gemacht werden sind von Sprecher zu Sprecher verschieden.
Résumé
Cette étude concerne une comparaison des caractéristiques prosodiques de textes spontanés émanant de 6 locuteurs, et des mêmes textes lus, après leur transcription orthographique, par leur auteur et les 5 autres locuteurs. Les différences entre les textes spontanés et les textes lus sont les suivantes: (1) il y a non-correspondance entre les frontières des “tone units”, (2) la position des accents diffère, et (3) il y a moins de pauses dans la parole lue, et leurs positions varient selon les lecteurs.},
	number = {2},
	urldate = {2023-10-26},
	journal = {Speech Communication},
	author = {Howell, Peter and Kadi-Hanifi, Karima},
	month = jun,
	year = {1991},
	keywords = {Prosody, coalescences, fragmentation, pauses, spontaneous speech, stress, tone units},
	pages = {163--169},
}

@article{crystal_segmental_1982,
	title = {Segmental durations in connected speech signals: preliminary results},
	volume = {72},
	issn = {0001-4966},
	shorttitle = {Segmental durations in connected speech signals},
	doi = {10.1121/1.388251},
	abstract = {The data base, methods for a study of the durations of phonetic units in connected speech, and some preliminary results are described. From readings of two scripts by many talkers, two sets of seven talkers each were selected, based on total reading time, to form a fast group and a slow group of talkers. Using computer graphics and digital playback procedures, the recordings were segmented into breath groups and pauses, and the first four sentences in each script were segmented into phones. The hold and release (that is, plosion and/or frication) portions of stops were identified and measured; less than 50\% of the stops included releases. To establish the usefulness of the data base, the first-order statistics of the phonetic segments were determined, and a variety of durational characteristics were compared to existing reports. Analysis of number of breath groups, phonation time, and pause characterized the difference between so-called average fast and average slow talkers; however, no script-independent measure of these variables was found which would accurately predict the classification of individual talkers. The mean durations of various phonetic categories showed essentially the same percentage change when the fast and slow talkers were compared. Preliminary analyses of contextual influences on durations showed some expected changes, and also indicated that certain traditional predictions may not hold for informal connected speech. Gamma functions were fitted to the distributions of durations of various gross categories.},
	language = {eng},
	number = {3},
	journal = {The Journal of the Acoustical Society of America},
	author = {Crystal, T. H. and House, A. S.},
	month = sep,
	year = {1982},
	pmid = {7130529},
	keywords = {Computers, Female, Humans, Male, Phonation, Phonetics, Speech, Statistics as Topic, Time Factors},
	pages = {705--716},
}

@article{jacewicz_articulation_2009,
	title = {Articulation rate across dialect, age, and gender},
	volume = {21},
	issn = {0954-3945},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2790192/},
	doi = {10.1017/S0954394509990093},
	abstract = {The understanding of sociolinguistic variation is growing rapidly, but basic gaps still remain. Whether some languages or dialects are spoken faster or slower than others constitutes such a gap. Speech tempo is interconnected with social, physical and psychological markings of speech. This study examines regional variation in articulation rate and its manifestations across speaker age, gender and speaking situations (reading vs. free conversation). The results of an experimental investigation show that articulation rate differs significantly between two regional varieties of American English examined here. A group of Northern speakers (from Wisconsin) spoke significantly faster than a group of Southern speakers (from North Carolina). With regard to age and gender, young adults read faster than older adults in both regions; in free speech, only Northern young adults spoke faster than older adults. Effects of gender were smaller and less consistent; men generally spoke slightly faster than women. As the body of work on the sociophonetics of American English continues to grow in scope and depth, we argue that it is important to include fundamental phonetic information as part of our catalog of regional differences and patterns of change in American English.},
	number = {2},
	urldate = {2023-10-26},
	journal = {Language variation and change},
	author = {Jacewicz, Ewa and Fox, Robert A. and O’Neill, Caitlin and Salmons, Joseph},
	month = jul,
	year = {2009},
	pmid = {20161445},
	pmcid = {PMC2790192},
	pages = {233--256},
}

@book{balentine_its_2007,
	address = {United States of America},
	title = {Its better to be a good machine than a bad person: speech recognition and other exotic user interfaces at the twilight of the {Jetsonian} {Age}},
	isbn = {978-1-932558-09-8},
	shorttitle = {Its better to be a good machine than a bad person},
	language = {eng},
	publisher = {ICMI Press},
	author = {Balentine, Bruce and Degler, Leslie},
	year = {2007},
	note = {OCLC: 757713441},
}

@misc{moore_talking_2019,
	title = {Talking with {Robots}: {Opportunities} and {Challenges}},
	shorttitle = {Talking with {Robots}},
	url = {http://arxiv.org/abs/1912.00369},
	doi = {10.48550/arXiv.1912.00369},
	abstract = {Notwithstanding the tremendous progress that is taking place in spoken language technology, effective speech-based human-robot interaction still raises a number of important challenges. Not only do the fields of robotics and spoken language technology present their own special problems, but their combination raises an additional set of issues. In particular, there is a large gap between the formulaic speech that typifies contemporary spoken dialogue systems and the flexible nature of human-human conversation. It is pointed out that grounded and situated speech-based human-robot interaction may lead to deeper insights into the pragmatics of language usage, thereby overcoming the current `habitability gap'.},
	urldate = {2023-10-17},
	publisher = {arXiv},
	author = {Moore, Roger K.},
	month = dec,
	year = {2019},
	note = {arXiv:1912.00369 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction},
}

@book{nass_wired_2007,
	address = {Cambridge, Mass.},
	edition = {Annotated edition},
	title = {Wired for {Speech}: {How} {Voice} {Activates} and {Advances} the {Human}-{Computer} {Relationship}},
	isbn = {978-0-262-64065-7},
	shorttitle = {Wired for {Speech}},
	abstract = {How interactive voice-based technology can tap into the automatic and powerful responses all speech―whether from human or machine―evokes.Interfaces that talk and listen are populating computers, cars, call centers, and even home appliances and toys, but voice interfaces invariably frustrate rather than help. In Wired for Speech, Clifford Nass and Scott Brave reveal how interactive voice technologies can readily and effectively tap into the automatic responses all speech―whether from human or machine―evokes. Wired for Speech demonstrates that people are "voice-activated": we respond to voice technologies as we respond to actual people and behave as we would in any social situation. By leveraging this powerful finding, voice interfaces can truly emerge as the next frontier for efficient, user-friendly technology.Wired for Speech presents new theories and experiments and applies them to critical issues concerning how people interact with technology-based voices. It considers how people respond to a female voice in e-commerce (does stereotyping matter?), how a car's voice can promote safer driving (are "happy" cars better cars?), whether synthetic voices have personality and emotion (is sounding like a person always good?), whether an automated call center should apologize when it cannot understand a spoken request ("To Err is Interface; To Blame, Complex"), and much more. Nass and Brave's deep understanding of both social science and design, drawn from ten years of research at Nass's Stanford laboratory, produces results that often challenge conventional wisdom and common design practices. These insights will help designers and marketers build better interfaces, scientists construct better theories, and everyone gain better understandings of the future of the machines that speak with us.},
	language = {English},
	publisher = {MIT Press},
	author = {Nass, Clifford and Brave, Scott},
	month = feb,
	year = {2007},
}

@article{fletcher_does_2020,
	title = {Does {Non}-{Steroidal} {Anti}-inflammatories: {Does} carprofen or meloxicam have fewer gastrointestinal side effects?},
	volume = {5},
	copyright = {All rights reserved},
	issn = {2396-9776},
	shorttitle = {Does {Non}-{Steroidal} {Anti}-inflammatories},
	url = {https://veterinaryevidence.org/index.php/ve/article/view/301},
	doi = {10.18849/ve.v5i3.301},
	abstract = {PICO question 
In canines, does the oral administration of carprofen, when compared to meloxicam, result in fewer gastrointestinal side effects? 
  
Clinical bottom line 
Category of research question 
Treatment 
The number and type of study designs reviewed 
Three prospective randomised controlled trials were critically reviewed 
Strength of evidence 
Weak 
Outcomes reported 
Treatment with carprofen or meloxicam results in no significant difference in gastric lesion scoring, increased intestinal mucosal permeability or diminished small bowel absorptive capacity 
Conclusion 
There is insufficient evidence supporting preferential administration of carprofen or meloxicam to reduce gastrointestinal side effects 
  
How to apply this evidence in practice 
The application of evidence into practice should take into account multiple factors, not limited to: individual clinical expertise, patient’s circumstances and owners’ values, country, location or clinic where you work, the individual case in front of you, the availability of therapies and resources. 
Knowledge Summaries are a resource to help reinforce or inform decision-making. They do not override the responsibility or judgement of the practitioner to do what is best for the animal in their care},
	language = {en},
	number = {3},
	urldate = {2023-09-14},
	journal = {Veterinary Evidence},
	author = {Fletcher, Aaron Harold Andrew},
	month = sep,
	year = {2020},
}
