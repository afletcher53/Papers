@article{10.1162/qss_a_00155,
  author   = {Hsiao, Tzu-Kun and Schneider, Jodi},
  title    = {Continued use of retracted papers: Temporal trends in citations and (lack of) awareness of retractions shown in citation contexts in biomedicine},
  journal  = {Quantitative Science Studies},
  volume   = {2},
  number   = {4},
  pages    = {1144-1169},
  year     = {2021},
  month    = {12},
  abstract = {We present the first database-wide study on the citation contexts of retracted papers, which covers 7,813 retracted papers indexed in PubMed, 169,434 citations collected from iCite, and 48,134 citation contexts identified from the XML version of the PubMed Central Open Access Subset. Compared with previous citation studies that focused on comparing citation counts using two time frames (i.e., preretraction and postretraction), our analyses show the longitudinal trends of citations to retracted papers in the past 60 years (1960–2020). Our temporal analyses show that retracted papers continued to be cited, but that old retracted papers stopped being cited as time progressed. Analysis of the text progression of pre- and postretraction citation contexts shows that retraction did not change the way the retracted papers were cited. Furthermore, among the 13,252 postretraction citation contexts, only 722 (5.4\%) citation contexts acknowledged the retraction. In
              these 722 citation contexts, the retracted papers were most commonly cited as related work or as an example of problematic science. Our findings deepen the understanding of why retraction does not stop citation and demonstrate that the vast majority of postretraction citations in biomedicine do not document the retraction.},
  issn     = {2641-3337},
  doi      = {10.1162/qss_a_00155}
}

@article{Avenelle031909,
  author       = {Avenell, Alison and Stewart, Fiona and Grey, Andrew and Gamble, Greg and Bolland, Mark},
  title        = {An investigation into the impact and implications of published papers from retracted research: systematic search of affected literature},
  volume       = {9},
  number       = {10},
  elocation-id = {e031909},
  year         = {2019},
  doi          = {10.1136/bmjopen-2019-031909},
  publisher    = {British Medical Journal Publishing Group},
  abstract     = {Objective Analyses of the impact of a body of clinical trial reports subject to research misconduct have been few. Our objective was to examine the impact on clinically relevant research of a group of researchers{\textquoteright} trial reports ({\textquoteleft}affected trial reports{\textquoteright}) affected by research misconduct, and whether identification of misconduct invoked a reappraisal.Design In 2016, we used five databases and search engines to identify {\textquoteleft}citing publications{\textquoteright}, that is, guidelines, systematic and other reviews, and clinical trials citing any of 12 affected trial reports, published 1998{\textendash}2011, eventually retracted for research misconduct. The affected trial reports were assessed more likely to have had impact because they had hip fracture outcomes and were in journals with impact factor \&gt;4. Two authors assessed whether findings of the citing publications would change if the affected trial reports were removed. In 2018, we searched for evidence that the citing publications had undertaken a reassessment as a result of the potential influence of the affected trial reports.Results By 2016 the affected trial reports were cited in 1158 publications, including 68 systematic reviews, meta-analyses, narrative reviews, guidelines and clinical trials. We judged that 13 guidelines, systematic or other reviews would likely change their findings if the affected trial reports were removed, and in another eight it was unclear if findings would change. By 2018, only one of the 68 citing publications, a systematic review, appeared to have undertaken a reassessment, which led to a correction.Conclusions We found evidence that this group of affected trial reports distorted the evidence base. Correction of these distortions is slow, uncoordinated and inconsistent. Unless there is a rapid, systematic, coordinated approach by bibliographic databases, authors, journals and publishers to mitigate the impact of known cases of research misconduct, patients, other researchers and their funders may continue to be adversely affected.},
  issn         = {2044-6055},
  journal      = {BMJ Open}
}

@book{babbie_practice_2020,
  title     = {The {Practice} of {Social} {Research}},
  isbn      = {978-0-357-36076-7},
  abstract  = {Packed with hands-on applications, Babbie\&\#39;s THE PRACTICE OF SOCIAL RESEARCH, 15th Edition, equips your students with the tools they need to practically apply research concepts as both researchers and consumers. Known as the \&quot;gold standard\&quot; for research methods, the text delivers a comprehensive, straightforward introduction to the field of research as practiced by social scientists. Dr. Babbie emphasizes the research process by showing students how to design and construct projects, introducing the various observation modes in use today. The new edition includes \&quot;What do you think?\&quot; puzzles that immediately draw students into chapter concepts. General Social Survey data is updated throughout while new coverage includes the global use of social research, the emerging role of big data, demographic analysis and more. Also available: MindTap digital learning solution.},
  language  = {en},
  publisher = {Cengage AU},
  author    = {Babbie, Earl R.},
  month     = may,
  year      = {2020},
  note      = {Google-Books-ID: KrGeygEACAAJ},
  keywords  = {Social Science / Sociology / General}
}

@misc{bai2022constitutionalaiharmlessnessai,
  title         = {Constitutional AI: Harmlessness from AI Feedback},
  author        = {Yuntao Bai and Saurav Kadavath and Sandipan Kundu and Amanda Askell and Jackson Kernion and Andy Jones and Anna Chen and Anna Goldie and Azalia Mirhoseini and Cameron McKinnon and Carol Chen and Catherine Olsson and Christopher Olah and Danny Hernandez and Dawn Drain and Deep Ganguli and Dustin Li and Eli Tran-Johnson and Ethan Perez and Jamie Kerr and Jared Mueller and Jeffrey Ladish and Joshua Landau and Kamal Ndousse and Kamile Lukosuite and Liane Lovitt and Michael Sellitto and Nelson Elhage and Nicholas Schiefer and Noemi Mercado and Nova DasSarma and Robert Lasenby and Robin Larson and Sam Ringer and Scott Johnston and Shauna Kravec and Sheer El Showk and Stanislav Fort and Tamera Lanham and Timothy Telleen-Lawton and Tom Conerly and Tom Henighan and Tristan Hume and Samuel R. Bowman and Zac Hatfield-Dodds and Ben Mann and Dario Amodei and Nicholas Joseph and Sam McCandlish and Tom Brown and Jared Kaplan},
  year          = {2022},
  eprint        = {2212.08073},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2212.08073}
}

@book{balentine_its_2007,
  address    = {United States of America},
  title      = {Its better to be a good machine than a bad person: speech recognition and other exotic user interfaces at the twilight of the {Jetsonian} {Age}},
  isbn       = {978-1-932558-09-8},
  shorttitle = {Its better to be a good machine than a bad person},
  language   = {eng},
  publisher  = {ICMI Press},
  author     = {Balentine, Bruce and Degler, Leslie},
  year       = {2007},
  note       = {OCLC: 757713441}
}
@article{banks_thoughts_2018,
  title     = {Thoughts on {Publishing} the {Research} {Article} over the {Centuries}},
  volume    = {6},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  issn      = {2304-6775},
  url       = {https://www.mdpi.com/2304-6775/6/1/10},
  doi       = {10.3390/publications6010010},
  abstract  = {The first academic periodical was the Journal des Sçavans, which first appeared in January 1665. It was followed two months later by the Philosophical Transactions. The Journal des Sçavans was sponsored by the state and was made up mainly of book reviews and covered all the known disciplines of the time. The Philosophical Transactions was a private venture based on Oldenburg’s correspondence and was restricted to science and technology. Scientific writers were motivated by personal reputation, the desire to improve the human condition, and, sometimes, priority. The “publish or perish” syndrome is a recent development. Among the factors that have influenced it are the increasing professionalization of science, the development of the peer-review system, and, towards the end of the twentieth century, a desire for rapid publication. The fact that English has (recently) become the lingua franca of scientific publishing creates additional difficulties for non-Anglophone scientists, which their Anglophone colleagues do not have to face. Scientific language, similar to all languages, evolves constantly. One area that seems to be changing at the moment is that of passive use, which is the subject of ongoing research. Cultural differences may also have a role to play. For example, French scientists may have to overcome a basically Cartesian education.},
  language  = {en},
  number    = {1},
  journal   = {Publications},
  author    = {Banks, David},
  month     = mar,
  year      = {2018},
  keywords  = {\textit{Journal des Sçavans}, \textit{Philosophical Transactions}, \textit{lingua franca}, cultural difference, motivation, peer-review, publish or perish},
  pages     = {10}
}

@misc{berglund_reversal_2023,
  title      = {The {Reversal} {Curse}: {LLMs} trained on "{A} is {B}" fail to learn "{B} is {A}"},
  shorttitle = {The {Reversal} {Curse}},
  url        = {http://arxiv.org/abs/2309.12288},
  abstract   = {We expose a surprising failure of generalization in auto-regressive large language models (LLMs). If a model is trained on a sentence of the form "A is B", it will not automatically generalize to the reverse direction "B is A". This is the Reversal Curse. For instance, if a model is trained on "Olaf Scholz was the ninth Chancellor of Germany", it will not automatically be able to answer the question, "Who was the ninth Chancellor of Germany?". Moreover, the likelihood of the correct answer ("Olaf Scholz") will not be higher than for a random name. Thus, models exhibit a basic failure of logical deduction and do not generalize a prevalent pattern in their training set (i.e. if "A is B'' occurs, "B is A" is more likely to occur). We provide evidence for the Reversal Curse by finetuning GPT-3 and Llama-1 on fictitious statements such as "Uriah Hawthorne is the composer of 'Abyssal Melodies'" and showing that they fail to correctly answer "Who composed 'Abyssal Melodies?'". The Reversal Curse is robust across model sizes and model families and is not alleviated by data augmentation. We also evaluate ChatGPT (GPT-3.5 and GPT-4) on questions about real-world celebrities, such as "Who is Tom Cruise's mother? [A: Mary Lee Pfeiffer]" and the reverse "Who is Mary Lee Pfeiffer's son?". GPT-4 correctly answers questions like the former 79\% of the time, compared to 33\% for the latter. This shows a failure of logical deduction that we hypothesize is caused by the Reversal Curse. Code is available at https://github.com/lukasberglund/reversal\_curse.},
  urldate    = {2023-10-02},
  publisher  = {arXiv},
  author     = {Berglund, Lukas and Tong, Meg and Kaufmann, Max and Balesni, Mikita and Stickland, Asa Cooper and Korbak, Tomasz and Evans, Owain},
  month      = sep,
  year       = {2023},
  note       = {arXiv:2309.12288 [cs]},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning}
}
@misc{berrios_towards_2023,
  title      = {Towards {Language} {Models} {That} {Can} {See}: {Computer} {Vision} {Through} the {LENS} of {Natural} {Language}},
  shorttitle = {Towards {Language} {Models} {That} {Can} {See}},
  url        = {http://arxiv.org/abs/2306.16410},
  abstract   = {We propose LENS , a modular approach for tackling computer vision problems by leveraging the power of large language models (LLMs). Our system uses a language model to reason over outputs from a set of independent and highly descriptive vision modules that provide exhaustive information about an image. We evaluate the approach on pure computer vision settings such as zero- and few-shot object recognition, as well as on vision and language problems. LENS can be applied to any off-the-shelf LLM and we find that the LLMs with LENS perform highly competitively with much bigger and much more sophisticated systems, without any multimodal training whatsoever. We open-source our code at https://github.com/ContextualAI/lens and provide an interactive demo1.},
  language   = {en},
  urldate    = {2023-09-15},
  publisher  = {arXiv},
  author     = {Berrios, William and Mittal, Gautam and Thrush, Tristan and Kiela, Douwe and Singh, Amanpreet},
  month      = jun,
  year       = {2023},
  note       = {arXiv:2306.16410 [cs]},
  keywords   = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition}
}
@article{boux_cognitive_2023,
  title    = {Cognitive features of indirect speech acts},
  volume   = {38},
  issn     = {2327-3798, 2327-3801},
  url      = {https://www.tandfonline.com/doi/full/10.1080/23273798.2022.2077396},
  doi      = {10.1080/23273798.2022.2077396},
  abstract = {The oﬀer of some cake can be declined by saying “I am on a diet” – an indirect reply. Here, we asked whether certain well-established psychological and conceptual features are linked to the (in)directness of speech acts – an issue unexplored so far. Subjects rated direct and indirect speech acts performed by the same critical linguistic forms in diﬀerent dialogic contexts. We ﬁnd that indirect replies were understood with less certainty, were less predictable by, less coherent with and less semantically similar to their context question. These eﬀects were smaller when direct and indirect replies were matched for the type of speech acts for which they were used, compared to when they were not speech act matched. Crucially, all measured cognitive dimensions were strongly associated with each other. These ﬁndings suggest that indirectness goes hand-in-hand with a set of cognitive features, which should be taken into account when interpreting experimental ﬁndings, including neuroimaging studies of indirectness.},
  language = {en},
  number   = {1},
  urldate  = {2023-09-15},
  journal  = {Language, Cognition and Neuroscience},
  author   = {Boux, Isabella P. and Margiotoudi, Konstantina and Dreyer, Felix R. and Tomasello, Rosario and Pulvermüller, Friedemann},
  month    = jan,
  year     = {2023},
  pages    = {40--64}
}

@article{butterfill_how_2013,
  title      = {How to {Construct} a {Minimal} {Theory} of {Mind}: {How} to {Construct} a {Minimal} {Theory} of {Mind}},
  volume     = {28},
  issn       = {02681064},
  shorttitle = {How to {Construct} a {Minimal} {Theory} of {Mind}},
  url        = {https://onlinelibrary.wiley.com/doi/10.1111/mila.12036},
  doi        = {10.1111/mila.12036},
  abstract   = {What could someone represent that would enable her to track, at least within limits, others’ perceptions, knowledge states and beliefs including false beliefs? An obvious possibility is that she might represent these very attitudes as such. It is sometimes tacitly or explicitly assumed that this is the only possible answer. However, we argue that several recent discoveries in developmental, cognitive, and comparative psychology indicate the need for other, less obvious possibilities. Our aim is to meet this need by describing the construction of a minimal theory of mind. Minimal theory of mind is rich enough to explain systematic success on tasks held to be acid tests for theory of mind cognition including many false belief tasks. Yet minimal theory of mind does not require representing propositional attitudes, or any other kind of representation, as such. Minimal theory of mind may be what enables those with limited cognitive resources or little conceptual sophistication, such as infants, chimpanzees, scrub-jays and human adults under load, to track others’ perceptions, knowledge states and beliefs.},
  language   = {en},
  number     = {5},
  urldate    = {2023-09-15},
  journal    = {Mind \& Language},
  author     = {Butterfill, Stephen A. and Apperly, Ian A.},
  month      = nov,
  year       = {2013},
  pages      = {606--637}
}

@inproceedings{byrne_taskmaster-1_2019,
  address    = {Hong Kong, China},
  title      = {Taskmaster-1: {Toward} a {Realistic} and {Diverse} {Dialog} {Dataset}},
  shorttitle = {Taskmaster-1},
  url        = {https://aclanthology.org/D19-1459},
  doi        = {10.18653/v1/D19-1459},
  abstract   = {A significant barrier to progress in data-driven approaches to building dialog systems is the lack of high quality, goal-oriented conversational data. To help satisfy this elementary requirement, we introduce the initial release of the Taskmaster-1 dataset which includes 13,215 task-based dialogs comprising six domains. Two procedures were used to create this collection, each with unique advantages. The first involves a two-person, spoken “Wizard of Oz” (WOz) approach in which trained agents and crowdsourced workers interact to complete the task while the second is “self-dialog” in which crowdsourced workers write the entire dialog themselves. We do not restrict the workers to detailed scripts or to a small knowledge base and hence we observe that our dataset contains more realistic and diverse conversations in comparison to existing datasets. We offer several baseline models including state of the art neural seq2seq architectures with benchmark performance as well as qualitative human evaluations. Dialogs are labeled with API calls and arguments, a simple and cost effective approach which avoids the requirement of complex annotation schema. The layer of abstraction between the dialog model and the service provider API allows for a given model to interact with multiple services that provide similar functionally. Finally, the dataset will evoke interest in written vs. spoken language, discourse patterns, error handling and other linguistic phenomena related to dialog system research, development and design.},
  booktitle  = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
  publisher  = {Association for Computational Linguistics},
  author     = {Byrne, Bill and Krishnamoorthi, Karthik and Sankar, Chinnadhurai and Neelakantan, Arvind and Goodrich, Ben and Duckworth, Daniel and Yavuz, Semih and Dubey, Amit and Kim, Kyu-Young and Cedilnik, Andy},
  month      = nov,
  year       = {2019},
  pages      = {4516--4525}
}

@article{candal-pedreira_retracted_2022,
  title      = {Retracted papers originating from paper mills: cross sectional study},
  volume     = {379},
  copyright  = {© Author(s) (or their employer(s)) 2019. Re-use permitted under CC BY-NC. No commercial re-use. See rights and permissions. Published by BMJ.. http://creativecommons.org/licenses/by-nc/4.0/This is an Open Access article distributed in accordance with the Creative Commons Attribution Non Commercial (CC BY-NC 4.0) license, which permits others to distribute, remix, adapt, build upon this work non-commercially, and license their derivative works on different terms, provided the original work is properly cited and the use is non-commercial. See: http://creativecommons.org/licenses/by-nc/4.0/.},
  issn       = {1756-1833},
  shorttitle = {Retracted papers originating from paper mills},
  url        = {https://www.bmj.com/content/379/bmj-2022-071517},
  doi        = {10.1136/bmj-2022-071517},
  abstract   = {Objectives To describe retracted papers originating from paper mills, including their characteristics, visibility, and impact over time, and the journals in which they were published.
                Design Cross sectional study.
                Setting The Retraction Watch database was used for identification of retracted papers from paper mills, Web of Science was used for the total number of published papers, and data from Journal Citation Reports were collected to show characteristics of journals.
                Participants All paper mill papers retracted from 1 January 2004 to 26 June 2022 were included in the study. Papers bearing an expression of concern were excluded.
                Main outcome measures Descriptive statistics were used to characterise the sample and analyse the trend of retracted paper mill papers over time, and to analyse their impact and visibility by reference to the number of citations received.
                Results 1182 retracted paper mill papers were identified. The publication of the first paper mill paper was in 2004 and the first retraction was in 2016; by 2021, paper mill retractions accounted for 772 (21.8\%) of the 3544 total retractions. Overall, retracted paper mill papers were mostly published in journals of the second highest Journal Citation Reports quartile for impact factor (n=529 (44.8\%)) and listed four to six authors (n=602 (50.9\%)). Of the 1182 papers, almost all listed authors of 1143 (96.8\%) paper mill retractions came from Chinese institutions and 909 (76.9\%) listed a hospital as a primary affiliation. 15 journals accounted for 812 (68.7\%) of 1182 paper mill retractions, with one journal accounting for 166 (14.0\%). Nearly all (n=1083, 93.8\%) paper mill retractions had received at least one citation since publication, with a median of 11 (interquartile range 5-22) citations received.
                Conclusions Papers retracted originating from paper mills are increasing in frequency, posing a problem for the research community. Retracted paper mill papers most commonly originated from China and were published in a small number of journals. Nevertheless, detected paper mill papers might be substantially different from those that are not detected. New mechanisms are needed to identify and avoid this relatively new type of misconduct.},
  language   = {en},
  journal    = {BMJ},
  author     = {Candal-Pedreira, Cristina and Ross, Joseph S. and Ruano-Ravina, Alberto and Egilman, David S. and Fernández, Esteve and Pérez-Ríos, Mónica},
  month      = nov,
  year       = {2022},
  pmid       = {36442874},
  pages      = {e071517}
}
@article{candal-pedreira_retracted_2022-1,
  title      = {Retracted papers originating from paper mills: cross sectional study},
  volume     = {379},
  copyright  = {© Author(s) (or their employer(s)) 2019. Re-use permitted under CC BY-NC. No commercial re-use. See rights and permissions. Published by BMJ.. http://creativecommons.org/licenses/by-nc/4.0/This is an Open Access article distributed in accordance with the Creative Commons Attribution Non Commercial (CC BY-NC 4.0) license, which permits others to distribute, remix, adapt, build upon this work non-commercially, and license their derivative works on different terms, provided the original work is properly cited and the use is non-commercial. See: http://creativecommons.org/licenses/by-nc/4.0/.},
  issn       = {1756-1833},
  shorttitle = {Retracted papers originating from paper mills},
  url        = {https://www.bmj.com/content/379/bmj-2022-071517},
  doi        = {10.1136/bmj-2022-071517},
  abstract   = {Objectives To describe retracted papers originating from paper mills, including their characteristics, visibility, and impact over time, and the journals in which they were published.
                Design Cross sectional study.
                Setting The Retraction Watch database was used for identification of retracted papers from paper mills, Web of Science was used for the total number of published papers, and data from Journal Citation Reports were collected to show characteristics of journals.
                Participants All paper mill papers retracted from 1 January 2004 to 26 June 2022 were included in the study. Papers bearing an expression of concern were excluded.
                Main outcome measures Descriptive statistics were used to characterise the sample and analyse the trend of retracted paper mill papers over time, and to analyse their impact and visibility by reference to the number of citations received.
                Results 1182 retracted paper mill papers were identified. The publication of the first paper mill paper was in 2004 and the first retraction was in 2016; by 2021, paper mill retractions accounted for 772 (21.8\%) of the 3544 total retractions. Overall, retracted paper mill papers were mostly published in journals of the second highest Journal Citation Reports quartile for impact factor (n=529 (44.8\%)) and listed four to six authors (n=602 (50.9\%)). Of the 1182 papers, almost all listed authors of 1143 (96.8\%) paper mill retractions came from Chinese institutions and 909 (76.9\%) listed a hospital as a primary affiliation. 15 journals accounted for 812 (68.7\%) of 1182 paper mill retractions, with one journal accounting for 166 (14.0\%). Nearly all (n=1083, 93.8\%) paper mill retractions had received at least one citation since publication, with a median of 11 (interquartile range 5-22) citations received.
                Conclusions Papers retracted originating from paper mills are increasing in frequency, posing a problem for the research community. Retracted paper mill papers most commonly originated from China and were published in a small number of journals. Nevertheless, detected paper mill papers might be substantially different from those that are not detected. New mechanisms are needed to identify and avoid this relatively new type of misconduct.},
  language   = {en},
  urldate    = {2024-05-24},
  journal    = {BMJ},
  author     = {Candal-Pedreira, Cristina and Ross, Joseph S. and Ruano-Ravina, Alberto and Egilman, David S. and Fernández, Esteve and Pérez-Ríos, Mónica},
  month      = nov,
  year       = {2022},
  pmid       = {36442874},
  note       = {Publisher: British Medical Journal Publishing Group
                Section: Research},
  pages      = {e071517}
}
@article{capone_pragmemes_2005,
  title    = {Pragmemes (a study with reference to {English} and {Italian})},
  volume   = {37},
  issn     = {03782166},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S0378216605000421},
  doi      = {10.1016/j.pragma.2005.01.013},
  abstract = {In this paper I shall argue that we have to focus on units of pragmatic analysis such as the pragmeme. A pragmeme is a situated speech act in which the rules of language and of society combine in determining meaning, intended as a socially recognized object sensitive to social expectations about the situation in which the utterance to be interpreted is embedded. Given a sequence of utterances, literally interpretable as having certain meanings and goals, features of the situation are utilized to produce pragmatic inferences, thus completing or expanding a minimal proposition vocalized by an utterance; other features of the context of utterance, in the form of defeasible or otherwise noncancellable aspects of meaning, will transform the literal signiﬁcation of an utterance, imposing the stamp of the situation on it, making certain rules of interpretation relevant to it, and constituting a set of constraints that strictly enforce certain readings by discarding or eliminating others. In this paper, after expatiating on studies that advocate the importance of studying the context of use of a speech act, I offer analytic considerations on what seem to me interesting cases of pragmemes.},
  language = {en},
  number   = {9},
  urldate  = {2023-09-15},
  journal  = {Journal of Pragmatics},
  author   = {Capone, Alessandro},
  month    = sep,
  year     = {2005},
  pages    = {1355--1371}
}

@article{caton2024fairness,
  author     = {Caton, Simon and Haas, Christian},
  title      = {Fairness in Machine Learning: A Survey},
  year       = {2024},
  issue_date = {July 2024},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {56},
  number     = {7},
  issn       = {0360-0300},
  url        = {https://doi.org/10.1145/3616865},
  doi        = {10.1145/3616865},
  abstract   = {When Machine Learning technologies are used in contexts that affect citizens, companies as well as researchers need to be confident that there will not be any unexpected social implications, such as bias towards gender, ethnicity, and/or people with disabilities. There is significant literature on approaches to mitigate bias and promote fairness, yet the area is complex and hard to penetrate for newcomers to the domain. This article seeks to provide an overview of the different schools of thought and approaches that aim to increase the fairness of Machine Learning. It organizes approaches into the widely accepted framework of pre-processing, in-processing, and post-processing methods, subcategorizing into a further 11 method areas. Although much of the literature emphasizes binary classification, a discussion of fairness in regression, recommender systems, and unsupervised learning is also provided along with a selection of currently available open source libraries. The article concludes by summarizing open challenges articulated as five dilemmas for fairness research.},
  journal    = {ACM Comput. Surv.},
  month      = apr,
  articleno  = {166},
  numpages   = {38},
  keywords   = {Fairness, accountability, transparency, machine learning}
}



@article{chiong_learning_2008,
  title    = {Learning game strategy design through iterated {Prisoner}'s {Dilemma}},
  volume   = {32},
  issn     = {0952-8091, 1741-5047},
  url      = {http://www.inderscience.com/link.php?id=20957},
  doi      = {10.1504/IJCAT.2008.020957},
  language = {en},
  number   = {3},
  urldate  = {2024-07-14},
  journal  = {International Journal of Computer Applications in Technology},
  author   = {Chiong, Raymond and Jankovic, Lubo},
  year     = {2008},
  pages    = {216}
}

@article{cohen_eva_nodate,
  title    = {Eva:  {A} {Planning}-{Based} {Explanatory} 	{Collaborative} {Dialogue} {System}},
  abstract = {Eva is a multimodal conversational system that helps users to accomplish their domain goals through collaborative dialogue. The system does this by inferring users’ intentions and plans to achieve those goals, detects whether obstacles are present to their achievement, finds plans to overcome those obstacles or to achieve higher-level goals, and plans its actions, including speech acts, to help users accomplish them. In that sense, Eva is a collaborative “planning-based” dialogue system – one whose dialogue engine is a planner. Eva does not have a pre-structured plan or script that it follows in a dialogue, but rather generates , executes, and repairs plans during the course of an interaction in order to enable the user’s domain goals to succeed. In doing so, the system maintains and reasons with its own beliefs, goals and intentions, and explicitly reasons about those of its user. Belief reasoning is accomplished with a modal Horn-clause metainterpreter. The plan recognition/obstacle processing enables the system to reason about the intended interpretation of indirect speech acts, in addition to issuing collaborative responses. Notably, the system obeys principles of intention and the dependence of intentions on yet others, which form its plans. Specifically, the planning and reasoning subsystems obey the principles of persistent goals and intentions as described in (Cohen and Levesque, 1990a), including the formation and decomposition of intentions to perform complex actions, as well as the conditions under which they can be given up. Among the system’s repertoire of actions are speech acts, some of whose definitions have been given in other papers of ours. In virtue of its planning process, the system treats its speech acts just like its other actions — physical acts affect physical states, digital acts affect digital states, and speech acts affect mental and social states. This general approach enables Eva to plan a variety of speech acts including requests, informs, questions, confirmations, recommendations, offers, acceptances, greetings, and emotive expressions. Each of these has a formally specified semantics which is used during the planning and reasoning processes. Because it can keep track of different users’ mental states, it can engage in multi-party dialogues. Importantly, Eva can explain its utterances because it has created a plan standing behind each of them. Finally, Eva employs multimodal input and output, driving an avatar that can perceive and employ facial and head movements along with emotive speech acts.},
  language = {en},
  author   = {Cohen, Philip R and Galescu, Lucian}
}





@article{collaborative_working_group_from_the_conference_keeping_the_pool_clean_prevention_and_management_of_misconduct_related_retractions_repair_2018,
  title      = {{RePAIR} consensus guidelines: {Responsibilities} of {Publishers}, {Agencies}, {Institutions}, and {Researchers} in protecting the integrity of the research record},
  volume     = {3},
  issn       = {2058-8615},
  shorttitle = {{RePAIR} consensus guidelines},
  url        = {https://researchintegrityjournal.biomedcentral.com/articles/10.1186/s41073-018-0055-1},
  doi        = {10.1186/s41073-018-0055-1},
  language   = {en},
  number     = {1},
  urldate    = {2025-01-24},
  journal    = {Research Integrity and Peer Review},
  author     = {{Collaborative Working Group from the conference “Keeping the Pool Clean: Prevention and Management of Misconduct Related Retractions”}},
  month      = dec,
  year       = {2018},
  pages      = {15},
  file       = {Full Text:/home/aaronfletcher/snap/zotero-snap/common/Zotero/storage/BU2ZDUBL/Collaborative Working Group from the conference “Keeping the Pool Clean Prevention and Management of Misconduct Related Retractions” - 2018 - RePAIR consensus guidelines Responsibilities of Publishers, Agencies, Institutions, and Researchers.pdf:application/pdf}
}









@incollection{congiunti_ethics_2023,
  address   = {Cham},
  title     = {Ethics and {Integrity} in {Academic} {Publishing}},
  isbn      = {978-3-031-24059-1 978-3-031-24060-7},
  url       = {https://link.springer.com/10.1007/978-3-031-24060-7_5},
  language  = {en},
  urldate   = {2025-01-24},
  booktitle = {Ethics in {Research}},
  publisher = {Springer Nature Switzerland},
  author    = {Caporale, Cinzia and Zagarella, Roberta Martina},
  editor    = {Congiunti, Lorella and Lo Piccolo, Francesco and Russo, Antonio and Serio, Mario},
  year      = {2023},
  doi       = {10.1007/978-3-031-24060-7_5},
  note      = {Series Title: UNIPA Springer Series},
  pages     = {53--69}
}






@misc{cormack_autonomy_2015,
  title     = {Autonomy and {Reliability} of {Continuous} {Active} {Learning} for {Technology}-{Assisted} {Review}},
  url       = {http://arxiv.org/abs/1504.06868},
  abstract  = {We enhance the autonomy of the continuous active learning method shown by Cormack and Grossman (SIGIR 2014) to be effective for technology-assisted review, in which documents from a collection are retrieved and reviewed, using relevance feedback, until substantially all of the relevant documents have been reviewed. Autonomy is enhanced through the elimination of topic-specific and dataset-specific tuning parameters, so that the sole input required by the user is, at the outset, a short query, topic description, or single relevant document; and, throughout the review, ongoing relevance assessments of the retrieved documents. We show that our enhancements consistently yield superior results to Cormack and Grossman's version of continuous active learning, and other methods, not only on average, but on the vast majority of topics from four separate sets of tasks: the legal datasets examined by Cormack and Grossman, the Reuters RCV1-v2 subject categories, the TREC 6 AdHoc task, and the construction of the TREC 2002 filtering test collection.},
  language  = {en},
  urldate   = {2024-06-06},
  publisher = {arXiv},
  author    = {Cormack, Gordon V. and Grossman, Maura R.},
  month     = apr,
  year      = {2015},
  note      = {arXiv:1504.06868 [cs]},
  keywords  = {Computer Science - Information Retrieval, Computer Science - Machine Learning}
}


@inproceedings{cormack_efficient_1998,
  address   = {New York, NY, USA},
  series    = {{SIGIR} '98},
  title     = {Efficient construction of large test collections},
  isbn      = {978-1-58113-015-7},
  url       = {https://dl.acm.org/doi/10.1145/290941.291009},
  doi       = {10.1145/290941.291009},
  urldate   = {2024-05-29},
  booktitle = {Proceedings of the 21st annual international {ACM} {SIGIR} conference on {Research} and development in information retrieval},
  publisher = {Association for Computing Machinery},
  author    = {Cormack, Gordon V. and Palmer, Christopher R. and Clarke, Charles L. A.},
  month     = aug,
  year      = {1998},
  pages     = {282--289}
}


@inproceedings{cormack_engineering_2016,
  address   = {New York, NY, USA},
  series    = {{SIGIR} '16},
  title     = {Engineering {Quality} and {Reliability} in {Technology}-{Assisted} {Review}},
  isbn      = {978-1-4503-4069-4},
  url       = {https://dl.acm.org/doi/10.1145/2911451.2911510},
  doi       = {10.1145/2911451.2911510},
  abstract  = {The objective of technology-assisted review ("TAR") is to find as much relevant information as possible with reasonable effort. Quality is a measure of the extent to which a TAR method achieves this objective, while reliability is a measure of how consistently it achieves an acceptable result. We are concerned with how to define, measure, and achieve high quality and high reliability in TAR. When quality is defined using the traditional goal-post method of specifying a minimum acceptable recall threshold, the quality and reliability of a TAR method are both, by definition, equal to the probability of achieving the threshold. Assuming this definition of quality and reliability, we show how to augment any TAR method to achieve guaranteed reliability, for a quantifiable level of additional review effort. We demonstrate this result by augmenting the TAR method supplied as the baseline model implementation for the TREC 2015 Total Recall Track, measuring reliability and effort for 555 topics from eight test collections. While our empirical results corroborate our claim of guaranteed reliability, we observe that the augmentation strategy may entail disproportionate effort, especially when the number of relevant documents is low. To address this limitation, we propose stopping criteria for the model implementation that may be applied with no additional review effort, while achieving empirical reliability that compares favorably to the provably reliable method. We further argue that optimizing reliability according to the traditional goal-post method is inconsistent with certain subjective aspects of quality, and that optimizing a Taguchi quality loss function may be more apt.},
  urldate   = {2024-06-13},
  booktitle = {Proceedings of the 39th {International} {ACM} {SIGIR} conference on {Research} and {Development} in {Information} {Retrieval}},
  publisher = {Association for Computing Machinery},
  author    = {Cormack, Gordon V. and Grossman, Maura R.},
  month     = jul,
  year      = {2016},
  keywords  = {continuous active learning, e-discovery, electronic discovery, predictive coding, quality, relevance feedback, reliability, systematic review, technology-assisted review, test collections},
  pages     = {75--84}
}


@inproceedings{cormack_engineering_2016-1,
  address   = {New York, NY, USA},
  series    = {{SIGIR} '16},
  title     = {Engineering {Quality} and {Reliability} in {Technology}-{Assisted} {Review}},
  isbn      = {978-1-4503-4069-4},
  url       = {https://dl.acm.org/doi/10.1145/2911451.2911510},
  doi       = {10.1145/2911451.2911510},
  abstract  = {The objective of technology-assisted review ("TAR") is to find as much relevant information as possible with reasonable effort. Quality is a measure of the extent to which a TAR method achieves this objective, while reliability is a measure of how consistently it achieves an acceptable result. We are concerned with how to define, measure, and achieve high quality and high reliability in TAR. When quality is defined using the traditional goal-post method of specifying a minimum acceptable recall threshold, the quality and reliability of a TAR method are both, by definition, equal to the probability of achieving the threshold. Assuming this definition of quality and reliability, we show how to augment any TAR method to achieve guaranteed reliability, for a quantifiable level of additional review effort. We demonstrate this result by augmenting the TAR method supplied as the baseline model implementation for the TREC 2015 Total Recall Track, measuring reliability and effort for 555 topics from eight test collections. While our empirical results corroborate our claim of guaranteed reliability, we observe that the augmentation strategy may entail disproportionate effort, especially when the number of relevant documents is low. To address this limitation, we propose stopping criteria for the model implementation that may be applied with no additional review effort, while achieving empirical reliability that compares favorably to the provably reliable method. We further argue that optimizing reliability according to the traditional goal-post method is inconsistent with certain subjective aspects of quality, and that optimizing a Taguchi quality loss function may be more apt.},
  urldate   = {2024-06-06},
  booktitle = {Proceedings of the 39th {International} {ACM} {SIGIR} conference on {Research} and {Development} in {Information} {Retrieval}},
  publisher = {Association for Computing Machinery},
  author    = {Cormack, Gordon V. and Grossman, Maura R.},
  month     = jul,
  year      = {2016},
  keywords  = {continuous active learning, e-discovery, electronic discovery, predictive coding, quality, relevance feedback, reliability, systematic review, technology-assisted review, test collections},
  pages     = {75--84}
}



@inproceedings{cormack_scalability_2016,
  address   = {Indianapolis Indiana USA},
  title     = {Scalability of {Continuous} {Active} {Learning} for {Reliable} {High}-{Recall} {Text} {Classification}},
  isbn      = {978-1-4503-4073-1},
  url       = {https://dl.acm.org/doi/10.1145/2983323.2983776},
  doi       = {10.1145/2983323.2983776},
  language  = {en},
  urldate   = {2024-06-06},
  booktitle = {Proceedings of the 25th {ACM} {International} on {Conference} on {Information} and {Knowledge} {Management}},
  publisher = {ACM},
  author    = {Cormack, Gordon V. and Grossman, Maura R.},
  month     = oct,
  year      = {2016},
  pages     = {1039--1048}
}


@article{crystal_segmental_1982,
  title      = {Segmental durations in connected speech signals: preliminary results},
  volume     = {72},
  issn       = {0001-4966},
  shorttitle = {Segmental durations in connected speech signals},
  doi        = {10.1121/1.388251},
  abstract   = {The data base, methods for a study of the durations of phonetic units in connected speech, and some preliminary results are described. From readings of two scripts by many talkers, two sets of seven talkers each were selected, based on total reading time, to form a fast group and a slow group of talkers. Using computer graphics and digital playback procedures, the recordings were segmented into breath groups and pauses, and the first four sentences in each script were segmented into phones. The hold and release (that is, plosion and/or frication) portions of stops were identified and measured; less than 50\% of the stops included releases. To establish the usefulness of the data base, the first-order statistics of the phonetic segments were determined, and a variety of durational characteristics were compared to existing reports. Analysis of number of breath groups, phonation time, and pause characterized the difference between so-called average fast and average slow talkers; however, no script-independent measure of these variables was found which would accurately predict the classification of individual talkers. The mean durations of various phonetic categories showed essentially the same percentage change when the fast and slow talkers were compared. Preliminary analyses of contextual influences on durations showed some expected changes, and also indicated that certain traditional predictions may not hold for informal connected speech. Gamma functions were fitted to the distributions of durations of various gross categories.},
  language   = {eng},
  number     = {3},
  journal    = {The Journal of the Acoustical Society of America},
  author     = {Crystal, T. H. and House, A. S.},
  month      = sep,
  year       = {1982},
  pmid       = {7130529},
  keywords   = {Computers, Female, Humans, Male, Phonation, Phonetics, Speech, Statistics as Topic, Time Factors},
  pages      = {705--716}
}



@book{dan_speech_2024,
  author  = {Daniel Jurafsky and James H. Martin},
  title   = {Speech and Language Processing: An Introduction to
             Natural Language Processing, Computational Linguistics,
             and Speech Recognition with Language Models},
  year    = {2025},
  url     = {https://web.stanford.edu/~jurafsky/slp3/},
  note    = {Online manuscript released January 12, 2025},
  edition = {3rd},
  urldate = {2025-1-12},
  pages   = {1--23}
}

@inproceedings{dasgupta_hierarchical_2008,
  address   = {Helsinki, Finland},
  title     = {Hierarchical sampling for active learning},
  isbn      = {978-1-60558-205-4},
  url       = {http://portal.acm.org/citation.cfm?doid=1390156.1390183},
  doi       = {10.1145/1390156.1390183},
  abstract  = {We present an active learning scheme that exploits cluster structure in data.},
  language  = {en},
  urldate   = {2024-06-13},
  booktitle = {Proceedings of the 25th international conference on {Machine} learning - {ICML} '08},
  publisher = {ACM Press},
  author    = {Dasgupta, Sanjoy and Hsu, Daniel},
  year      = {2008},
  pages     = {208--215}
}


@misc{devlin_bert_2019,
  title      = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
  shorttitle = {{BERT}},
  doi        = {10.48550/arXiv.1810.04805},
  abstract   = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  publisher  = {arXiv},
  author     = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  month      = may,
  year       = {2019},
  keywords   = {Computer Science - Computation and Language}
}


@inproceedings{di_nunzio_study_2018,
  address   = {Cham},
  title     = {A {Study} of an {Automatic} {Stopping} {Strategy} for {Technologically} {Assisted} {Medical} {Reviews}},
  isbn      = {978-3-319-76941-7},
  doi       = {10.1007/978-3-319-76941-7_61},
  abstract  = {Systematic medical reviews are a method to collect the findings from multiple studies in a reliable way. Given budget and time constraints, limiting the recall of a search may undermine the quality of a review to such a degree that the validity of its findings is questionable. In this paper, we investigate a variable threshold approach to tackle the problem of a total recall task in medical reviews proposed by a Cross-Language Evaluation Forum (CLEF) eHealth lab in 2017. Compared to the official results submitted to the CLEF eHealth task, our approach performed consistently better over all the range of thresholds considered achieving a recall greater than 0.95 with 25,000 documents less than the best performing systems. The runs and the source code to generate the analyses of this paper are available at the following GitHub repository (https://github.com/gmdn/ECIR2018).},
  language  = {en},
  booktitle = {Advances in {Information} {Retrieval}},
  publisher = {Springer International Publishing},
  author    = {Di Nunzio, Giorgio Maria},
  editor    = {Pasi, Gabriella and Piwowarski, Benjamin and Azzopardi, Leif and Hanbury, Allan},
  year      = {2018},
  pages     = {672--677}
}

@article{dillion_can_2023,
  title    = {Can {AI} language models replace human participants?},
  volume   = {27},
  issn     = {13646613},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S1364661323000980},
  doi      = {10.1016/j.tics.2023.04.008},
  language = {en},
  number   = {7},
  urldate  = {2023-09-15},
  journal  = {Trends in Cognitive Sciences},
  author   = {Dillion, Danica and Tandon, Niket and Gu, Yuling and Gray, Kurt},
  month    = jul,
  year     = {2023},
  pages    = {597--600}
}


@article{dingemanse_is_2013,
  title      = {Is “{Huh}?” a {Universal} {Word}? {Conversational} {Infrastructure} and the {Convergent} {Evolution} of {Linguistic} {Items}},
  volume     = {8},
  issn       = {1932-6203},
  shorttitle = {Is “{Huh}?},
  url        = {https://dx.plos.org/10.1371/journal.pone.0078273},
  doi        = {10.1371/journal.pone.0078273},
  language   = {en},
  number     = {11},
  urldate    = {2023-10-02},
  journal    = {PLoS ONE},
  author     = {Dingemanse, Mark and Torreira, Francisco and Enfield, N. J.},
  editor     = {Bolhuis, Johan J.},
  month      = nov,
  year       = {2013},
  pages      = {e78273}
}

@article{doi:10.1126/science.342.6162.1035,
  author   = {Mara Hvistendahl },
  title    = {China's Publication Bazaar},
  journal  = {Science},
  volume   = {342},
  number   = {6162},
  pages    = {1035-1039},
  year     = {2013},
  doi      = {10.1126/science.342.6162.1035},
  abstract = {A Science investigation has uncovered a smorgasbord of questionable practices including paying for author's slots on papers written by other scientists and buying papers from online brokers. Science has exposed a thriving academic black market in China involving shady agencies, corrupt scientists, and compromised editors—many of them operating in plain view. The commodity: papers in journals indexed by Thomson Reuters' Science Citation Index, Thomson Reuters' Social Sciences Citation Index, and Elsevier's Engineering Index.}
}

@article{doi:10.3138/jsp.49.3.02,
  author   = {Liu, Xiaomei and Chen, Xiaotian},
  title    = {Journal Retractions: Some Unique Features of Research Misconduct in China},
  journal  = {Journal of Scholarly Publishing},
  volume   = {49},
  number   = {3},
  pages    = {305-319},
  year     = {2018},
  doi      = {10.3138/jsp.49.3.02},
  abstract = { This study used data from the Retraction Watch website and from published reports on retractions and paper mills to summarize key features of research misconduct in China. Compared with publicized cases of falsified or fabricated data by authors from other countries of the world, the number of Chinese academics exposed for research misconduct has increased dramatically in recent years. Chinese authors do not have to generate fake data or fake peer reviews for themselves because paper mills in China will do the work for them for a price. Major retractions of articles by authors from China were all announced by international publishers. In contrast, there are few reports of retractions announced by China's domestic publishers. China's publication requirements for physicians seeking promotions and its leniency toward research misconduct are two major factors promoting the boom of paper mills in China. }
}

@article{dong_survey_2022,
  title    = {A {Survey} of {Natural} {Language} {Generation}},
  volume   = {55},
  issn     = {0360-0300},
  url      = {https://doi.org/10.1145/3554727},
  doi      = {10.1145/3554727},
  abstract = {This article offers a comprehensive review of the research on Natural Language Generation (NLG) over the past two decades, especially in relation to data-to-text generation and text-to-text generation deep learning methods, as well as new applications of NLG technology. This survey aims to (a) give the latest synthesis of deep learning research on the NLG core tasks, as well as the architectures adopted in the field; (b) detail meticulously and comprehensively various NLG tasks and datasets, and draw attention to the challenges in NLG evaluation, focusing on different evaluation methods and their relationships; (c) highlight some future emphasis and relatively recent research issues that arise due to the increasing synergy between NLG and other artificial intelligence areas, such as computer vision, text, and computational creativity.},
  number   = {8},
  urldate  = {2024-07-14},
  journal  = {ACM Comput. Surv.},
  author   = {Dong, Chenhe and Li, Yinghui and Gong, Haifan and Chen, Miaoxin and Li, Junxin and Shen, Ying and Yang, Min},
  month    = dec,
  year     = {2022},
  pages    = {173:1--173:38}
}

@book{dreyfus_what_1992,
  address    = {Cambridge, Mass},
  title      = {What computers still can't do: a critique of artificial reason},
  isbn       = {978-0-262-04134-8 978-0-262-54067-4},
  shorttitle = {What computers still can't do},
  publisher  = {MIT Press},
  author     = {Dreyfus, Hubert L.},
  year       = {1992},
  keywords   = {Artificial intelligence}
}

@article{enrici_theory_2019,
  title      = {Theory of {Mind}, pragmatics and the brain: {Converging} evidence for the role of intention processing as a core feature ofhuman communication},
  volume     = {26},
  issn       = {0929-0907, 1569-9943},
  shorttitle = {Theory of {Mind}, pragmatics and the brain},
  url        = {http://www.jbe-platform.com/content/journals/10.1075/pc.19010.enr},
  doi        = {10.1075/pc.19010.enr},
  abstract   = {Abstract
                
                Theory of Mind
                (ToM) is a neurocognitive system that allows the perceiver to attribute mental
                states, such as intentions, beliefs, or feelings, to others’ actions. The aim of the present work is to analyse the engagement of
                the ToM system in communication, in particular, in communicative intention processing. To this aim, we propose an
                Intention Processing Network
                (IPN) with its own principles and mechanisms, that is, a brain network
                differentially engaged according to the complex intertwining of the context, goal, and action involved. According to our IPN
                model, a set of brain regions of the ToM system (i.e. left and right temporoparietal junction, precuneus, and medial prefrontal
                cortex) are differentially involved in comprehending different types of intention, such as private or social intentions. We
                provide independent and convergent evidence on the role of the IPN model in communicative intention processing and we show that
                the engagement of the IPN does not depend upon the communicative means used, that is, written language, auditory language, or
                gesture. Evidence deriving from different experimental paradigms, including neuroimaging, lesion, neurodegenerative, and brain
                stimulation studies are discussed. In our view, this evidence establishes a link between ToM and pragmatics studies and suggests
                the role of intention processing as a core feature of human communication.},
  language   = {en},
  number     = {1},
  urldate    = {2023-09-16},
  journal    = {Pragmatics \& Cognition},
  author     = {Enrici, Ivan and Bara, Bruno G. and Adenzato, Mauro},
  month      = dec,
  year       = {2019},
  pages      = {5--38}
}

@article{feder_causal_2022,
  title      = {Causal {Inference} in {Natural} {Language} {Processing}: {Estimation}, {Prediction}, {Interpretation} and {Beyond}},
  volume     = {10},
  issn       = {2307-387X},
  shorttitle = {Causal {Inference} in {Natural} {Language} {Processing}},
  url        = {https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00511/113490/Causal-Inference-in-Natural-Language-Processing},
  doi        = {10.1162/tacl_a_00511},
  abstract   = {Abstract
                A fundamental goal of scientific research is to learn about causal relationships. However, despite its critical role in the life and social sciences, causality has not had the same importance in Natural Language Processing (NLP), which has traditionally placed more emphasis on predictive tasks. This distinction is beginning to fade, with an emerging area of interdisciplinary research at the convergence of causal inference and language processing. Still, research on causality in NLP remains scattered across domains without unified definitions, benchmark datasets and clear articulations of the challenges and opportunities in the application of causal inference to the textual domain, with its unique properties. In this survey, we consolidate research across academic areas and situate it in the broader NLP landscape. We introduce the statistical challenge of estimating causal effects with text, encompassing settings where text is used as an outcome, treatment, or to address confounding. In addition, we explore potential uses of causal inference to improve the robustness, fairness, and interpretability of NLP models. We thus provide a unified overview of causal inference for the NLP community.1},
  language   = {en},
  urldate    = {2024-04-04},
  journal    = {Transactions of the Association for Computational Linguistics},
  author     = {Feder, Amir and Keith, Katherine A. and Manzoor, Emaad and Pryzant, Reid and Sridhar, Dhanya and Wood-Doughty, Zach and Eisenstein, Jacob and Grimmer, Justin and Reichart, Roi and Roberts, Margaret E. and Stewart, Brandon M. and Veitch, Victor and Yang, Diyi},
  month      = oct,
  year       = {2022},
  pages      = {1138--1158}
}

@misc{fletcher_afletcher53retractionwatch_nodate,
  title    = {afletcher53/{RetractionWatch}},
  url      = {https://github.com/afletcher53/RetractionWatch},
  abstract = {Contribute to afletcher53/RetractionWatch development by creating an account on GitHub.},
  urldate  = {2024-07-20},
  author   = {Fletcher, Aaron}
}

@article{fletcher_does_2020,
  title      = {Does {Non}-{Steroidal} {Anti}-inflammatories: {Does} carprofen or meloxicam have fewer gastrointestinal side effects?},
  volume     = {5},
  copyright  = {All rights reserved},
  issn       = {2396-9776},
  shorttitle = {Does {Non}-{Steroidal} {Anti}-inflammatories},
  url        = {https://veterinaryevidence.org/index.php/ve/article/view/301},
  doi        = {10.18849/ve.v5i3.301},
  abstract   = {PICO question 
                In canines, does the oral administration of carprofen, when compared to meloxicam, result in fewer gastrointestinal side effects? 
                
                Clinical bottom line 
                Category of research question 
                Treatment 
                The number and type of study designs reviewed 
                Three prospective randomised controlled trials were critically reviewed 
                Strength of evidence 
                Weak 
                Outcomes reported 
                Treatment with carprofen or meloxicam results in no significant difference in gastric lesion scoring, increased intestinal mucosal permeability or diminished small bowel absorptive capacity 
                Conclusion 
                There is insufficient evidence supporting preferential administration of carprofen or meloxicam to reduce gastrointestinal side effects 
                
                How to apply this evidence in practice 
                The application of evidence into practice should take into account multiple factors, not limited to: individual clinical expertise, patient’s circumstances and owners’ values, country, location or clinic where you work, the individual case in front of you, the availability of therapies and resources. 
                Knowledge Summaries are a resource to help reinforce or inform decision-making. They do not override the responsibility or judgement of the practitioner to do what is best for the animal in their care},
  language   = {en},
  number     = {3},
  urldate    = {2023-09-14},
  journal    = {Veterinary Evidence},
  author     = {Fletcher, Aaron Harold Andrew},
  month      = sep,
  year       = {2020}
}

@article{funer_responsibility_2023,
  title        = {Responsibility and decision-making authority in using clinical decision support systems: an empirical-ethical exploration of German prospective professionals' preferences and concerns},
  volume       = {50},
  issn         = {1473-4257},
  doi          = {10.1136/jme-2022-108814},
  shorttitle   = {Responsibility and decision-making authority in using clinical decision support systems},
  abstract     = {Machine learning-driven clinical decision support systems ({ML}-{CDSSs}) seem impressively promising for future routine and emergency care. However, reflection on their clinical implementation reveals a wide array of ethical challenges. The preferences, concerns and expectations of professional stakeholders remain largely unexplored. Empirical research, however, may help to clarify the conceptual debate and its aspects in terms of their relevance for clinical practice. This study explores, from an ethical point of view, future healthcare professionals' attitudes to potential changes of responsibility and decision-making authority when using {ML}-{CDSS}. Twenty-seven semistructured interviews were conducted with German medical students and nursing trainees. The data were analysed based on qualitative content analysis according to Kuckartz. Interviewees' reflections are presented under three themes the interviewees describe as closely related: (self-)attribution of responsibility, decision-making authority and need of (professional) experience. The results illustrate the conceptual interconnectedness of professional responsibility and its structural and epistemic preconditions to be able to fulfil clinicians' responsibility in a meaningful manner. The study also sheds light on the four relata of responsibility understood as a relational concept. The article closes with concrete suggestions for the ethically sound clinical implementation of {ML}-{CDSS}.},
  pages        = {6--11},
  number       = {1},
  journaltitle = {Journal of Medical Ethics},
  shortjournal = {J Med Ethics},
  author       = {Funer, Florian and Liedtke, Wenke and Tinnemeyer, Sara and Klausen, Andrea Diana and Schneider, Diana and Zacharias, Helena U. and Langanke, Martin and Salloch, Sabine},
  date         = {2023-12-14},
  pmid         = {37217277},
  pmcid        = {PMC10803986},
  keywords     = {Ethics, Humans, Decision Making, Attitude of Health Personnel, Decision Support Systems, Clinical, Empirical Research, Ethics- Medical, Group Processes, Health Personnel, Information Technology, Prospective Studies, Qualitative Research},
  file         = {Full Text:/home/aaron/snap/zotero-snap/common/Zotero/storage/Z29GR7DM/Funer et al. - 2023 - Responsibility and decision-making authority in using clinical decision support systems an empirica.pdf:application/pdf}
}

@misc{futrell_neural_2019,
  title      = {Neural {Language} {Models} as {Psycholinguistic} {Subjects}: {Representations} of {Syntactic} {State}},
  shorttitle = {Neural {Language} {Models} as {Psycholinguistic} {Subjects}},
  url        = {http://arxiv.org/abs/1903.03260},
  abstract   = {We deploy the methods of controlled psycholinguistic experimentation to shed light on the extent to which the behavior of neural network language models reﬂects incremental representations of syntactic state. To do so, we examine model behavior on artiﬁcial sentences containing a variety of syntactically complex structures. We test four models: two publicly available LSTM sequence models of English (Jozefowicz et al., 2016; Gulordava et al., 2018) trained on large datasets; an RNNG (Dyer et al., 2016) trained on a small, parsed dataset; and an LSTM trained on the same small corpus as the RNNG. We ﬁnd evidence that the LSTMs trained on large datasets represent syntactic state over large spans of text in a way that is comparable to the RNNG, while the LSTM trained on the small dataset does not or does so only weakly.},
  language   = {en},
  urldate    = {2023-09-15},
  publisher  = {arXiv},
  author     = {Futrell, Richard and Wilcox, Ethan and Morita, Takashi and Qian, Peng and Ballesteros, Miguel and Levy, Roger},
  month      = mar,
  year       = {2019},
  note       = {arXiv:1903.03260 [cs]},
  keywords   = {Computer Science - Computation and Language}
}

@article{gaudino_trends_2021,
  title    = {Trends and {Characteristics} of {Retracted} {Articles} in the {Biomedical} {Literature}, 1971 to 2020},
  volume   = {181},
  issn     = {2168-6106},
  doi      = {10.1001/jamainternmed.2021.1807},
  abstract = {This cross-sectional study describes trends and characteristics of retracted articles in the biomedical literature from 1971 to August 2020.},
  number   = {8},
  journal  = {JAMA Internal Medicine},
  author   = {Gaudino, Mario and Robinson, N. Bryce and Audisio, Katia and Rahouma, Mohamed and Benedetto, Umberto and Kurlansky, Paul and Fremes, Stephen E.},
  month    = aug,
  year     = {2021},
  pmid     = {33970185},
  pmcid    = {PMC8111562},
  pages    = {1118--1121}
}

@article{Grey2024,
  author  = {Grey, A. and Avenell, A. and Klein, A. A. and Byrne, J. A. and Wilmshurst, P. and Bolland, M. J.},
  title   = {How to improve assessments of publication integrity},
  journal = {Nature},
  year    = {2024},
  volume  = {632},
  number  = {8023},
  pages   = {26--28},
  doi     = {10.1038/d41586-024-02449-8}
}


@misc{gu_mamba_2023,
  title      = {Mamba: {Linear}-{Time} {Sequence} {Modeling} with {Selective} {State} {Spaces}},
  shorttitle = {Mamba},
  url        = {http://arxiv.org/abs/2312.00752},
  doi        = {10.48550/arXiv.2312.00752},
  abstract   = {Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5\${\textbackslash}times\$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.},
  urldate    = {2024-04-16},
  publisher  = {arXiv},
  author     = {Gu, Albert and Dao, Tri},
  month      = dec,
  year       = {2023},
  note       = {arXiv:2312.00752 [cs]},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning}
}

@inproceedings{gubelmann_when_2023,
  address   = {Toronto, Canada},
  title     = {When {Truth} {Matters} - {Addressing} {Pragmatic} {Categories} in {Natural} {Language} {Inference} ({NLI}) by {Large} {Language} {Models} ({LLMs})},
  url       = {https://aclanthology.org/2023.starsem-1.4},
  doi       = {10.18653/v1/2023.starsem-1.4},
  abstract  = {In this paper, we focus on the ability of large language models (LLMs) to accommodate different pragmatic sentence types, such as questions, commands, as well as sentence fragments for natural language inference (NLI). On the commonly used notion of logical inference, nothing can be inferred from a question, a command, or an incomprehensible sentence fragment. We find MNLI, arguably the most important NLI dataset, and hence models fine-tuned on this dataset, insensitive to this fact. Using a symbolic semantic parser, we develop and make publicly available, fine-tuning datasets designed specifically to address this issue, with promising results. We also make a first exploration of ChatGPT’s concept of entailment.},
  language  = {en},
  urldate   = {2023-09-15},
  booktitle = {Proceedings of the 12th {Joint} {Conference} on {Lexical} and {Computational} {Semantics} (*{SEM} 2023)},
  publisher = {Association for Computational Linguistics},
  author    = {Gubelmann, Reto and Kalouli, Aikaterini-lida and Niklaus, Christina and Handschuh, Siegfried},
  year      = {2023},
  pages     = {24--39}
}

@article{haeb-umbach_speech_2019,
  title      = {Speech {Processing} for {Digital} {Home} {Assistants}: {Combining} {Signal} {Processing} {With} {Deep}-{Learning} {Techniques}},
  volume     = {36},
  issn       = {1053-5888, 1558-0792},
  shorttitle = {Speech {Processing} for {Digital} {Home} {Assistants}},
  url        = {https://ieeexplore.ieee.org/document/8887564/},
  doi        = {10.1109/MSP.2019.2918706},
  number     = {6},
  urldate    = {2023-10-02},
  journal    = {IEEE Signal Processing Magazine},
  author     = {Haeb-Umbach, Reinhold and Watanabe, Shinji and Nakatani, Tomohiro and Bacchiani, Michiel and Hoffmeister, Bjorn and Seltzer, Michael L. and Zen, Heiga and Souden, Mehrez},
  month      = nov,
  year       = {2019},
  pages      = {111--124}
}

@book{harman_price_2011,
  title      = {The {Price} {Of} {Altruism}: {George} {Price} and the {Search} for the {Origins} of {Kindness}},
  isbn       = {978-0-09-953166-1},
  shorttitle = {The {Price} {Of} {Altruism}},
  abstract   = {When George Price died in January 1975, his funeral in London was attended by five homeless men. Alongside them were Bill Hamilton and John Maynard Smith, two distinguished British evolutionary biologists. All seven men had come to mourn an eccentric American genius who helped to unpick the riddle of how altruism, or unselfish concern for the welfare of others, could exist in a world driven by survival of the fittest and who committed suicide aged just 52.In The Price of Altruism Price's personal and professional journey is intricately woven into a sweeping arc of modern politics and science that takes us from Darwin's Beagle to the court of the Russian Tsar, from Marxist manifestos to Nazi heresies, and from First World War trenches to Vietnam demonstrations. Featuring some of the most brilliant minds of the modern age, it is the riveting tale of mankind's search for the origins of kindness.},
  language   = {English},
  publisher  = {Vintage},
  author     = {Harman, Oren},
  month      = may,
  year       = {2011}
}

@book{hastie_elements_2009,
  location  = {New York, {NY}},
  edition   = {Second},
  title     = {The Elements of Statistical Learning},
  rights    = {http://www.springer.com/tdm},
  isbn      = {978-0-387-84857-0 978-0-387-84858-7},
  series    = {Springer Series in Statistics},
  publisher = {Springer New York},
  author    = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  date      = {2009},
  doi       = {10.1007/978-0-387-84858-7}
}

@article{heibi_qualitative_2021,
  title        = {A qualitative and quantitative analysis of open citations to retracted articles: the Wakefield 1998 et al.'s case},
  volume       = {126},
  issn         = {0138-9130, 1588-2861},
  url          = {https://link.springer.com/10.1007/s11192-021-04097-5},
  doi          = {10.1007/s11192-021-04097-5},
  shorttitle   = {A qualitative and quantitative analysis of open citations to retracted articles},
  year         = {2021},
  abstract     = {Abstract
                  
                  In this article, we show the results of a quantitative and qualitative analysis of open citations on a popular and highly cited retracted paper: “Ileal-lymphoid-nodular hyperplasia, non-specific colitis and pervasive developmental disorder in children” by Wakefield et al
                  .
                  , published in 1998. The main purpose of our study is to understand the behavior of the publications citing one retracted article and the characteristics of the citations the retracted article accumulated over time. Our analysis is based on a methodology which illustrates how we gathered the data, extracted the topics of the citing articles and visualized the results. The data and services used are all open and free to foster the reproducibility of the analysis. The outcomes concerned the analysis of the entities citing Wakefield et al
                  .
                  ’s article and their related in-text citations. We observed a constant increasing number of citations in the last 20 years, accompanied with a constant increment in the percentage of those acknowledging its retraction. Citing articles have started either discussing or dealing with the retraction of Wakefield et al
                  .
                  ’s article even before its full retraction happened in 2010. Articles in the social sciences domain citing the Wakefield et al
                  .
                  ’s one were among those that have mostly discussed its retraction. In addition, when observing the in-text citations, we noticed that a large number of the citations received by Wakefield et al
                  .
                  ’s article has focused on general discussions without recalling strictly medical details, especially after the full retraction. Medical studies did not hesitate in acknowledging the retraction of the Wakefield et al
                  .
                  ’s article and often provided strong negative statements on it.},
  pages        = {8433--8470},
  number       = {10},
  journal      = {Scientometrics},
  shortjournal = {Scientometrics},
  author       = {Heibi, Ivan and Peroni, Silvio},
  date         = {2021-10},
  langid       = {english},
  file         = {Full Text:/home/aaron/snap/zotero-snap/common/Zotero/storage/CD8JXALC/Heibi and Peroni - 2021 - A qualitative and quantitative analysis of open citations to retracted articles the Wakefield 1998.pdf:application/pdf}
}

@article{heyes_submentalizing_2014,
  title      = {Submentalizing: {I} {Am} {Not} {Really} {Reading} {Your} {Mind}},
  volume     = {9},
  issn       = {1745-6916, 1745-6924},
  shorttitle = {Submentalizing},
  url        = {http://journals.sagepub.com/doi/10.1177/1745691613518076},
  doi        = {10.1177/1745691613518076},
  abstract   = {The nativist view of mentalizing—the view that humans have an inherent capacity to think about the mental states of others—has been recently reinvigorated by reports that adults and infants automatically represent mental states—that they engage in implicit mentalizing. In this article, I take a close look at the strongest evidence of implicit mentalizing in adults, which suggests that people automatically represent what others see, intend, and believe. I argue that although these experiments have been ingeniously designed and carefully implemented, they do not provide evidence of implicit mentalizing because their results could be due instead to submentalizing—domain-general cognitive mechanisms that simulate the effects of mentalizing in social contexts. These include the processes that mediate involuntary attentional orienting, spatial coding of response locations, object-centered spatial coding of stimulus locations, retroactive interference, and distraction. If my analysis is correct, it suggests that the same domain-general processes can provide a fast and efficient alternative to mentalizing in everyday life, allowing people to navigate a wide range of social situations without thinking about mental states. Thus, submentalizing could be both a substrate and a substitute for mentalizing.},
  language   = {en},
  number     = {2},
  urldate    = {2023-09-15},
  journal    = {Perspectives on Psychological Science},
  author     = {Heyes, Cecilia},
  month      = mar,
  year       = {2014},
  pages      = {131--143}
}

@article{howell_comparison_1991,
  title    = {Comparison of prosodic properties between read and spontaneous speech material},
  volume   = {10},
  issn     = {0167-6393},
  url      = {https://www.sciencedirect.com/science/article/pii/016763939190039V},
  doi      = {10.1016/0167-6393(91)90039-V},
  abstract = {In the present study, selected prosodic properties of spontaneous material which was later read by the same, and other, speakers are compared. The major differences between spontaneous speech and read speech are (1) readers tend to make the boundaries between tone units at different points, (2) the position of the stresses differs, and (3) there are fewer pauses in read speech, and the location of these differs between readers.
              Zusammenfassung
              Die prosodischen Einheiten von spontaner Rede werden mit ausgewählten prosodischen Eigenschaften von gelesenen Aufzeichungen desselben Sprachmaterials verglichen. Das Gelesene wird vom ursprünglichen Sprecher und von anderen Sprechern aufgezeichnet. Die Hauptunterschiede die zwischen spontaner und gelesener Rede gefunden wurden, sind folgende: (1) die Grenzen zwischen Toneinheiten werden an unterschiedliche Stellen gesetzt, (2) die Satzakzente werden auf andere Wörter gesetzt, und (3) es werden viel weniger Pausen in gelesener Rede gemacht, und die Stellen an denen sie gemacht werden sind von Sprecher zu Sprecher verschieden.
              Résumé
              Cette étude concerne une comparaison des caractéristiques prosodiques de textes spontanés émanant de 6 locuteurs, et des mêmes textes lus, après leur transcription orthographique, par leur auteur et les 5 autres locuteurs. Les différences entre les textes spontanés et les textes lus sont les suivantes: (1) il y a non-correspondance entre les frontières des “tone units”, (2) la position des accents diffère, et (3) il y a moins de pauses dans la parole lue, et leurs positions varient selon les lecteurs.},
  number   = {2},
  urldate  = {2023-10-26},
  journal  = {Speech Communication},
  author   = {Howell, Peter and Kadi-Hanifi, Karima},
  month    = jun,
  year     = {1991},
  keywords = {Prosody, coalescences, fragmentation, pauses, spontaneous speech, stress, tone units},
  pages    = {163--169}
}

@article{https://doi.org/10.1002/1873-3468.13747,
  author   = {Byrne, Jennifer A. and Christopher, Jana},
  title    = {Digital magic, or the dark arts of the 21st century—how can journals and peer reviewers detect manuscripts and publications from paper mills?},
  journal  = {FEBS Letters},
  volume   = {594},
  number   = {4},
  pages    = {583-589},
  doi      = {https://doi.org/10.1002/1873-3468.13747},
  abstract = {In recent years, it has been proposed that unrealistic requirements for academics and medical doctors to publish in scientific journals, combined with monetary publication rewards, have led to forms of contract cheating offered by organizations known as paper mills. Paper mills are alleged to offer products ranging from research data through to ghostwritten fraudulent or fabricated manuscripts and submission services. While paper mill operations remain poorly understood, it seems likely that paper mills need to balance product quantity and quality, such that they produce or contribute to large numbers of manuscripts that will be accepted for publication. Producing manuscripts at scale may be facilitated by the use of manuscript templates, which could give rise to shared features such as textual and organizational similarities, the description of highly generic study hypotheses and experimental approaches, digital images that show evidence of manipulation and/or reuse, and/or errors affecting verifiable experimental reagents. Based on these features, we propose practical steps that editors, journal staff, and peer reviewers can take to recognize and respond to research manuscripts and publications that may have been produced with undeclared assistance from paper mills.},
  year     = {2020}
}

@misc{hu_fine-grained_2023,
  title     = {A fine-grained comparison of pragmatic language understanding in humans and language models},
  url       = {http://arxiv.org/abs/2212.06801},
  abstract  = {Pragmatics and non-literal language understanding are essential to human communication, and present a long-standing challenge for artificial language models. We perform a finegrained comparison of language models and humans on seven pragmatic phenomena, using zero-shot prompting on an expert-curated set of English materials. We ask whether models (1) select pragmatic interpretations of speaker utterances, (2) make similar error patterns as humans, and (3) use similar linguistic cues as humans to solve the tasks. We find that the largest models achieve high accuracy and match human error patterns: within incorrect responses, models favor literal interpretations over heuristic-based distractors. We also find preliminary evidence that models and humans are sensitive to similar linguistic cues. Our results suggest that pragmatic behaviors can emerge in models without explicitly constructed representations of mental states. However, models tend to struggle with phenomena relying on social expectation violations.},
  language  = {en},
  urldate   = {2023-09-15},
  publisher = {arXiv},
  author    = {Hu, Jennifer and Floyd, Sammy and Jouravlev, Olessia and Fedorenko, Evelina and Gibson, Edward},
  month     = may,
  year      = {2023},
  note      = {arXiv:2212.06801 [cs]},
  keywords  = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language}
}

@inproceedings{huang_overview_1993,
  title     = {An {Overview} of the {SPHINX}-{II} {Speech} {Recognition} {System}},
  url       = {https://aclanthology.org/H93-1016},
  urldate   = {2023-10-16},
  booktitle = {Human {Language} {Technology}: {Proceedings} of a {Workshop} {Held} at {Plainsboro}, {New} {Jersey}, {March} 21-24, 1993},
  author    = {Huang, Xuedong and Alleva, Fileno and Hwang, Mei-Yuh and Rosenfeld, Ronald},
  year      = {1993}
}

@misc{hui_rot_2024,
  title      = {{RoT}: {Enhancing} {Large} {Language} {Models} with {Reflection} on {Search} {Trees}},
  shorttitle = {{RoT}},
  url        = {http://arxiv.org/abs/2404.05449},
  abstract   = {Large language models (LLMs) have demonstrated impressive capability in reasoning and planning when integrated with tree-search-based prompting methods. However, since these methods ignore the previous search experiences, they often make the same mistakes in the search process. To address this issue, we introduce Reflection on search Trees (RoT), an LLM reflection framework designed to improve the performance of tree-search-based prompting methods. It uses a strong LLM to summarize guidelines from previous tree search experiences to enhance the ability of a weak LLM. The guidelines are instructions about solving this task through tree search which can prevent the weak LLMs from making similar mistakes in the past search process. In addition, we proposed a novel state selection method, which identifies the critical information from historical search processes to help RoT generate more specific and meaningful guidelines. In our extensive experiments, we find that RoT significantly improves the performance of LLMs in reasoning or planning tasks with various tree-search-based prompting methods (e.g., BFS and MCTS). Non-tree-search-based prompting methods such as Chain-of-Thought (CoT) can also benefit from RoT guidelines since RoT can provide task-specific knowledge collected from the search experience.},
  urldate    = {2024-04-25},
  publisher  = {arXiv},
  author     = {Hui, Wenyang and Jiang, Chengyue and Wang, Yan and Tu, Kewei},
  month      = apr,
  year       = {2024},
  note       = {arXiv:2404.05449 [cs]},
  keywords   = {Computer Science - Computation and Language}
}

@article{INR-019,
  author     = {Robertson, Stephen and Zaragoza, Hugo},
  title      = {The Probabilistic Relevance Framework: BM25 and Beyond},
  year       = {2009},
  issue_date = {April 2009},
  publisher  = {Now Publishers Inc.},
  address    = {Hanover, MA, USA},
  volume     = {3},
  number     = {4},
  issn       = {1554-0669},
  url        = {https://doi.org/10.1561/1500000019},
  doi        = {10.1561/1500000019},
  abstract   = {The Probabilistic Relevance Framework (PRF) is a formal framework for document retrieval, grounded in work done in the 1970—1980s, which led to the development of one of the most successful text-retrieval algorithms, BM25. In recent years, research in the PRF has yielded new retrieval models capable of taking into account document meta-data (especially structure and link-graph information). Again, this has led to one of the most successful Web-search and corporate-search algorithms, BM25F. This work presents the PRF from a conceptual point of view, describing the probabilistic modelling assumptions behind the framework and the different ranking algorithms that result from its application: the binary independence model, relevance feedback models, BM25 and BM25F. It also discusses the relation between the PRF and other statistical models for IR, and covers some related topics, such as the use of non-textual features, and parameter optimisation for models with free parameters.},
  journal    = {Found. Trends Inf. Retr.},
  month      = apr,
  pages      = {333–389},
  numpages   = {57}
}

@inproceedings{iseli_age-and_2006,
  title    = {Age-and {Gender}-{Dependent} {Analysis} of {Voice} {Source} {Characteristics}},
  volume   = {1},
  doi      = {10.1109/ICASSP.2006.1660039},
  abstract = {The effects of age, gender, and vocal tract configurations on the glottal excitation signal are still only partially understood. In this paper we examine some of these effects, and show that the voice source parameters, such as fundamental frequency (Fo), open quotient (related to H*1 - H*2), and spectral tilt (related to H*1 - A*3) are not only affected by age and gender but are also intercorrelated (the asterisk superscript denotes correction for the influence of various formants). Recordings of 92 male and female speakers from three age groups (8, 15, 20-39) are analyzed. The main observations are: for low-pitched talkers H*1 - H* 2 (hence, the open quotient) is proportional to Fo, while for high-pitched talkers H*1 \$H*2 is proportional to F1 (high to low vowels) for F1 {\textless} 700 Hz. The parameter H*1 - A*3 showed a strong dependence on F2 and F3 for all talkers and age groups: increasing F2 or F3 yielded an increase in H*1 - A*3. Spectral tilt was seen to be vowel dependent and for male talkers, spectral tilt changed dramatically with age. A better understanding of the dependencies of voice source parameters on age and gender will help improve voice source parameter estimation and analysis for a variety of speech processing and medical applications},
  author   = {Iseli, Markus and Shue, Yen-Liang and Alwan, Abeer},
  month    = jun,
  year     = {2006},
  pages    = {I--I}
}


%% The Retraction Watch Database [Internet]. New York: The Center for Scientific Integrity. 2018. ISSN: 2692-4579. [Cited (applicable date)]. Available from: http://retractiondatabase.org/.

@article{jacewicz_articulation_2009,
  title    = {Articulation rate across dialect, age, and gender},
  volume   = {21},
  issn     = {0954-3945},
  url      = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2790192/},
  doi      = {10.1017/S0954394509990093},
  abstract = {The understanding of sociolinguistic variation is growing rapidly, but basic gaps still remain. Whether some languages or dialects are spoken faster or slower than others constitutes such a gap. Speech tempo is interconnected with social, physical and psychological markings of speech. This study examines regional variation in articulation rate and its manifestations across speaker age, gender and speaking situations (reading vs. free conversation). The results of an experimental investigation show that articulation rate differs significantly between two regional varieties of American English examined here. A group of Northern speakers (from Wisconsin) spoke significantly faster than a group of Southern speakers (from North Carolina). With regard to age and gender, young adults read faster than older adults in both regions; in free speech, only Northern young adults spoke faster than older adults. Effects of gender were smaller and less consistent; men generally spoke slightly faster than women. As the body of work on the sociophonetics of American English continues to grow in scope and depth, we argue that it is important to include fundamental phonetic information as part of our catalog of regional differences and patterns of change in American English.},
  number   = {2},
  urldate  = {2023-10-26},
  journal  = {Language variation and change},
  author   = {Jacewicz, Ewa and Fox, Robert A. and O’Neill, Caitlin and Salmons, Joseph},
  month    = jul,
  year     = {2009},
  pmid     = {20161445},
  pmcid    = {PMC2790192},
  pages    = {233--256}
}



@misc{jeretic_are_2020,
  title      = {Are {Natural} {Language} {Inference} {Models} {IMPPRESsive}? {Learning} {IMPlicature} and {PRESupposition}},
  shorttitle = {Are {Natural} {Language} {Inference} {Models} {IMPPRESsive}?},
  url        = {http://arxiv.org/abs/2004.03066},
  abstract   = {Natural language inference (NLI) is an increasingly important task for natural language understanding, which requires one to infer whether a sentence entails another. However, the ability of NLI models to make pragmatic inferences remains understudied. We create an IMPlicature and PRESupposition diagnostic dataset (IMPPRES), consisting of {\textgreater}25k semiautomatically generated sentence pairs illustrating well-studied pragmatic inference types. We use IMPPRES to evaluate whether BERT, InferSent, and BOW NLI models trained on MultiNLI (Williams et al., 2018) learn to make pragmatic inferences. Although MultiNLI appears to contain very few pairs illustrating these inference types, we ﬁnd that BERT learns to draw pragmatic inferences. It reliably treats scalar implicatures triggered by “some” as entailments. For some presupposition triggers like only, BERT reliably recognizes the presupposition as an entailment, even when the trigger is embedded under an entailment canceling operator like negation. BOW and InferSent show weaker evidence of pragmatic reasoning. We conclude that NLI training encourages models to learn some, but not all, pragmatic inferences.},
  language   = {en},
  urldate    = {2023-09-15},
  publisher  = {arXiv},
  author     = {Jeretic, Paloma and Warstadt, Alex and Bhooshan, Suvrat and Williams, Adina},
  month      = jul,
  year       = {2020},
  note       = {arXiv:2004.03066 [cs]},
  keywords   = {Computer Science - Computation and Language}
}

@misc{jin_integrating_2023,
  title      = {Integrating {AI} {Planning} with {Natural} {Language} {Processing}: {A} {Combination} of {Explicit} and {Tacit} {Knowledge}},
  shorttitle = {Integrating {AI} {Planning} with {Natural} {Language} {Processing}},
  url        = {http://arxiv.org/abs/2202.07138},
  abstract   = {Natural language processing (NLP) aims at investigating the interactions between agents and humans, processing and analyzing large amounts of natural language data. Large-scale language models play an important role in current natural language processing. However, the challenges of explainability and complexity come along with the developments of language models. One way is to introduce logical relations and rules into natural language processing models, such as making use of Automated Planning. Automated planning (AI planning) focuses on building symbolic domain models and synthesizing plans to transit initial states to goals based on domain models. Recently, there have been plenty of works related to these two fields, which have the abilities to generate explicit knowledge, e.g., preconditions and effects of action models, and learn from tacit knowledge, e.g., neural models, respectively. Integrating AI planning and natural language processing effectively improves the communication between human and intelligent agents. This paper outlines the commons and relations between AI planning and natural language processing, argues that each of them can effectively impact on the other one by five areas: (1) planning-based text understanding, (2) planning-based natural language processing, (3) planning-based explainability, (4) text-based human-robot interaction, and (5) applications. We also explore some potential future issues between AI planning and natural language processing. To the best of our knowledge, this survey is the first work that addresses the deep connections between AI planning and Natural language processing. CCS Concepts: • Computing methodologies → Natural language processing; Planning and scheduling; Information extraction; Natural language generation.},
  language   = {en},
  urldate    = {2023-09-15},
  publisher  = {arXiv},
  author     = {Jin, Kebing and Zhuo, Hankz Hankui},
  month      = apr,
  year       = {2023},
  note       = {arXiv:2202.07138 [cs]},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language}
}

@misc{kaddour_challenges_2023,
  title     = {Challenges and {Applications} of {Large} {Language} {Models}},
  url       = {http://arxiv.org/abs/2307.10169},
  abstract  = {Large Language Models (LLMs) went from non-existent to ubiquitous in the machine learning discourse within a few years. Due to the fast pace of the field, it is difficult to identify the remaining challenges and already fruitful application areas. In this paper, we aim to establish a systematic set of open problems and application successes so that ML researchers can comprehend the field’s current state more quickly and become productive.},
  language  = {en},
  urldate   = {2023-09-15},
  publisher = {arXiv},
  author    = {Kaddour, Jean and Harris, Joshua and Mozes, Maximilian and Bradley, Herbie and Raileanu, Roberta and McHardy, Robert},
  month     = jul,
  year      = {2023},
  note      = {arXiv:2307.10169 [cs]},
  keywords  = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning}
}

@article{kanoulas_clef_2019,
  title      = {{CLEF} 2019 technology assisted reviews in empirical medicine overview: 20th {Working} {Notes} of {CLEF} {Conference} and {Labs} of the {Evaluation} {Forum}, {CLEF} 2019},
  volume     = {2380},
  issn       = {1613-0073},
  shorttitle = {{CLEF} 2019 technology assisted reviews in empirical medicine overview},
  url        = {http://ceur-ws.org/Vol-2380/},
  abstract   = {Systematic reviews are a widely used method to provide an overview over the current scientific consensus, by bringing together multiple studies in a systematic, reliable, and transparent way. The large and growing number of published studies, and their increasing rate of publication, makes the task of identifying all relevant studies in an unbiased way both complex and time consuming to the extent that jeopardizes the validity of their findings and the ability to inform policy and practice in a timely manner. The CLEF 2019 e-Health TAR Lab accommodated two tasks. Task 1 focused on retrieving relevant studies from PubMed without the use of a Boolean query, while Task 2 focused on the efficient and effective ranking of studies during the abstract and title screening phase of conducting a systematic review. In the 2019 lab we also expanded upon the type of systematics reviews considered. Hence, beyond Diagnostic Test Accuracy reviews, we also included Intervention, Prognosis, and Qualitative systematic reviews. We constructed a benchmark collection of 31 reviews published by Cochrane, and the corresponding relevant and irrelevant articles found by the original Boolean query. Three teams participated in Task 2, submitting automatic and semi-automatic runs, using information retrieval and machine learning algorithms over a variety of text representations, in a batch and iterative manner. This paper reports both the methodology used to construct the benchmark collection, and the results of the evaluation.},
  number     = {250},
  urldate    = {2024-04-25},
  journal    = {CEUR Workshop Proceedings},
  author     = {Kanoulas, Evangelos and Li, Dan and Azzopardi, Leif and Spijker, Rene},
  month      = sep,
  year       = {2019},
  keywords   = {Information Retrieval, TAR, active learning, evaluation, systematic reviews, text classification}
}

@article{kearney_research_2024,
  title      = {Research integrity and academic medicine: the pressure to publish and research misconduct},
  volume     = {124},
  issn       = {2702-3648},
  shorttitle = {Research integrity and academic medicine},
  doi        = {10.1515/jom-2023-0211},
  abstract   = {CONTEXT: This narrative review article explores research integrity and the implications of scholarly work in medical education. The paper describes how the current landscape of medical education emphasizes research and scholarly activity for medical students, resident physicians, and faculty physician educators. There is a gap in the existing literature that fully explores research integrity, the challenges surrounding the significant pressure to perform scholarly activity, and the potential for ethical lapses by those involved in medical education.
                OBJECTIVES: The objectives of this review article are to provide a background on authorship and publication safeguards, outline common types of research misconduct, describe the implications of publication in medical education, discuss the consequences of ethical breaches, and outline possible solutions to promote research integrity in academic medicine.
                METHODS: To complete this narrative review, the authors explored the current literature utilizing multiple databases beginning in June of 2021, and they completed the literature review in January of 2023. To capture the wide scope of the review, numerous searches were performed. A number of Medical Subject Headings (MeSH) terms were utilized to identify relevant articles. The MeSH terms included "scientific misconduct," "research misconduct," "authorship," "plagiarism," "biomedical research/ethics," "faculty, medical," "fellowships and scholarships," and "internship and residency." Additional references were accessed to include medical school and residency accreditation standards, residency match statistics, regulatory guidelines, and standard definitions.
                RESULTS: Within the realm of academic medicine, research misconduct and misrepresentation continue to occur without clear solutions. There is a wide range of severity in breaches of research integrity, ranging from minor infractions to fraud. Throughout the medical education system in the United States, there is pressure to publish research and scholarly work. Higher rates of publications are associated with a successful residency match for students and academic promotion for faculty physicians. For those who participate in research misconduct, there is a multitude of potential adverse consequences. Potential solutions to ensure research integrity exist but are not without barriers to implementation.
                CONCLUSIONS: Pressure in the world of academic medicine to publish contributes to the potential for research misconduct and authorship misrepresentation. Lapses in research integrity can result in a wide range of potentially adverse consequences for the offender, their institution, the scientific community, and the public. If adopted, universal research integrity policies and procedures could make major strides in eliminating research misconduct in the realm of academic medicine.},
  language   = {eng},
  number     = {5},
  journal    = {Journal of Osteopathic Medicine},
  author     = {Kearney, Molly and Downing, Maren and Gignac, Elizabeth A.},
  month      = may,
  year       = {2024},
  pmid       = {38407191},
  keywords   = {authorship, Authorship, Biomedical Research, Education, Medical, Ethics, Research, Humans, internship, medical education, plagiarism, Publishing, research integrity, residency, Scientific Misconduct},
  pages      = {187--194},
  file       = {Full Text:/home/aaronfletcher/snap/zotero-snap/common/Zotero/storage/NZKQFLSR/Kearney et al. - 2024 - Research integrity and academic medicine the pressure to publish and research misconduct.pdf:application/pdf}
}

@article{keshav_how_2007,
  title    = {How to read a paper},
  volume   = {37},
  issn     = {0146-4833},
  url      = {https://dl.acm.org/doi/10.1145/1273445.1273458},
  doi      = {10.1145/1273445.1273458},
  abstract = {Researchers spend a great deal of time reading research papers. However, this skill is rarely taught, leading to much wasted effort. This article outlines a practical and efficient
              three-pass method
              for reading research papers. I also describe how to use this method to do a literature survey.},
  language = {en},
  number   = {3},
  urldate  = {2023-10-02},
  journal  = {ACM SIGCOMM Computer Communication Review},
  author   = {Keshav, S.},
  month    = jul,
  year     = {2007},
  pages    = {83--84}
}

@misc{krishnan_mitigating_2021,
  title     = {Mitigating {Sampling} {Bias} and {Improving} {Robustness} in {Active} {Learning}},
  url       = {http://arxiv.org/abs/2109.06321},
  doi       = {10.48550/arXiv.2109.06321},
  abstract  = {This paper presents simple and efficient methods to mitigate sampling bias in active learning while achieving state-of-the-art accuracy and model robustness. We introduce supervised contrastive active learning by leveraging the contrastive loss for active learning under a supervised setting. We propose an unbiased query strategy that selects informative data samples of diverse feature representations with our methods: supervised contrastive active learning (SCAL) and deep feature modeling (DFM). We empirically demonstrate our proposed methods reduce sampling bias, achieve state-of-the-art accuracy and model calibration in an active learning setup with the query computation 26x faster than Bayesian active learning by disagreement and 11x faster than CoreSet. The proposed SCAL method outperforms by a big margin in robustness to dataset shift and out-of-distribution.},
  urldate   = {2024-06-13},
  publisher = {arXiv},
  author    = {Krishnan, Ranganath and Sinha, Alok and Ahuja, Nilesh and Subedar, Mahesh and Tickoo, Omesh and Iyer, Ravi},
  month     = sep,
  year      = {2021},
  note      = {arXiv:2109.06321 [cs]},
  keywords  = {Computer Science - Machine Learning}
}

@article{Kuhberger2022-if,
  title        = {Self-correction in science: The effect of retraction on the frequency of citations},
  volume       = {17},
  issn         = {1932-6203},
  doi          = {10.1371/journal.pone.0277814},
  shorttitle   = {Self-correction in science},
  abstract     = {We investigate the citation frequency of retracted scientific papers in science. For the period of five years before and after retraction, we counted the citations to papers in a sample of over 3,000 retracted, and a matched sample of another 3,000 non-retracted papers. Retraction led to a decrease in average annual citation frequency from about 5 before, to 2 citations after retraction. In contrast, for non-retracted control papers the citation counts were 4, and 5, respectively. Put differently, we found only a limited effect of retraction: retraction decreased citation frequency only by about 60\%, as compared to non-retracted papers. Thus, retracted papers often live on. For effective self-correction the scientific enterprise needs to be more effective in removing retracted papers from the scientific record. We discuss recent proposals to do so.},
  pages        = {e0277814},
  number       = {12},
  journal      = {{PloS} One},
  shortjournal = {{PLoS} One},
  author       = {Kühberger, Anton and Streit, Daniel and Scherndl, Thomas},
  date         = {2022},
  pmid         = {36477092},
  pmcid        = {PMC9728909},
  file         = {Full Text:/home/aaron/snap/zotero-snap/common/Zotero/storage/WHB9KZTV/Kühberger et al. - 2022 - Self-correction in science The effect of retraction on the frequency of citations.pdf:application/pdf}
}

@article{lenci_understanding_nodate,
  title    = {Understanding {Natural} {Language} {Understanding} {Systems}. {A} {Critical} {Analysis}},
  language = {en},
  author   = {Lenci, Alessandro}
}

@misc{lewis_retrieval-augmented_2021,
  title     = {Retrieval-{Augmented} {Generation} for {Knowledge}-{Intensive} {NLP} {Tasks}},
  url       = {http://arxiv.org/abs/2005.11401},
  doi       = {10.48550/arXiv.2005.11401},
  abstract  = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
  urldate   = {2023-09-14},
  publisher = {arXiv},
  author    = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Küttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rocktäschel, Tim and Riedel, Sebastian and Kiela, Douwe},
  month     = apr,
  year      = {2021},
  note      = {arXiv:2005.11401 [cs]},
  keywords  = {Computer Science - Computation and Language, Computer Science - Machine Learning}
}

@article{li_when_2020,
  title      = {When to {Stop} {Reviewing} in {Technology}-{Assisted} {Reviews}: {Sampling} from an {Adaptive} {Distribution} to {Estimate} {Residual} {Relevant} {Documents}},
  volume     = {38},
  issn       = {1046-8188},
  shorttitle = {When to {Stop} {Reviewing} in {Technology}-{Assisted} {Reviews}},
  url        = {https://dl.acm.org/doi/10.1145/3411755},
  doi        = {10.1145/3411755},
  abstract   = {Technology-Assisted Reviews (TAR) aim to expedite document reviewing (e.g., medical articles or legal documents) by iteratively incorporating machine learning algorithms and human feedback on document relevance. Continuous Active Learning (CAL) algorithms have demonstrated superior performance compared to other methods in efficiently identifying relevant documents. One of the key challenges for CAL algorithms is deciding when to stop displaying documents to reviewers. Existing work either lacks transparency—it provides an ad-hoc stopping point, without indicating how many relevant documents are still not found, or lacks efficiency by paying an extra cost to estimate the total number of relevant documents in the collection prior to the actual review. In this article, we handle the problem of deciding the stopping point of TAR under the continuous active learning framework by jointly training a ranking model to rank documents, and by conducting a “greedy” sampling to estimate the total number of relevant documents in the collection. We prove the unbiasedness of the proposed estimators under a with-replacement sampling design, while experimental results demonstrate that the proposed approach, similar to CAL, effectively retrieves relevant documents; but it also provides a transparent, accurate, and effective stopping point.},
  number     = {4},
  urldate    = {2024-06-13},
  journal    = {ACM Transactions on Information Systems},
  author     = {Li, Dan and Kanoulas, Evangelos},
  month      = sep,
  year       = {2020},
  keywords   = {Total recall, active sampling, unbiased estimator},
  pages      = {41:1--41:36}
}

@article{li2022survey,
  author     = {Li, Qian and Peng, Hao and Li, Jianxin and Xia, Congying and Yang, Renyu and Sun, Lichao and Yu, Philip S. and He, Lifang},
  title      = {A Survey on Text Classification: From Traditional to Deep Learning},
  year       = {2022},
  issue_date = {April 2022},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {13},
  number     = {2},
  issn       = {2157-6904},
  url        = {https://doi.org/10.1145/3495162},
  doi        = {10.1145/3495162},
  abstract   = {Text classification is the most fundamental and essential task in natural language processing. The last decade has seen a surge of research in this area due to the unprecedented success of deep learning. Numerous methods, datasets, and evaluation metrics have been proposed in the literature, raising the need for a comprehensive and updated survey. This paper fills the gap by reviewing the state-of-the-art approaches from 1961 to 2021, focusing on models from traditional models to deep learning. We create a taxonomy for text classification according to the text involved and the models used for feature extraction and classification. We then discuss each of these categories in detail, dealing with both the technical developments and benchmark datasets that support tests of predictions. A comprehensive comparison between different techniques, as well as identifying the pros and cons of various evaluation metrics are also provided in this survey. Finally, we conclude by summarizing key implications, future research directions, and the challenges facing the research area.},
  journal    = {ACM Trans. Intell. Syst. Technol.},
  month      = apr,
  articleno  = {31},
  numpages   = {41},
  keywords   = {Deep learning, traditional models, text classification, evaluation metrics, challenges}
}

@misc{liesenfeld_timing_2023,
  title      = {The timing bottleneck: {Why} timing and overlap are mission-critical for conversational user interfaces, speech recognition and dialogue systems},
  shorttitle = {The timing bottleneck},
  url        = {http://arxiv.org/abs/2307.15493},
  abstract   = {Speech recognition systems are a key intermediary in voice-driven human-computer interaction. Although speech recognition works well for pristine monologic audio, real-life use cases in open-ended interactive settings still present many challenges. We argue that timing is mission-critical for dialogue systems, and evaluate 5 major commercial ASR systems for their conversational and multilingual support. We find that word error rates for natural conversational data in 6 languages remain abysmal, and that overlap remains a key challenge (study 1). This impacts especially the recognition of conversational words (study 2), and in turn has dire consequences for downstream intent recognition (study 3). Our findings help to evaluate the current state of conversational ASR, contribute towards multidimensional error analysis and evaluation, and identify phenomena that need most attention on the way to build robust interactive speech technologies.},
  language   = {en},
  urldate    = {2023-09-15},
  publisher  = {arXiv},
  author     = {Liesenfeld, Andreas and Lopez, Alianda and Dingemanse, Mark},
  month      = jul,
  year       = {2023},
  note       = {arXiv:2307.15493 [cs]},
  keywords   = {Computer Science - Computation and Language}
}

@misc{liu_testing_2022,
  title     = {Testing the {Ability} of {Language} {Models} to {Interpret} {Figurative} {Language}},
  url       = {http://arxiv.org/abs/2204.12632},
  abstract  = {Figurative and metaphorical language are commonplace in discourse, and figurative expressions play an important role in communication and cognition. However, figurative language has been a relatively under-studied area in NLP, and it remains an open question to what extent modern language models can interpret nonliteral phrases. To address this question, we introduce Fig-QA, a Winograd-style nonliteral language understanding task consisting of correctly interpreting paired figurative phrases with divergent meanings. We evaluate the performance of several state-of-the-art language models on this task, and find that although language models achieve performance significantly over chance, they still fall short of human performance, particularly in zero- or few-shot settings. This suggests that further work is needed to improve the nonliteral reasoning capabilities of language models.},
  language  = {en},
  urldate   = {2023-09-15},
  publisher = {arXiv},
  author    = {Liu, Emmy and Cui, Chen and Zheng, Kenneth and Neubig, Graham},
  month     = may,
  year      = {2022},
  note      = {arXiv:2204.12632 [cs]},
  keywords  = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language}
}

@inproceedings{manakul_long-span_2021,
  address   = {Online},
  title     = {Long-{Span} {Summarization} via {Local} {Attention} and {Content} {Selection}},
  url       = {https://aclanthology.org/2021.acl-long.470},
  doi       = {10.18653/v1/2021.acl-long.470},
  abstract  = {Transformer-based models have achieved state-of-the-art results in a wide range of natural language processing (NLP) tasks including document summarization. Typically these systems are trained by fine-tuning a large pre-trained model to the target task. One issue with these transformer-based models is that they do not scale well in terms of memory and compute requirements as the input length grows. Thus, for long document summarization, it can be challenging to train or fine-tune these models. In this work, we exploit large pre-trained transformer-based models and address long-span dependencies in abstractive summarization using two methods: local self-attention; and explicit content selection. These approaches are compared on a range of network configurations. Experiments are carried out on standard long-span summarization tasks, including Spotify Podcast, arXiv, and PubMed datasets. We demonstrate that by combining these methods, we can achieve state-of-the-art results on all three tasks in the ROUGE scores. Moreover, without a large-scale GPU card, our approach can achieve comparable or better results than existing approaches.},
  urldate   = {2024-02-27},
  booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
  publisher = {Association for Computational Linguistics},
  author    = {Manakul, Potsawee and Gales, Mark},
  editor    = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
  month     = aug,
  year      = {2021},
  pages     = {6026--6041}
}

@inproceedings{mao_reproducibility_2024,
  address    = {Cham},
  title      = {A {Reproducibility} {Study} of {Goldilocks}: {Just}-{Right} {Tuning} of {BERT} for {TAR}},
  isbn       = {978-3-031-56066-8},
  shorttitle = {A {Reproducibility} {Study} of {Goldilocks}},
  doi        = {10.1007/978-3-031-56066-8_13},
  abstract   = {Screening documents is a tedious and time-consuming aspect of high-recall retrieval tasks, such as compiling a systematic literature review, where the goal is to identify all relevant documents for a topic. To help streamline this process, many Technology-Assisted Review (TAR) methods leverage active learning techniques to reduce the number of documents requiring review. BERT-based models have shown high effectiveness in text classification, leading to interest in their potential use in TAR workflows. In this paper, we investigate recent work that examined the impact of further pre-training epochs on the effectiveness and efficiency of a BERT-based active learning pipeline. We first report that we could replicate the original experiments on two specific TAR datasets, confirming some of the findings: importantly, that further pre-training is critical to high effectiveness, but requires attention in terms of selecting the correct training epoch. We then investigate the generalisability of the pipeline on a different TAR task, that of medical systematic reviews. In this context, we show that there is no need for further pre-training if a domain-specific BERT backbone is used within the active learning pipeline. This finding provides practical implications for using the studied active learning pipeline within domain-specific TAR tasks.},
  language   = {en},
  booktitle  = {Advances in {Information} {Retrieval}},
  publisher  = {Springer Nature Switzerland},
  author     = {Mao, Xinyu and Koopman, Bevan and Zuccon, Guido},
  editor     = {Goharian, Nazli and Tonellotto, Nicola and He, Yulan and Lipani, Aldo and McDonald, Graham and Macdonald, Craig and Ounis, Iadh},
  year       = {2024},
  keywords   = {Active Learning, Systematic Reviews, Technology-Assisted Review (TAR)},
  pages      = {132--146}
}

@article{marshall_machine_2018,
  title      = {Machine learning for identifying {Randomized} {Controlled} {Trials}: {An} evaluation and practitioner's guide},
  volume     = {9},
  issn       = {1759-2879, 1759-2887},
  shorttitle = {Machine learning for identifying {Randomized} {Controlled} {Trials}},
  url        = {https://onlinelibrary.wiley.com/doi/10.1002/jrsm.1287},
  doi        = {10.1002/jrsm.1287},
  abstract   = {Machine learning (ML) algorithms have proven highly accurate for identifying Randomized Controlled Trials (RCTs) but are not used much in practice, in part because the best way to make use of the technology in a typical workflow is unclear. In this work, we evaluate ML models for RCT classification (support vector machines, convolutional neural networks, and ensemble approaches). We trained and optimized support vector machine and convolutional neural network models on the titles and abstracts of the
                Cochrane Crowd
                RCT set. We evaluated the models on an external dataset (Clinical Hedges), allowing direct comparison with traditional database search filters. We estimated area under receiver operating characteristics (AUROC) using the Clinical Hedges dataset.
                
                We demonstrate that ML approaches better discriminate between RCTs and non‐RCTs than widely used traditional database search filters at all sensitivity levels; our best‐performing model also achieved the best results to date for ML in this task (AUROC 0.987, 95\% CI, 0.984‐0.989). We provide practical guidance on the role of ML in (1) systematic reviews (high‐sensitivity strategies) and (2) rapid reviews and clinical question answering (high‐precision strategies) together with recommended probability cutoffs for each use case. Finally, we provide open‐source software to enable these approaches to be used in practice.},
  language   = {en},
  number     = {4},
  urldate    = {2024-03-18},
  journal    = {Research Synthesis Methods},
  author     = {Marshall, Iain J. and Noel‐Storr, Anna and Kuiper, Joël and Thomas, James and Wallace, Byron C.},
  month      = dec,
  year       = {2018},
  pages      = {602--614}
}

@article{mehrabi2021survey,
  author     = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
  title      = {A Survey on Bias and Fairness in Machine Learning},
  year       = {2021},
  issue_date = {July 2022},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {54},
  number     = {6},
  issn       = {0360-0300},
  url        = {https://doi.org/10.1145/3457607},
  doi        = {10.1145/3457607},
  abstract   = {With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.},
  journal    = {ACM Comput. Surv.},
  month      = jul,
  articleno  = {115},
  numpages   = {35},
  keywords   = {representation learning, natural language processing, machine learning, deep learning, Fairness and bias in artificial intelligence}
}

@misc{mikolov_efficient_2013,
  title     = {Efficient {Estimation} of {Word} {Representations} in {Vector} {Space}},
  url       = {http://arxiv.org/abs/1301.3781},
  abstract  = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
  urldate   = {2024-01-22},
  publisher = {arXiv},
  author    = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  month     = sep,
  year      = {2013},
  note      = {arXiv:1301.3781 [cs]},
  keywords  = {Computer Science - Computation and Language}
}

@article{minaee_deep_2021,
  title      = {Deep {Learning}--based {Text} {Classification}: {A} {Comprehensive} {Review}},
  volume     = {54},
  issn       = {0360-0300},
  shorttitle = {Deep {Learning}--based {Text} {Classification}},
  url        = {https://doi.org/10.1145/3439726},
  doi        = {10.1145/3439726},
  abstract   = {Deep learning--based models have surpassed classical machine learning--based approaches in various text classification tasks, including sentiment analysis, news categorization, question answering, and natural language inference. In this article, we provide a comprehensive review of more than 150 deep learning--based models for text classification developed in recent years, and we discuss their technical contributions, similarities, and strengths. We also provide a summary of more than 40 popular datasets widely used for text classification. Finally, we provide a quantitative analysis of the performance of different deep learning models on popular benchmarks, and we discuss future research directions.},
  number     = {3},
  journal    = {ACM Comput. Surv.},
  author     = {Minaee, Shervin and Kalchbrenner, Nal and Cambria, Erik and Nikzad, Narjes and Chenaghlu, Meysam and Gao, Jianfeng},
  month      = apr,
  year       = {2021},
  pages      = {62:1--62:40}
}

@inproceedings{molinari_active_2022,
  title      = {Active {Learning} and the {Saerens}-{Latinne}-{Decaestecker} {Algorithm}: {An} {Evaluation}},
  shorttitle = {Active {Learning} and the {Saerens}-{Latinne}-{Decaestecker} {Algorithm}},
  url        = {https://openreview.net/forum?id=ITu3RkBqdO},
  language   = {en},
  urldate    = {2024-06-13},
  author     = {Molinari, Alessio and Esuli, Andrea and Sebastiani, Fabrizio},
  month      = jan,
  year       = {2022}
}

@misc{molinari_active_2022-1,
  title      = {Active {Learning} and the {Saerens}-{Latinne}-{Decaestecker} {Algorithm}: {An} {Evaluation}},
  shorttitle = {Active {Learning} and the {Saerens}-{Latinne}-{Decaestecker} {Algorithm}},
  url        = {https://zenodo.org/records/7090015},
  abstract   = {The Saerens-Latinne-Decaestecker (SLD) algorithm is a method whose goal is improving the quality of the posterior probabilities (or simply “posteriors”) returned by a probabilistic classifier in scenarios characterized by prior probability shift (PPS) between the training set and the unlabelled (“test”) set. This is an important task, (a) because posteriors are of the utmost importance in downstream tasks such as, e.g., multiclass classification and cost-sensitive classification, and (b) because PPS is ubiquitous in many applications. In this paper we explore whether using SLD can indeed improve the quality of posteriors returned by a classifier trained via active learning (AL), a class of machine learning (ML) techniques that indeed tend to generate substantial PPS. Specifically, we target AL via relevance sampling (ALvRS) and AL via uncertainty sampling (ALvUS), two AL techniques that are very well-known especially because, due to their low computational cost, are suitable to being applied in scenarios characterized by large datasets. We present experimental results obtained on the RCV1-v2 dataset, showing that SLD fails to deliver better-quality posteriors with both ALvRS and ALvUS, thus contradicting previous findings in the literature, and that this is due not to the amount of PPS that these techniques generate, but to how the examples they prioritize for annotation are distributed.},
  urldate    = {2024-06-13},
  author     = {Molinari, Alessio and Esuli, Andrea and Sebastiani, Fabrizio},
  month      = jul,
  year       = {2022},
  doi        = {10.5281/zenodo.7090015},
  note       = {Publisher: Zenodo}
}

@article{molinari_active_nodate,
  title    = {Active {Learning} {\textbackslash}{\textbackslash} and the {Saerens}-{Latinne}-{Decaestecker} {Algorithm}: {\textbackslash}{\textbackslash} {An} {Evaluation}},
  abstract = {The Saerens-Latinne-Decaestecker (SLD) algorithm is a method whose goal is improving the quality of the posterior probabilities (or simply “posteriors”) returned by a probabilistic classifier in scenarios characterized by prior probability shift (PPS) between the training set and the unlabelled (“test”) set. This is an important task, (a) because posteriors are of the utmost importance in downstream tasks such as, e.g., multiclass classification and cost-sensitive classification, and (b) because PPS is ubiquitous in many applications. In this paper we explore whether using SLD can indeed improve the quality of posteriors returned by a classifier trained via active learning (AL), a class of machine learning (ML) techniques that indeed tend to generate substantial PPS. Specifically, we target AL via relevance sampling (ALvRS) and AL via uncertainty sampling (ALvUS), two AL techniques that are very well-known especially because, due to their low computational cost, are suitable to being applied in scenarios characterized by large datasets. We present experimental results obtained on the RCV1-v2 dataset, showing that SLD fails to deliver better-quality posteriors with both ALvRS and ALvUS, thus contradicting previous findings in the literature, and that this is due not to the amount of PPS that these techniques generate, but to how the examples they prioritize for annotation are distributed.},
  language = {en},
  author   = {Molinari, Alessio and Esuli, Andrea and Sebastiani, Fabrizio}
}

@article{molinari_active_nodate-1,
  title    = {Active {Learning} {\textbackslash}{\textbackslash} and the {Saerens}-{Latinne}-{Decaestecker} {Algorithm}: {\textbackslash}{\textbackslash} {An} {Evaluation}},
  abstract = {The Saerens-Latinne-Decaestecker (SLD) algorithm is a method whose goal is improving the quality of the posterior probabilities (or simply “posteriors”) returned by a probabilistic classifier in scenarios characterized by prior probability shift (PPS) between the training set and the unlabelled (“test”) set. This is an important task, (a) because posteriors are of the utmost importance in downstream tasks such as, e.g., multiclass classification and cost-sensitive classification, and (b) because PPS is ubiquitous in many applications. In this paper we explore whether using SLD can indeed improve the quality of posteriors returned by a classifier trained via active learning (AL), a class of machine learning (ML) techniques that indeed tend to generate substantial PPS. Specifically, we target AL via relevance sampling (ALvRS) and AL via uncertainty sampling (ALvUS), two AL techniques that are very well-known especially because, due to their low computational cost, are suitable to being applied in scenarios characterized by large datasets. We present experimental results obtained on the RCV1-v2 dataset, showing that SLD fails to deliver better-quality posteriors with both ALvRS and ALvUS, thus contradicting previous findings in the literature, and that this is due not to the amount of PPS that these techniques generate, but to how the examples they prioritize for annotation are distributed.},
  language = {en},
  author   = {Molinari, Alessio and Esuli, Andrea and Sebastiani, Fabrizio}
}

@article{molinari_sal_2023,
  title      = {{SALτ}: efficiently stopping {TAR} by improving priors estimates},
  issn       = {1573-756X},
  shorttitle = {{SALτ}},
  url        = {https://doi.org/10.1007/s10618-023-00961-5},
  doi        = {10.1007/s10618-023-00961-5},
  abstract   = {In high recall retrieval tasks, human experts review a large pool of documents with the goal of satisfying an information need. Documents are prioritized for review through an active learning policy, and the process is usually referred to as Technology-Assisted Review (TAR). TAR tasks also aim to stop the review process once the target recall is achieved to minimize the annotation cost. In this paper, we introduce a new stopping rule called SAL\$\$\_{\textbackslash}tau {\textasciicircum}R\$\$(SLD for Active Learning), a modified version of the Saerens–Latinne–Decaestecker algorithm (SLD) that has been adapted for use in active learning. Experiments show that our algorithm stops the review well ahead of the current state-of-the-art methods, while providing the same guarantees of achieving the target recall.},
  language   = {en},
  urldate    = {2023-10-16},
  journal    = {Data Mining and Knowledge Discovery},
  author     = {Molinari, Alessio and Esuli, Andrea},
  month      = aug,
  year       = {2023},
  keywords   = {Active learning, Systematic review, TAR, Technology-assisted review, e-Discovery}
}

@article{mondal_chatgpt_2023,
  title      = {{ChatGPT} in academic writing: {Maximizing} its benefits and minimizing the risks},
  volume     = {71},
  issn       = {0301-4738},
  shorttitle = {{ChatGPT} in academic writing},
  url        = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10788737/},
  doi        = {10.4103/IJO.IJO_718_23},
  abstract   = {This review article explores the use of ChatGPT in academic writing and provides insights on how to utilize it judiciously. With the increasing popularity of AI-powered language models, ChatGPT has emerged as a potential tool for assisting writers in the research and writing process. We have provided a list of potential uses of ChatGPT by a novice researcher for getting help during research proposal preparation and manuscript writing. However, there are concerns regarding its reliability and potential risks associated with its use. The review highlights the importance of maintaining human judgment in the writing process and using ChatGPT as a complementary tool rather than a replacement for human effort. The article concludes with recommendations for researchers and writers to ensure responsible and effective use of ChatGPT in academic writing.},
  number     = {12},
  urldate    = {2024-05-28},
  journal    = {Indian Journal of Ophthalmology},
  author     = {Mondal, Himel and Mondal, Shaikat},
  month      = dec,
  year       = {2023},
  pmid       = {37991290},
  pmcid      = {PMC10788737},
  pages      = {3600--3606}
}

@misc{moore_talking_2019,
  title      = {Talking with {Robots}: {Opportunities} and {Challenges}},
  shorttitle = {Talking with {Robots}},
  url        = {http://arxiv.org/abs/1912.00369},
  doi        = {10.48550/arXiv.1912.00369},
  abstract   = {Notwithstanding the tremendous progress that is taking place in spoken language technology, effective speech-based human-robot interaction still raises a number of important challenges. Not only do the fields of robotics and spoken language technology present their own special problems, but their combination raises an additional set of issues. In particular, there is a large gap between the formulaic speech that typifies contemporary spoken dialogue systems and the flexible nature of human-human conversation. It is pointed out that grounded and situated speech-based human-robot interaction may lead to deeper insights into the pragmatics of language usage, thereby overcoming the current `habitability gap'.},
  urldate    = {2023-10-17},
  publisher  = {arXiv},
  author     = {Moore, Roger K.},
  month      = dec,
  year       = {2019},
  note       = {arXiv:1912.00369 [cs]},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction}
}

@article{moylan_why_2016,
  author       = {Moylan, Elizabeth C and Kowalczuk, Maria K},
  title        = {Why articles are retracted: a retrospective cross-sectional study of retraction notices at BioMed Central},
  volume       = {6},
  number       = {11},
  elocation-id = {e012047},
  year         = {2016},
  doi          = {10.1136/bmjopen-2016-012047},
  publisher    = {British Medical Journal Publishing Group},
  abstract     = {Objectives To assess why articles are retracted from BioMed Central journals, whether retraction notices adhered to the Committee on Publication Ethics (COPE) guidelines, and are becoming more frequent as a proportion of published articles.Design/setting Retrospective cross-sectional analysis of 134 retractions from January 2000 to December 2015.Results 134 retraction notices were published during this timeframe. Although they account for 0.07\% of all articles published (190 514 excluding supplements, corrections, retractions and commissioned content), the rate of retraction is rising. COPE guidelines on retraction were adhered to in that an explicit reason for each retraction was given. However, some notices did not document who retracted the article (eight articles, 6\%) and others were unclear whether the underlying cause was honest error or misconduct (15 articles, 11\%). The largest proportion of notices was issued by the authors (47 articles, 35\%). The majority of retractions were due to some form of misconduct (102 articles, 76\%), that is, compromised peer review (44 articles, 33\%), plagiarism (22 articles, 16\%) and data falsification/fabrication (10 articles, 7\%). Honest error accounted for 17 retractions (13\%) of which 10 articles (7\%) were published in error. The median number of days from publication to retraction was 337.5 days.Conclusions The most common reason to retract was compromised peer review. However, the majority of these cases date to March 2015 and appear to be the result of a systematic attempt to manipulate peer review across several publishers. Retractions due to plagiarism account for the second largest category and may be reduced by screening manuscripts before publication although this is not guaranteed. Retractions due to problems with the data may be reduced by appropriate data sharing and deposition before publication. Adopting a checklist (linked to COPE guidelines) and templates for various classes of retraction notices would increase transparency of retraction notices in future.},
  issn         = {2044-6055},
  journal      = {BMJ Open}
}

@article{moylan_why_2016-1,
  title      = {Why articles are retracted: a retrospective cross-sectional study of retraction notices at {BioMed} {Central}},
  volume     = {6},
  issn       = {2044-6055, 2044-6055},
  shorttitle = {Why articles are retracted},
  url        = {https://bmjopen.bmj.com/lookup/doi/10.1136/bmjopen-2016-012047},
  doi        = {10.1136/bmjopen-2016-012047},
  abstract   = {Objectives: To assess why articles are retracted from BioMed Central journals, whether retraction notices adhered to the Committee on Publication Ethics (COPE) guidelines, and are becoming more frequent as a proportion of published articles. Design/setting: Retrospective cross-sectional analysis of 134 retractions from January 2000 to December 2015.
                Results: 134 retraction notices were published during this timeframe. Although they account for 0.07\% of all articles published (190 514 excluding supplements, corrections, retractions and commissioned content), the rate of retraction is rising. COPE guidelines on retraction were adhered to in that an explicit reason for each retraction was given. However, some notices did not document who retracted the article (eight articles, 6\%) and others were unclear whether the underlying cause was honest error or misconduct (15 articles, 11\%). The largest proportion of notices was issued by the authors (47 articles, 35\%). The majority of retractions were due to some form of misconduct (102 articles, 76\%), that is, compromised peer review (44 articles, 33\%), plagiarism (22 articles, 16\%) and data falsification/fabrication (10 articles, 7\%). Honest error accounted for 17 retractions (13\%) of which 10 articles (7\%) were published in error. The median number of days from publication to retraction was 337.5 days.
                Conclusions: The most common reason to retract was compromised peer review. However, the majority of these cases date to March 2015 and appear to be the result of a systematic attempt to manipulate peer review across several publishers. Retractions due to plagiarism account for the second largest category and may be reduced by screening manuscripts before publication although this is not guaranteed. Retractions due to problems with the data may be reduced by appropriate data sharing and deposition before publication. Adopting a checklist (linked to COPE guidelines) and templates for various classes of retraction notices would increase transparency of retraction notices in future.},
  language   = {en},
  number     = {11},
  urldate    = {2024-06-13},
  journal    = {BMJ Open},
  author     = {Moylan, Elizabeth C and Kowalczuk, Maria K},
  month      = nov,
  year       = {2016},
  pages      = {e012047}
}

@book{nass_wired_2007,
  address    = {Cambridge, Mass.},
  edition    = {Annotated edition},
  title      = {Wired for {Speech}: {How} {Voice} {Activates} and {Advances} the {Human}-{Computer} {Relationship}},
  isbn       = {978-0-262-64065-7},
  shorttitle = {Wired for {Speech}},
  abstract   = {How interactive voice-based technology can tap into the automatic and powerful responses all speech―whether from human or machine―evokes.Interfaces that talk and listen are populating computers, cars, call centers, and even home appliances and toys, but voice interfaces invariably frustrate rather than help. In Wired for Speech, Clifford Nass and Scott Brave reveal how interactive voice technologies can readily and effectively tap into the automatic responses all speech―whether from human or machine―evokes. Wired for Speech demonstrates that people are "voice-activated": we respond to voice technologies as we respond to actual people and behave as we would in any social situation. By leveraging this powerful finding, voice interfaces can truly emerge as the next frontier for efficient, user-friendly technology.Wired for Speech presents new theories and experiments and applies them to critical issues concerning how people interact with technology-based voices. It considers how people respond to a female voice in e-commerce (does stereotyping matter?), how a car's voice can promote safer driving (are "happy" cars better cars?), whether synthetic voices have personality and emotion (is sounding like a person always good?), whether an automated call center should apologize when it cannot understand a spoken request ("To Err is Interface; To Blame, Complex"), and much more. Nass and Brave's deep understanding of both social science and design, drawn from ten years of research at Nass's Stanford laboratory, produces results that often challenge conventional wisdom and common design practices. These insights will help designers and marketers build better interfaces, scientists construct better theories, and everyone gain better understandings of the future of the machines that speak with us.},
  language   = {English},
  publisher  = {MIT Press},
  author     = {Nass, Clifford and Brave, Scott},
  month      = feb,
  year       = {2007}
}

@online{noauthor_anonymized_nodate,
  title   = {Anonymised Repository - Anonymous {GitHub}},
  url     = {https://anonymous.4open.science/r/RetractionWatch/README.md},
  urldate = {2025-01-15},
  file    = {Anonymized Repository - Anonymous GitHub:/home/aaron/snap/zotero-snap/common/Zotero/storage/29442NTF/README.html:text/html}
}

@online{noauthor_arxivorg_nodate,
  title   = {{arXiv}.org e-Print archive},
  url     = {https://arxiv.org/},
  urldate = {2025-01-15},
  file    = {arXiv.org e-Print archive:/home/aaron/snap/zotero-snap/common/Zotero/storage/6LZ2MLQB/arxiv.org.html:text/html}
}

@misc{noauthor_banks_nodate,
  title   = {Banks: {Thoughts} on publishing the research article... - {Google} {Scholar}},
  url     = {https://scholar.google.com/scholar_lookup?journal=Publications&title=Thoughts+on+publishing+the+research+article+over+the+centuries&author=D+Banks&volume=6&publication_year=2018&pages=10&doi=10.3390/publications6010010&},
  urldate = {2024-06-14}
}

@misc{noauthor_editorial_nodate,
  title   = {Editorial policies - {Corrections} and {Retractions} {\textbar} {Springer} {\textbar} {Springer} — {International} {Publisher}},
  url     = {https://www.springer.com/gp/editorial-policies/corrections-and-retractions},
  urldate = {2024-05-28}
}

@misc{noauthor_editorial_nodate-1,
  title   = {Editorial policies - {Corrections} and {Retractions} {\textbar} {Springer} {\textbar} {Springer} — {International} {Publisher}},
  url     = {https://www.springer.com/gp/editorial-policies/corrections-and-retractions},
  urldate = {2024-05-28}
}

@misc{noauthor_engineering_nodate,
  title   = {Engineering {Quality} and {Reliability} in {Technology}-{Assisted} {Review} {\textbar} {Proceedings} of the 39th {International} {ACM} {SIGIR} conference on {Research} and {Development} in {Information} {Retrieval}},
  url     = {https://dl.acm.org/doi/10.1145/2911451.2911510},
  urldate = {2024-06-06}
}

@misc{noauthor_google-bertbert-base-uncased_2024,
  title    = {google-bert/bert-base-uncased · {Hugging} {Face}},
  url      = {https://huggingface.co/google-bert/bert-base-uncased},
  abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
  urldate  = {2024-07-24},
  month    = mar,
  year     = {2024}
}

@misc{noauthor_high-performance_nodate,
  title   = {High-performance medicine: the convergence of human and artificial intelligence - {PubMed}},
  url     = {https://pubmed.ncbi.nlm.nih.gov/30617339/},
  urldate = {2024-05-04}
}

@online{noauthor_introducing_nodate,
  title    = {Introducing Claude 3.5 Sonnet},
  url      = {https://www.anthropic.com/news/claude-3-5-sonnet},
  abstract = {Introducing Claude 3.5 Sonnet—our most intelligent model yet. Sonnet now outperforms competitor models and Claude 3 Opus on key evaluations, at twice the speed.},
  urldate  = {11/12/2024},
  langid   = {english}
}

@misc{noauthor_inverted_2023,
  title     = {Inverted index},
  copyright = {Creative Commons Attribution-ShareAlike License},
  url       = {https://en.wikipedia.org/w/index.php?title=Inverted_index&oldid=1170015016},
  abstract  = {In computer science, an inverted index (also referred to as a postings list, postings file, or inverted file) is a database index storing a mapping from content, such as words or numbers, to its locations in a table, or in a document or a set of documents (named in contrast to a forward index, which maps from documents to content). The purpose of an inverted index is to allow fast full-text searches, at a cost of increased processing when a document is added to the database. The inverted file may be the database file itself, rather than its index. It is the most popular data structure used in document retrieval systems, used on a large scale for example in search engines. Additionally, several significant general-purpose mainframe-based database management systems have used inverted list architectures, including ADABAS, DATACOM/DB, and Model 204.
               There are two main variants of inverted indexes: A record-level inverted index (or inverted file index or just inverted file) contains a list of references to documents for each word. A word-level inverted index (or full inverted index or inverted list) additionally contains the positions of each word within a document. The latter form offers more functionality (like phrase searches), but needs more processing power and space to be created.},
  language  = {en},
  urldate   = {2024-06-17},
  journal   = {Wikipedia},
  month     = aug,
  year      = {2023},
  note      = {Page Version ID: 1170015016}
}

@misc{noauthor_natural_nodate,
  title   = {Natural {Language} {Processing} {\textbar} {SpringerLink}},
  url     = {https://link.springer.com/chapter/10.1007/978-81-322-3972-7_19},
  urldate = {2024-07-14}
}

@misc{noauthor_notitle_nodate
}

@online{noauthor_openai_nodate,
  title    = {{OpenAI} Platform},
  url      = {https://platform.openai.com},
  abstract = {Explore developer resources, tutorials, {API} docs, and dynamic examples to get the most out of {OpenAI}'s platform.},
  langid   = {english},
  file     = {Snapshot:/home/aaron/snap/zotero-snap/common/Zotero/storage/6WSXGCHX/gpt-4o-mini.html:text/html},
  urldate  = {1/12/2024}
}

@misc{noauthor_rewarding_nodate,
  title   = {Rewarding reviewers – sense or sensibility? {A} {Wiley} study explained},
  url     = {https://onlinelibrary.wiley.com/doi/epdf/10.1002/leap.1002},
  urldate = {2024-07-14}
}

@misc{noauthor_when_nodate,
  title   = {When to {Stop} {Reviewing} in {Technology}-{Assisted} {Reviews}: {Sampling} from an {Adaptive} {Distribution} to {Estimate} {Residual} {Relevant} {Documents}: {ACM} {Transactions} on {Information} {Systems}: {Vol} 38, {No} 4},
  url     = {https://dl.acm.org/doi/10.1145/3411755},
  urldate = {2024-06-13}
}

@misc{noauthor_work_2024,
  title    = {Work object {\textbar} {OpenAlex} technical documentation},
  url      = {https://docs.openalex.org/api-entities/works/work-object},
  language = {en},
  urldate  = {2024-06-17},
  month    = may,
  year     = {2024}
}

@article{oard_jointly_2018,
  title    = {Jointly {Minimizing} the {Expected} {Costs} of {Review} for {Responsiveness} and {Privilege} in {E}-{Discovery}},
  volume   = {37},
  issn     = {1046-8188},
  url      = {https://dl.acm.org/doi/10.1145/3268928},
  doi      = {10.1145/3268928},
  abstract = {Discovery is an important aspect of the civil litigation process in the United States of America, in which all parties to a lawsuit are permitted to request relevant evidence from other parties. With the rapid growth of digital content, the emerging need for “e-discovery” has created a strong demand for techniques that can be used to review massive collections both for “responsiveness” (i.e., relevance) to the request and for “privilege” (i.e., presence of legally protected content that the party performing the review may have a right to withhold). In this process, the party performing the review may incur costs of two types, namely, annotation costs (deriving from the fact that human reviewers need to be paid for their work) and misclassification costs (deriving from the fact that failing to correctly determine the responsiveness or privilege of a document may adversely affect the interests of the parties in various ways). Relying exclusively on automatic classification would minimize annotation costs but could result in substantial misclassification costs, while relying exclusively on manual classification could generate the opposite consequences. This article proposes a risk minimization framework (called MINECORE, for “{\textless}underline{\textgreater}min{\textless}/underline{\textgreater}imizing the {\textless}underline{\textgreater}e{\textless}/underline{\textgreater}xpected {\textless}underline{\textgreater}co{\textless}/underline{\textgreater}sts of {\textless}underline{\textgreater}re{\textless}/underline{\textgreater}view”) that seeks to strike an optimal balance between these two extreme stands. In MINECORE (a) the documents are first automatically classified for both responsiveness and privilege, and then (b) some of the automatically classified documents are annotated by human reviewers for responsiveness (typically by junior reviewers) and/or, in cascade, for privilege (typically by senior reviewers), with the overall goal of minimizing the expected cost (i.e., the risk) of the entire process. Risk minimization is achieved by optimizing, for both responsiveness and privilege, the choice of which documents to manually review. We present a simulation study in which classes from a standard text classification test collection (RCV1-v2) are used as surrogates for responsiveness and privilege. The results indicate that MINECORE can yield substantially lower total cost than any of a set of strong baselines.},
  number   = {1},
  urldate  = {2024-06-13},
  journal  = {ACM Transactions on Information Systems},
  author   = {Oard, Douglas W. and Sebastiani, Fabrizio and Vinjumur, Jyothi K.},
  month    = nov,
  year     = {2018},
  keywords = {E-discovery, semi-automated text classification, technology-assisted review, utility theory},
  pages    = {11:1--11:35}
}

@misc{pakhale_comprehensive_2023,
  title      = {Comprehensive {Overview} of {Named} {Entity} {Recognition}: {Models}, {Domain}-{Specific} {Applications} and {Challenges}},
  shorttitle = {Comprehensive {Overview} of {Named} {Entity} {Recognition}},
  url        = {http://arxiv.org/abs/2309.14084},
  doi        = {10.48550/arXiv.2309.14084},
  abstract   = {In the domain of Natural Language Processing (NLP), Named Entity Recognition (NER) stands out as a pivotal mechanism for extracting structured insights from unstructured text. This manuscript offers an exhaustive exploration into the evolving landscape of NER methodologies, blending foundational principles with contemporary AI advancements. Beginning with the rudimentary concepts of NER, the study spans a spectrum of techniques from traditional rule-based strategies to the contemporary marvels of transformer architectures, particularly highlighting integrations such as BERT with LSTM and CNN. The narrative accentuates domain-specific NER models, tailored for intricate areas like finance, legal, and healthcare, emphasizing their specialized adaptability. Additionally, the research delves into cutting-edge paradigms including reinforcement learning, innovative constructs like E-NER, and the interplay of Optical Character Recognition (OCR) in augmenting NER capabilities. Grounding its insights in practical realms, the paper sheds light on the indispensable role of NER in sectors like finance and biomedicine, addressing the unique challenges they present. The conclusion outlines open challenges and avenues, marking this work as a comprehensive guide for those delving into NER research and applications.},
  urldate    = {2023-09-27},
  publisher  = {arXiv},
  author     = {Pakhale, Kalyani},
  month      = sep,
  year       = {2023},
  note       = {arXiv:2309.14084 [cs]},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval}
}

@inproceedings{patil_inverted_2011,
  address   = {New York, NY, USA},
  series    = {{SIGIR} '11},
  title     = {Inverted indexes for phrases and strings},
  isbn      = {978-1-4503-0757-4},
  url       = {https://doi.org/10.1145/2009916.2009992},
  doi       = {10.1145/2009916.2009992},
  abstract  = {Inverted indexes are the most fundamental and widely used data structures in information retrieval. For each unique word occurring in a document collection, the inverted index stores a list of the documents in which this word occurs. Compression techniques are often applied to further reduce the space requirement of these lists. However, the index has a shortcoming, in that only predefined pattern queries can be supported efficiently. In terms of string documents where word boundaries are undefined, if we have to index all the substrings of a given document, then the storage quickly becomes quadratic in the data size. Also, if we want to apply the same type of indexes for querying phrases or sequence of words, then the inverted index will end up storing redundant information. In this paper, we show the first set of inverted indexes which work naturally for strings as well as phrase searching. The central idea is to exclude document d in the inverted list of a string P if every occurrence of P in d is subsumed by another string of which P is a prefix. With this we show that our space utilization is close to the optimal. Techniques from succinct data structures are deployed to achieve compression while allowing fast access in terms of frequency and document id based retrieval. Compression and speed trade-offs are evaluated for different variants of the proposed index. For phrase searching, we show that our indexes compare favorably against a typical inverted index deploying position-wise intersections. We also show efficient top-k based retrieval under relevance metrics like frequency and tf-idf.},
  urldate   = {2024-06-17},
  booktitle = {Proceedings of the 34th international {ACM} {SIGIR} conference on {Research} and development in {Information} {Retrieval}},
  publisher = {Association for Computing Machinery},
  author    = {Patil, Manish and Thankachan, Sharma V. and Shah, Rahul and Hon, Wing-Kai and Vitter, Jeffrey Scott and Chandrasekaran, Sabrina},
  month     = jul,
  year      = {2011},
  keywords  = {compressed data structures, inverted indexes, phrase searching, top-k queries},
  pages     = {555--564}
}

@misc{pearce_understanding_2021,
  title     = {Understanding {Softmax} {Confidence} and {Uncertainty}},
  url       = {http://arxiv.org/abs/2106.04972},
  abstract  = {It is often remarked that neural networks fail to increase their uncertainty when predicting on data far from the training distribution. Yet naively using softmax confidence as a proxy for uncertainty achieves modest success in tasks exclusively testing for this, e.g., out-of-distribution (OOD) detection. This paper investigates this contradiction, identifying two implicit biases that do encourage softmax confidence to correlate with epistemic uncertainty: 1) Approximately optimal decision boundary structure, and 2) Filtering effects of deep networks. It describes why low-dimensional intuitions about softmax confidence are misleading. Diagnostic experiments quantify reasons softmax confidence can fail, finding that extrapolations are less to blame than overlap between training and OOD data in final-layer representations. Pre-trained/fine-tuned networks reduce this overlap.},
  urldate   = {2023-12-06},
  publisher = {arXiv},
  author    = {Pearce, Tim and Brintrup, Alexandra and Zhu, Jun},
  month     = jun,
  year      = {2021},
  note      = {arXiv:2106.04972 [cs, stat]},
  keywords  = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{pedregosa_scikit-learn_2011,
  title      = {Scikit-learn: {Machine} {Learning} in {Python}},
  volume     = {12},
  issn       = {1533-7928},
  shorttitle = {Scikit-learn},
  url        = {http://jmlr.org/papers/v12/pedregosa11a.html},
  abstract   = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language.  Emphasis is put on ease of use, performance, documentation, and API consistency.  It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings.  Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
  number     = {85},
  urldate    = {2024-07-20},
  journal    = {Journal of Machine Learning Research},
  author     = {Pedregosa, Fabian and Varoquaux, Gaël and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, Édouard},
  year       = {2011},
  pages      = {2825--2830}
}

@misc{peng_systematic_2023,
  title     = {A {Systematic} {Evaluation} of {Federated} {Learning} on {Biomedical} {Natural} {Language} {Processing}},
  url       = {http://arxiv.org/abs/2307.11254},
  doi       = {10.48550/arXiv.2307.11254},
  abstract  = {Language models (LMs) like BERT and GPT have revolutionized natural language processing (NLP). However, privacy-sensitive domains, particularly the medical field, face challenges to train LMs due to limited data access and privacy constraints imposed by regulations like the Health Insurance Portability and Accountability Act (HIPPA) and the General Data Protection Regulation (GDPR). Federated learning (FL) offers a decentralized solution that enables collaborative learning while ensuring the preservation of data privacy. In this study, we systematically evaluate FL in medicine across \$2\$ biomedical NLP tasks using \$6\$ LMs encompassing \$8\$ corpora. Our results showed that: 1) FL models consistently outperform LMs trained on individual client's data and sometimes match the model trained with polled data; 2) With the fixed number of total data, LMs trained using FL with more clients exhibit inferior performance, but pre-trained transformer-based models exhibited greater resilience. 3) LMs trained using FL perform nearly on par with the model trained with pooled data when clients' data are IID distributed while exhibiting visible gaps with non-IID data. Our code is available at: https://github.com/PL97/FedNLP},
  urldate   = {2023-09-27},
  publisher = {arXiv},
  author    = {Peng, Le and zhou, sicheng and chen, jiandong and Zhang, Rui and Xu, Ziyue and Sun, Ju},
  month     = jul,
  year      = {2023},
  note      = {arXiv:2307.11254 [cs]},
  keywords  = {Computer Science - Computation and Language}
}

@article{perera_recent_2017,
  title      = {Recent {Advances} in {Natural} {Language} {Generation}: {A} {Survey} and {Classification} of the {Empirical} {Literature}},
  volume     = {36},
  copyright  = {Copyright (c) 2021 COMPUTING AND INFORMATICS},
  issn       = {2585-8807},
  shorttitle = {Recent {Advances} in {Natural} {Language} {Generation}},
  url        = {https://www.cai.sk/ojs/index.php/cai/article/view/2017_1_1},
  doi        = {https://doi.org/10.4149/cai_2017_1_1},
  abstract   = {Natural Language Generation (NLG) is defined as the systematic approach for producing human understandable natural language text based on non-textual data or from meaning representations. This is a significant area which empowers human-computer interaction. It has also given rise to a variety of theoretical as well as empirical approaches. This paper intends to provide a detailed overview and a classification of the state-of-the-art approaches in Natural Language Generation. The paper explores NLG architectures and tasks classed under document planning, micro-planning and surface realization modules. Additionally, this paper also identifies the gaps existing in the NLG research which require further work in order to make NLG a widely usable technology.},
  language   = {en},
  number     = {1},
  journal    = {Computing and Informatics},
  author     = {Perera, Rivindu and Nand, Parma},
  month      = may,
  year       = {2017},
  keywords   = {Natural language processing, document planning, micro-planning, surface realization},
  pages      = {1--32}
}

@article{prictor_where_2023,
  title        = {{WHERE} {DOES} {RESPONSIBILITY} {LIE}? {ANALYSING} {LEGAL} {AND} {REGULATORY} {RESPONSES} {TO} {FLAWED} {CLINICAL} {DECISION} {SUPPORT} {SYSTEMS} {WHEN} {PATIENTS} {SUFFER} {HARM}},
  volume       = {31},
  rights       = {https://creativecommons.org/licenses/by/4.0/},
  issn         = {0967-0742, 1464-3790},
  url          = {https://academic.oup.com/medlaw/article/31/1/1/6646802},
  doi          = {10.1093/medlaw/fwac022},
  shorttitle   = {{WHERE} {DOES} {RESPONSIBILITY} {LIE}?},
  abstract     = {Abstract
                  Clinical decision support systems ({CDSSs}) are digital healthcare information systems that apply algorithms to patient data to generate tailored recommendations. They are designed to support, but neither dictate nor execute, clinical decisions. {CDSSs} can introduce new risks, both by design features that heighten clinician burden and by outright errors that generate faulty recommendations for care. In the latter instance, if such unintercepted recommendations were to result in harm to the patient, novel legal questions emerge. Does legal responsibility for this harm lie with the clinician, the software developer or both? What is the clearest path to a remedy? Further, how does the Australian regulatory framework provide for oversight and redress? This article analyses the potential forms of legal redress in negligence, contract and under statutory consumer law, for the patient and the clinician. It also examines the Australian regulatory framework, specifically in relation to the Australian Competition and Consumer Commission and the Therapeutic Goods Administration, and reflects on the framework’s adequacy to protect patients and clinicians. It finds that the regulatory approach and the contour of legal risk still centre upon the clinician’s duty to exercise decisional autonomy and to intercept flawed recommendations generated by algorithmic errors within {CDSSs}.},
  pages        = {1--24},
  number       = {1},
  journaltitle = {Medical Law Review},
  author       = {Prictor, Megan},
  urldate      = {2025-03-13},
  date         = {2023-02-27},
  langid       = {english},
  file         = {Full Text:/home/aaron/snap/zotero-snap/common/Zotero/storage/RMZ5XXX5/Prictor - 2023 - WHERE DOES RESPONSIBILITY LIE ANALYSING LEGAL AND REGULATORY RESPONSES TO FLAWED CLINICAL DECISION.pdf:application/pdf}
}

@misc{priem_openalex_2022,
  title      = {{OpenAlex}: {A} fully-open index of scholarly works, authors, venues, institutions, and concepts},
  shorttitle = {{OpenAlex}},
  doi        = {10.48550/arXiv.2205.01833},
  abstract   = {OpenAlex is a new, fully-open scientific knowledge graph (SKG), launched to replace the discontinued Microsoft Academic Graph (MAG). It contains metadata for 209M works (journal articles, books, etc); 2013M disambiguated authors; 124k venues (places that host works, such as journals and online repositories); 109k institutions; and 65k Wikidata concepts (linked to works via an automated hierarchical multi-tag classifier). The dataset is fully and freely available via a web-based GUI, a full data dump, and high-volume REST API. The resource is under active development and future work will improve accuracy and coverage of citation information and author/institution parsing and deduplication.},
  publisher  = {arXiv},
  author     = {Priem, Jason and Piwowar, Heather and Orr, Richard},
  month      = jun,
  year       = {2022},
  keywords   = {Computer Science - Digital Libraries}
}

@article{publications4020009,
  author         = {Tian, Mei and Su, Yan and Ru, Xin},
  title          = {Perish or Publish in China: Pressures on Young Chinese Scholars to Publish in Internationally Indexed Journals},
  journal        = {Publications},
  volume         = {4},
  year           = {2016},
  number         = {2},
  article-number = {9},
  issn           = {2304-6775},
  abstract       = {To boost their research productivities, Chinese universities are putting great pressure on their research-active staff to publish in internationally indexed journals. However, the emerging publish-or-perish culture in China has seen little empirical investigation thus far. In the research reported in this article, semi-structured interviews were conducted with seven young researchers in science and engineering disciplines at a research-centered university in central China. The study showed that these young scholars faced great pressure to publish papers in internationally indexed journals. Consequently, the participants were reluctant to spend time on other academic activities, including teaching training. They also reported considerable work time devoted to writing, which resulted in fatigue and negatively affected family relations. The participants admitted that they had to rush to publish, and therefore were less likely to produce papers of better quality or those with novel discoveries. The research contributes to our reflection upon Chinese universities’ increasing use of the number of international publications as a major assessment and incentive measurement of their faculties’ academic performance.},
  doi            = {10.3390/publications4020009}
}

@misc{pubmed_medlinepubmed_2023,
  title     = {{MEDLINE}/{PubMed} {Baseline} {Repository} ({MBR})},
  copyright = {Courtesy of the U.S. National Library of Medicine},
  url       = {https://lhncbc.nlm.nih.gov/ii/information/MBR.html},
  abstract  = {The MEDLINE/PubMed Baseline Repository (MBR) provides access to each MEDLINE/PubMed Baseline snapshot starting with the 2002 MEDLINE Baseline. Each baseline contains a snapshot of MEDLINE citations in the state they were at a given moment in time without the MeSH vocabulary updates and other revisions that occur during the year. The baseline snapshot is created at the beginning of each new MeSH Indexing Year. The records included in the MEDLINE/PubMed Baseline databases represent a static view of the data at the time each baseline database was created.},
  urldate   = {2024-04-25},
  journal   = {MEDLINE/PubMed Baseline Repository (MBR)},
  author    = {Pubmed},
  month     = jan,
  year      = {2023},
  note      = {Courtesy of the U.S. National Library of Medicine}
}

@misc{rahman_chatgpt_2023,
  address    = {Rochester, NY},
  type       = {{SSRN} {Scholarly} {Paper}},
  title      = {{ChatGPT} and {Academic} {Research}: {A} {Review} and {Recommendations} {Based} on {Practical} {Examples}},
  shorttitle = {{ChatGPT} and {Academic} {Research}},
  url        = {https://papers.ssrn.com/abstract=4407462},
  abstract   = {In the academic world, academicians, researchers, and students have already employed Large Language Models (LLMs) such as ChatGPT to complete their various academic and non-academic tasks, including essay writing, different formal and informal speech writing, summarising literature, and generating ideas. However,  yet, it is a controversial issue to use ChatGPT in academic research. Recently, its impact on academic research and publication has been scrutinized. The fundamental objective of this study is to highlight the application of ChatGPT in academic research by demonstrating a practical example with some recommendations. Data for this study was gathered using published articles, websites, blogs, and visual and numerical artefacts. We have analyzed, synthesized, and described our gathered data using an "introductory literature review." The findings revealed that for the initial idea generation for academic scientific research, ChatGPT could be an effective tool. However, in the case of literature synthesis, citations, problem statements, research gaps, and data analysis, the researchers might encounter some challenges. Therefore, in these cases, researchers must be cautious about using ChatGPT in academic research. Considering the potential applications and consequences of ChatGPT, it is a must for the academic and scientific community to establish the necessary guidelines for the appropriate use of LLMs, especially ChatGPT, in research and publishing.},
  language   = {en},
  urldate    = {2024-05-28},
  author     = {Rahman, Md Mizanur and Terano, Harold Jan and Rahman, Md Nafizur and Salamzadeh, Aidin and Rahaman, Md Saidur},
  month      = mar,
  year       = {2023},
  keywords   = {ChatGPT, Large Language Models, generative AI, publishing, research}
}

@misc{raman_planning_2022,
  title     = {Planning with {Large} {Language} {Models} via {Corrective} {Re}-prompting},
  url       = {http://arxiv.org/abs/2211.09935},
  abstract  = {Extracting the common sense knowledge present in Large Language Models (LLMs) offers a path to designing intelligent, embodied agents. Related works have queried LLMs with a wide-range of contextual information, such as goals, sensor observations and scene descriptions, to generate high-level action plans for speciﬁc tasks; however these approaches often involve human intervention or additional machinery to enable sensor-motor interactions. In this work, we propose a prompting-based strategy for extracting executable plans from an LLM, which leverages a novel and readily-accessible source of information: precondition errors. Our approach assumes that actions are only afforded execution in certain contexts, i.e., implicit preconditions must be met for an action to execute (e.g., a door must be unlocked to open it), and that the embodied agent has the ability to determine if the action is/is not executable in the current context (e.g., detect if a precondition error is present). When an agent is unable to execute an action, our approach re-prompts the LLM with precondition error information to extract an executable corrective action to achieve the intended goal in the current context. We evaluate our approach in the VirtualHome simulation environment on 88 different tasks and 7 scenes. We evaluate different prompt templates and compare to methods that naively re-sample actions from the LLM. Our approach, using precondition errors, improves executability and semantic correctness of plans, while also reducing the number of re-prompts required when querying actions.},
  language  = {en},
  urldate   = {2023-09-15},
  publisher = {arXiv},
  author    = {Raman, Shreyas Sundara and Cohen, Vanya and Rosen, Eric and Idrees, Ifrah and Paulius, David and Tellex, Stefanie},
  month     = nov,
  year      = {2022},
  note      = {arXiv:2211.09935 [cs]},
  keywords  = {68T20, 68T50, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Robotics, I.2.2, I.2.4, I.2.7, I.2.8}
}

@misc{rao_withdrarxiv_2024,
  title      = {{WithdrarXiv}: A Large-Scale Dataset for Retraction Study},
  url        = {http://arxiv.org/abs/2412.03775},
  doi        = {10.48550/arXiv.2412.03775},
  shorttitle = {{WithdrarXiv}},
  abstract   = {Retractions play a vital role in maintaining scientific integrity, yet systematic studies of retractions in computer science and other {STEM} fields remain scarce. We present {WithdrarXiv}, the first large-scale dataset of withdrawn papers from {arXiv}, containing over 14,000 papers and their associated retraction comments spanning the repository's entire history through September 2024. Through careful analysis of author comments, we develop a comprehensive taxonomy of retraction reasons, identifying 10 distinct categories ranging from critical errors to policy violations. We demonstrate a simple yet highly accurate zero-shot automatic categorization of retraction reasons, achieving a weighted average F1-score of 0.96. Additionally, we release {WithdrarXiv}-{SciFy}, an enriched version including scripts for parsed full-text {PDFs}, specifically designed to enable research in scientific feasibility studies, claim verification, and automated theorem proving. These findings provide valuable insights for improving scientific quality control and automated verification systems. Finally, and most importantly, we discuss ethical issues and take a number of steps to implement responsible data release while fostering open science in this area.},
  number     = {{arXiv}:2412.03775},
  publisher  = {{arXiv}},
  author     = {Rao, Delip and Young, Jonathan and Dietterich, Thomas and Callison-Burch, Chris},
  urldate    = {2025-02-18},
  date       = {2024-12-04},
  eprinttype = {arxiv},
  eprint     = {2412.03775 [cs]},
  keywords   = {Computer Science - Computation and Language, Computer Science - Digital Libraries, Computer Science - Machine Learning},
  file       = {Preprint PDF:/home/aaron/snap/zotero-snap/common/Zotero/storage/4FP4ZXKN/Rao et al. - 2024 - WithdrarXiv A Large-Scale Dataset for Retraction Study.pdf:application/pdf;Snapshot:/home/aaron/snap/zotero-snap/common/Zotero/storage/WAM3UUVG/2412.html:text/html}
}

@misc{retraction_watch,
  title     = {{The Retraction Watch Database [Internet]}},
  url       = {http://retractiondatabase.org/},
  year      = {2018},
  publisher = {The Center for Scientific Integrity},
  issn      = {2692-4579},
  month     = jun,
  year      = {2024}
}

@misc{retraction_watch_retraction_2024,
  type         = {Tracking retractions as a window into the scientific process},
  title        = {Retraction {Watch}},
  url          = {https://retractionwatch.com/},
  language     = {en-US},
  urldate      = {2024-06-14},
  howpublished = {The Center for Scientific Integrity},
  address      = {New York},
  year         = {2018},
  issn         = {2692-4579}
}

@misc{retraction_watch_retraction_2024-1,
  title   = {Retraction {Watch} {Database} {User} {Guide}},
  url     = {https://retractionwatch.com/wp-content/uploads/2023/12/Building-The-Database.pdf},
  urldate = {2024-06-14},
  journal = {Retraction Watch Database User Guide},
  month   = jun,
  year    = {2024}
}

@online{rittman_retraction_2025,
  title      = {Retraction Watch retractions now in the Crossref {API}},
  rights     = {https://creativecommons.org/licenses/by/4.0/},
  url        = {https://www.crossref.org/blog/retraction-watch-retractions-now-in-the-crossref-api/},
  titleaddon = {Crossref},
  author     = {Rittman, Martyn},
  urldate    = {2025-02-18},
  date       = {2025-01-29},
  doi        = {10.13003/692016}
}

@article{rubio-fernandez_incrementality_2020,
  title    = {Incrementality and efficiency shape pragmatics across languages},
  volume   = {117},
  issn     = {0027-8424, 1091-6490},
  url      = {https://pnas.org/doi/full/10.1073/pnas.1922067117},
  doi      = {10.1073/pnas.1922067117},
  abstract = {To correctly interpret a message, people must attend to the context in which it was produced. Here we investigate how this process, known as pragmatic reasoning, is guided by two universal forces in human communication: incrementality and efficiency, with speakers of all languages interpreting language incrementally and making the most efficient use of the incoming information. Crucially, however, the interplay between these two forces results in speakers of different languages having different pragmatic information available at each point in processing, including inferences about speaker intentions. In particular, the position of adjectives relative to nouns (e.g., “black lamp” vs. “lamp black”) makes visual context information available in reverse orders. In an eye-tracking study comparing four unrelated languages that have been understudied with regard to language processing (Catalan, Hindi, Hungarian, and Wolof), we show that speakers of languages with an adjective–noun order integrate context by first identifying properties (e.g., color, material, or size), whereas speakers of languages with a noun–adjective order integrate context by first identifying kinds (e.g., lamps or chairs). Most notably, this difference allows listeners of adjective–noun descriptions to infer the speaker’s intention when using an adjective (e.g., “the black…” as implying “not the blue one”) and anticipate the target referent, whereas listeners of noun–adjective descriptions are subject to temporary ambiguity when deriving the same interpretation. We conclude that incrementality and efficiency guide pragmatic reasoning across languages, with different word orders having different pragmatic affordances.},
  language = {en},
  number   = {24},
  urldate  = {2023-09-15},
  journal  = {Proceedings of the National Academy of Sciences},
  author   = {Rubio-Fernandez, Paula and Jara-Ettinger, Julian},
  month    = jun,
  year     = {2020},
  pages    = {13399--13404}
}

@misc{ruis_large_2022,
  title     = {Large language models are not zero-shot communicators},
  url       = {http://arxiv.org/abs/2210.14986},
  abstract  = {Despite widespread use of LLMs as conversational agents, evaluations of performance fail to capture a crucial aspect of communication: interpreting language in context. Humans interpret language using beliefs and prior knowledge about the world. For example, we intuitively understand the response “I wore gloves” to the question “Did you leave ﬁngerprints?” as meaning “No”. To investigate whether LLMs have the ability to make this type of inference, known as an implicature, we design a simple task and evaluate widely used state-of-the-art models. We ﬁnd that, despite only evaluating on utterances that require a binary inference (yes or no), most perform close to random. Models adapted to be “aligned with human intent” perform much better, but still show a signiﬁcant gap with human performance. We present our ﬁndings as the starting point for further research into evaluating how LLMs interpret language in context and to drive the development of more pragmatic and useful models of human discourse.},
  language  = {en},
  urldate   = {2023-09-15},
  publisher = {arXiv},
  author    = {Ruis, Laura and Khan, Akbir and Biderman, Stella and Hooker, Sara and Rocktäschel, Tim and Grefenstette, Edward},
  month     = oct,
  year      = {2022},
  note      = {arXiv:2210.14986 [cs]},
  keywords  = {Computer Science - Computation and Language}
}

@misc{sap_neural_2023,
  title      = {Neural {Theory}-of-{Mind}? {On} the {Limits} of {Social} {Intelligence} in {Large} {LMs}},
  shorttitle = {Neural {Theory}-of-{Mind}?},
  url        = {http://arxiv.org/abs/2210.13312},
  abstract   = {Social intelligence and Theory of Mind (TOM), i.e., the ability to reason about the different mental states, intents, and reactions of all people involved, allow humans to effectively navigate and understand everyday social interactions. As NLP systems are used in increasingly complex social situations, their ability to grasp social dynamics becomes crucial.},
  language   = {en},
  urldate    = {2023-09-15},
  publisher  = {arXiv},
  author     = {Sap, Maarten and LeBras, Ronan and Fried, Daniel and Choi, Yejin},
  month      = apr,
  year       = {2023},
  note       = {arXiv:2210.13312 [cs]},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language}
}

@inproceedings{sap_social_2019,
  address    = {Hong Kong, China},
  title      = {Social {IQa}: {Commonsense} {Reasoning} about {Social} {Interactions}},
  shorttitle = {Social {IQa}},
  url        = {https://www.aclweb.org/anthology/D19-1454},
  doi        = {10.18653/v1/D19-1454},
  abstract   = {We introduce SOCIAL IQA, the ﬁrst largescale benchmark for commonsense reasoning about social situations. SOCIAL IQA contains 38,000 multiple choice questions for probing emotional and social intelligence in a variety of everyday situations (e.g., Q: “Jordan wanted to tell Tracy a secret, so Jordan leaned towards Tracy. Why did Jordan do this?” A: “Make sure no one else could hear”). Through crowdsourcing, we collect commonsense questions along with correct and incorrect answers about social interactions, using a new framework that mitigates stylistic artifacts in incorrect answers by asking workers to provide the right answer to a different but related question. Empirical results show that our benchmark is challenging for existing question-answering models based on pretrained language models, compared to human performance ({\textgreater}20\% gap). Notably, we further establish SOCIAL IQA as a resource for transfer learning of commonsense knowledge, achieving state-of-the-art performance on multiple commonsense reasoning tasks (Winograd Schemas, COPA).},
  language   = {en},
  urldate    = {2023-09-15},
  booktitle  = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
  publisher  = {Association for Computational Linguistics},
  author     = {Sap, Maarten and Rashkin, Hannah and Chen, Derek and Le Bras, Ronan and Choi, Yejin},
  year       = {2019},
  pages      = {4462--4472}
}

@misc{schaeffer_are_2023,
  title     = {Are {Emergent} {Abilities} of {Large} {Language} {Models} a {Mirage}?},
  url       = {http://arxiv.org/abs/2304.15004},
  doi       = {10.48550/arXiv.2304.15004},
  abstract  = {Recent work claims that large language models display emergent abilities, abilities not present in smaller-scale models that are present in larger-scale models. What makes emergent abilities intriguing is two-fold: their sharpness, transitioning seemingly instantaneously from not present to present, and their unpredictability, appearing at seemingly unforeseeable model scales. Here, we present an alternative explanation for emergent abilities: that for a particular task and model family, when analyzing fixed model outputs, emergent abilities appear due to the researcher's choice of metric rather than due to fundamental changes in model behavior with scale. Specifically, nonlinear or discontinuous metrics produce apparent emergent abilities, whereas linear or continuous metrics produce smooth, continuous predictable changes in model performance. We present our alternative explanation in a simple mathematical model, then test it in three complementary ways: we (1) make, test and confirm three predictions on the effect of metric choice using the InstructGPT/GPT-3 family on tasks with claimed emergent abilities; (2) make, test and confirm two predictions about metric choices in a meta-analysis of emergent abilities on BIG-Bench; and (3) show to choose metrics to produce never-before-seen seemingly emergent abilities in multiple vision tasks across diverse deep networks. Via all three analyses, we provide evidence that alleged emergent abilities evaporate with different metrics or with better statistics, and may not be a fundamental property of scaling AI models.},
  urldate   = {2023-11-27},
  publisher = {arXiv},
  author    = {Schaeffer, Rylan and Miranda, Brando and Koyejo, Sanmi},
  month     = may,
  year      = {2023},
  note      = {arXiv:2304.15004 [cs]},
  keywords  = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning}
}

@book{scharre_army_2018,
  address    = {New York London},
  edition    = {First edition},
  title      = {Army of none: autonomous weapons and the future of war},
  isbn       = {978-0-393-60898-4},
  shorttitle = {Army of none},
  abstract   = {What happens when a Predator drone has as much autonomy as a Google car? Or when a weapon that can hunt its own targets is hacked? Although it sounds like science fiction, the technology already exists to create weapons that can attack targets without human input. Paul Scharre's, a leading expert in emerging weapons technologies, draws on deep research and firsthand experience to explore how these next-generation weapons are changing warfare.Scharre's far-ranging investigation examines the emergence of autonomous weapons, the movement to ban them, and the legal and ethical issues surrounding their use. He spotlights artificial intelligence in military technology, spanning decades of innovation from German noise-seeking Wren torpedoes in World War II antecedents of today's homing missiles to autonomous cyber weapons, submarine-hunting robot ships, and robot tank armies. Through interviews with defense experts, ethicists, psychologists, and activists, Scharre surveys what challenges might face "centaur warfighters" on future battlefields, which will combine human and machine cognition. We've made tremendous technological progress in the past few decades, but we have also glimpsed the terrifying mishaps that can result from complex automated systemssuch as when advanced F-22 fighter jets experienced a computer meltdown the first time they flew over the International Date Line. At least thirty countries already have defensive autonomous weapons that operate under human supervision. Around the globe, militaries are racing to build robotic weapons with increasing autonomy. The ethical questions within this book grow more pressing each day. To what extent should such technologies be advanced? And if responsible democracies ban them, would that stop rogue regimes from taking advantage? At the forefront of a game-changing debate, Army of None engages military history, global policy, and cutting-edge science to argue that we must embrace technology where it can make war more precise and humane, but without surrendering human judgment. When the choice is life or death, there is no replacement for the human heart},
  language   = {eng},
  publisher  = {W.W. Norton \& Company},
  author     = {Scharre, Paul},
  year       = {2018}
}

@misc{schlangen_dialogue_2023,
  title      = {Dialogue {Games} for {Benchmarking} {Language} {Understanding}: {Motivation}, {Taxonomy}, {Strategy}},
  shorttitle = {Dialogue {Games} for {Benchmarking} {Language} {Understanding}},
  url        = {http://arxiv.org/abs/2304.07007},
  abstract   = {How does one measure “ability to understand language”? If it is a person’s ability that is being measured, this is a question that almost never poses itself in an unqualiﬁed manner: Whatever formal test is applied, it takes place on the background of the person’s language use in daily social practice, and what is measured is a specialised variety of language understanding (e.g., of a second language; or of written, technical language). Computer programs do not have this background. What does that mean for the applicability of formal tests of language understanding? I argue that such tests need to be complemented with tests of language use embedded in a practice, to arrive at a more comprehensive evaluation of “artiﬁcial language understanding”. To do such tests systematically, I propose to use “Dialogue Games”—constructed activities that provide a situational embedding for language use. I describe a taxonomy of Dialogue Game types, linked to a model of underlying capabilites that are tested, and thereby giving an argument for the construct validity of the test. I close with showing how the internal structure of the taxonomy suggests an ordering from more specialised to more general situational language understanding, which potentially can provide some strategic guidance for development in this ﬁeld.},
  language   = {en},
  urldate    = {2023-09-15},
  publisher  = {arXiv},
  author     = {Schlangen, David},
  month      = apr,
  year       = {2023},
  note       = {arXiv:2304.07007 [cs]},
  keywords   = {Computer Science - Computation and Language}
}

@article{schneider_continued_2020,
  title        = {Continued post-retraction citation of a fraudulent clinical trial report, 11 years after it was retracted for falsifying data},
  volume       = {125},
  issn         = {0138-9130, 1588-2861},
  url          = {https://link.springer.com/10.1007/s11192-020-03631-1},
  doi          = {10.1007/s11192-020-03631-1},
  abstract     = {Abstract
                  
                  This paper presents a case study of long-term post-retraction citation to falsified clinical trial data (Matsuyama et al. in Chest 128(6):3817–3827, 2005.
                  10.1378/chest.128.6.3817
                  ), demonstrating problems with how the current digital library environment communicates retraction status. Eleven years after its retraction, the paper continues to be cited positively and uncritically to support a medical nutrition intervention, without mention of its 2008 retraction for falsifying data. To date no high quality clinical trials reporting on the efficacy of omega-3 fatty acids on reducing inflammatory markers have been published. Our paper uses network analysis, citation context analysis, and retraction status visibility analysis to illustrate the potential for extended propagation of misinformation over a citation network, updating and extending a case study of the first 6 years of post-retraction citation (Fulton et al. in Publications 3(1):7–26, 2015.
                  10.3390/publications3010017
                  ). The current study covers 148 direct citations from 2006 through 2019 and their 2542 second-generation citations and assesses retraction status visibility of the case study paper and its retraction notice on 12 digital platforms as of 2020. The retraction is not mentioned in 96\% (107/112) of direct post-retraction citations for which we were able to conduct citation context analysis. Over 41\% (44/107) of direct post-retraction citations that do not mention the retraction describe the case study paper in detail, giving a risk of diffusing misinformation from the case paper. We analyze 152 second-generation citations to the most recent 35 direct citations (2010–2019) that do not mention the retraction but do mention methods or results of the case paper, finding 23 possible diffusions of misinformation from these non-direct citations to the case paper. Link resolving errors from databases show a significant challenge in a reader reaching the retraction notice via a database search. Only 1/8 databases (and 1/9 database records) consistently resolved the retraction notice to its full-text correctly in our tests. Although limited to evaluation of a single case (
                  N 
                  = 1), this work demonstrates how retracted research can continue to spread and how the current information environment contributes to this problem.},
  pages        = {2877--2913},
  number       = {3},
  journal      = {Scientometrics},
  shortjournal = {Scientometrics},
  author       = {Schneider, Jodi and Ye, Di and Hill, Alison M. and Whitehorn, Ashley S.},
  date         = {2020-12},
  langid       = {english},
  year         = {2020},
  file         = {Full Text:/home/aaron/snap/zotero-snap/common/Zotero/storage/X6578DB2/Schneider et al. - 2020 - Continued post-retraction citation of a fraudulent clinical trial report, 11 years after it was retr.pdf:application/pdf}
}

@inproceedings{schumann_active_2019,
  address   = {Hong Kong, China},
  title     = {Active {Learning} via {Membership} {Query} {Synthesis} for {Semi}-{Supervised} {Sentence} {Classification}},
  url       = {https://aclanthology.org/K19-1044},
  doi       = {10.18653/v1/K19-1044},
  abstract  = {Active learning (AL) is a technique for reducing manual annotation effort during the annotation of training data for machine learning classifiers. For NLP tasks, pool-based and stream-based sampling techniques have been used to select new instances for AL while gen erating new, artificial instances via Membership Query Synthesis was, up to know, considered to be infeasible for NLP problems. We present the first successfull attempt to use Membership Query Synthesis for generating AL queries, using Variational Autoencoders for query generation. We evaluate our approach in a text classification task and demonstrate that query synthesis shows competitive performance to pool-based AL strategies while substantially reducing annotation time},
  urldate   = {2024-02-23},
  booktitle = {Proceedings of the 23rd {Conference} on {Computational} {Natural} {Language} {Learning} ({CoNLL})},
  publisher = {Association for Computational Linguistics},
  author    = {Schumann, Raphael and Rehbein, Ines},
  editor    = {Bansal, Mohit and Villavicencio, Aline},
  month     = nov,
  year      = {2019},
  pages     = {472--481}
}

@misc{seals_discourse_2023,
  title      = {Discourse over {Discourse}: {The} {Need} for an {Expanded} {Pragmatic} {Focus} in {Conversational} {AI}},
  shorttitle = {Discourse over {Discourse}},
  url        = {http://arxiv.org/abs/2304.14543},
  abstract   = {The summarization of conversation, that is, discourse over discourse, elevates pragmatic considerations as a pervasive limitation of both summarization and other applications of contemporary conversational AI. Building on impressive progress in both semantics and syntax, pragmatics concerns meaning in the practical sense. In this paper, we discuss several challenges in both summarization of conversations and other conversational AI applications, drawing on relevant theoretical work. We illustrate the importance of pragmatics with socalled star sentences, syntactically acceptable propositions that are pragmatically inappropriate in conversation or its summary. Because the baseline for quality of AI is indistinguishability from human behavior, we draw heavily on the psycho-linguistics literature, and label our complaints as "Turing Test Triggers" (TTTs). We discuss implications for the design and evaluation of conversation summarization methods and conversational AI applications like voice assistants and chatbots.},
  language   = {en},
  urldate    = {2023-09-15},
  publisher  = {arXiv},
  author     = {Seals, S. M. and Shalin, Valerie L.},
  month      = apr,
  year       = {2023},
  note       = {arXiv:2304.14543 [cs]},
  keywords   = {Computer Science - Computation and Language}
}

@article{sebastiani2002machine,
  author     = {Sebastiani, Fabrizio},
  title      = {Machine learning in automated text categorization},
  year       = {2002},
  issue_date = {March 2002},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {34},
  number     = {1},
  issn       = {0360-0300},
  url        = {https://doi.org/10.1145/505282.505283},
  doi        = {10.1145/505282.505283},
  abstract   = {The automated categorization (or classification) of texts into predefined categories has witnessed a booming interest in the last 10 years, due to the increased availability of documents in digital form and the ensuing need to organize them. In the research community the dominant approach to this problem is based on machine learning techniques: a general inductive process automatically builds a classifier by learning, from a set of preclassified documents, the characteristics of the categories. The advantages of this approach over the knowledge engineering approach (consisting in the manual definition of a classifier by domain experts) are a very good effectiveness, considerable savings in terms of expert labor power, and straightforward portability to different domains. This survey discusses the main approaches to text categorization that fall within the machine learning paradigm. We will discuss in detail issues pertaining to three different problems, namely, document representation, classifier construction, and classifier evaluation.},
  journal    = {ACM Comput. Surv.},
  month      = mar,
  pages      = {1–47},
  numpages   = {47},
  keywords   = {text classification, text categorization, Machine learning}
}

@misc{services_paper_2021,
  title      = {Paper {Retraction}: {Meaning} and {Main} {Reasons} {\textbar} {Elsevier} {Blog}},
  shorttitle = {Paper {Retraction}},
  url        = {https://scientific-publishing.webshop.elsevier.com/research-process/paper-retraction-meaning-and-main-reasons/},
  abstract   = {What is a Paper Retraction and what are the main reasons for it to happen? Sometimes, with no fault of the researcher, a paper needs to be retracted. Know more.},
  language   = {en-US},
  urldate    = {2024-05-24},
  journal    = {Elsevier Author Services - Articles},
  author     = {Services, Elsevier Author},
  month      = sep,
  year       = {2021}
}

@article{steen_retractions_2011,
  title      = {Retractions in the scientific literature: do authors deliberately commit research fraud?},
  volume     = {37},
  issn       = {1473-4257},
  shorttitle = {Retractions in the scientific literature},
  doi        = {10.1136/jme.2010.038125},
  abstract   = {BACKGROUND: Papers retracted for fraud (data fabrication or data falsification) may represent a deliberate effort to deceive, a motivation fundamentally different from papers retracted for error. It is hypothesised that fraudulent authors target journals with a high impact factor (IF), have other fraudulent publications, diffuse responsibility across many co-authors, delay retracting fraudulent papers and publish from countries with a weak research infrastructure.
                METHODS: All 788 English language research papers retracted from the PubMed database between 2000 and 2010 were evaluated. Data pertinent to each retracted paper were abstracted from the paper and the reasons for retraction were derived from the retraction notice and dichotomised as fraud or error. Data for each retracted article were entered in an Excel spreadsheet for analysis.
                RESULTS: Journal IF was higher for fraudulent papers (p{\textless}0.001). Roughly 53\% of fraudulent papers were written by a first author who had written other retracted papers ('repeat offender'), whereas only 18\% of erroneous papers were written by a repeat offender (χ=88.40; p{\textless}0.0001). Fraudulent papers had more authors (p{\textless}0.001) and were retracted more slowly than erroneous papers (p{\textless}0.005). Surprisingly, there was significantly more fraud than error among retracted papers from the USA (χ(2)=8.71; p{\textless}0.05) compared with the rest of the world.
                CONCLUSIONS: This study reports evidence consistent with the 'deliberate fraud' hypothesis. The results suggest that papers retracted because of data fabrication or falsification represent a calculated effort to deceive. It is inferred that such behaviour is neither naïve, feckless nor inadvertent.},
  language   = {eng},
  number     = {2},
  journal    = {Journal of Medical Ethics},
  author     = {Steen, R. Grant},
  month      = feb,
  year       = {2011},
  pmid       = {21081306},
  keywords   = {Authorship, Biomedical Research, Editorial Policies, Fraud, Humans, Journal Impact Factor, Periodicals as Topic, Research Report, Retraction of Publication as Topic, Scientific Misconduct},
  pages      = {113--117}
}

@article{steen_retractions_2011-1,
  title      = {Retractions in the scientific literature: is the incidence of research fraud increasing?},
  volume     = {37},
  issn       = {1473-4257},
  shorttitle = {Retractions in the scientific literature},
  doi        = {10.1136/jme.2010.040923},
  abstract   = {BACKGROUND: Scientific papers are retracted for many reasons including fraud (data fabrication or falsification) or error (plagiarism, scientific mistake, ethical problems). Growing attention to fraud in the lay press suggests that the incidence of fraud is increasing.
                METHODS: The reasons for retracting 742 English language research papers retracted from the PubMed database between 2000 and 2010 were evaluated. Reasons for retraction were initially dichotomised as fraud or error and then analysed to determine specific reasons for retraction.
                RESULTS: Error was more common than fraud (73.5\% of papers were retracted for error (or an undisclosed reason) vs 26.6\% retracted for fraud). Eight reasons for retraction were identified; the most common reason was scientific mistake in 234 papers (31.5\%), but 134 papers (18.1\%) were retracted for ambiguous reasons. Fabrication (including data plagiarism) was more common than text plagiarism. Total papers retracted per year have increased sharply over the decade (r=0.96; p{\textless}0.001), as have retractions specifically for fraud (r=0.89; p{\textless}0.001). Journals now reach farther back in time to retract, both for fraud (r=0.87; p{\textless}0.001) and for scientific mistakes (r=0.95; p{\textless}0.001). Journals often fail to alert the naïve reader; 31.8\% of retracted papers were not noted as retracted in any way.
                CONCLUSIONS: Levels of misconduct appear to be higher than in the past. This may reflect either a real increase in the incidence of fraud or a greater effort on the part of journals to police the literature. However, research bias is rarely cited as a reason for retraction.},
  language   = {eng},
  number     = {4},
  journal    = {Journal of Medical Ethics},
  author     = {Steen, R. Grant},
  month      = apr,
  year       = {2011},
  pmid       = {21186208},
  keywords   = {Authorship, Biomedical Research, Periodicals as Topic, Plagiarism, PubMed, Retraction of Publication as Topic, Scientific Misconduct},
  pages      = {249--253}
}

@article{Steen2013-rr,
  title        = {Why Has the Number of Scientific Retractions Increased?},
  volume       = {8},
  issn         = {1932-6203},
  url          = {https://dx.plos.org/10.1371/journal.pone.0068397},
  doi          = {10.1371/journal.pone.0068397},
  pages        = {e68397},
  number       = {7},
  journal      = {{PLoS} {ONE}},
  shortjournal = {{PLoS} {ONE}},
  author       = {Steen, R. Grant and Casadevall, Arturo and Fang, Ferric C.},
  editor       = {Derrick, Gemma Elizabeth},
  date         = {2013-07-08},
  langid       = {english},
  file         = {Full Text:/home/aaron/snap/zotero-snap/common/Zotero/storage/PHYJ7VUM/Steen et al. - 2013 - Why Has the Number of Scientific Retractions Increased.pdf:application/pdf}
}

@article{steer_peer_2021,
  title    = {Peer review - {Why}, when and how},
  volume   = {2},
  issn     = {2666-6685},
  url      = {https://www.sciencedirect.com/science/article/pii/S2666668521000070},
  doi      = {10.1016/j.ijcchd.2021.100083},
  abstract = {Peer review has a key role in ensuring that information published in scientific journals is as truthful, valid and accurate as possible. It relies on …},
  language = {en-US},
  journal  = {International Journal of Cardiology Congenital Heart Disease},
  author   = {Steer, Philip J and Ernst, Sabine},
  month    = feb,
  year     = {2021},
  pages    = {100083}
}

@inproceedings{stowe_impli_2022,
  address    = {Dublin, Ireland},
  title      = {{IMPLI}: {Investigating} {NLI} {Models}’ {Performance} on {Figurative} {Language}},
  shorttitle = {{IMPLI}},
  url        = {https://aclanthology.org/2022.acl-long.369},
  doi        = {10.18653/v1/2022.acl-long.369},
  language   = {en},
  urldate    = {2023-09-15},
  booktitle  = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
  publisher  = {Association for Computational Linguistics},
  author     = {Stowe, Kevin and Utama, Prasetya and Gurevych, Iryna},
  year       = {2022},
  pages      = {5375--5388}
}

@article{stretton_publication_2012,
  title      = {Publication misconduct and plagiarism retractions: a systematic, retrospective study},
  volume     = {28},
  issn       = {1473-4877},
  shorttitle = {Publication misconduct and plagiarism retractions},
  doi        = {10.1185/03007995.2012.728131},
  abstract   = {OBJECTIVES: To investigate whether plagiarism is more prevalent in publications retracted from the medical literature when first authors are affiliated with lower-income countries versus higher-income countries. Secondary objectives included investigating other factors associated with plagiarism (e.g., national language of the first author's country affiliation, publication type, journal ranking).
                DESIGN: Systematic, controlled, retrospective, bibliometric study.
                DATA SOURCE: Retracted publications dataset in MEDLINE (search filters: English, human, January 1966-February 2008).
                DATA SELECTION: Retracted misconduct publications were classified according to the first author's country affiliation, country income level, and country national language, publication type, and ranking of the publishing journal. Standardised definitions and data collection tools were used; data were analysed (odds ratio [OR], 95\% confidence limits [CL], chi-squared tests) by an independent academic statistician.
                RESULTS: Of the 213 retracted misconduct publications, 41.8\% (89/213) were retracted for plagiarism, 52.1\% (111/213) for falsification/fabrication, 2.3\% (5/213) for author disputes, 2.3\% (5/213) for ethical issues, and 1.4\% (3/213) for unknown reasons. The OR (95\% CL) of plagiarism retractions (other misconduct retractions as reference) were higher (P {\textless} 0.001) for first authors affiliated with lower-income versus higher-income countries (15.4 [4.5, 52.9]) and with non-English versus English national language countries (3.2 [1.8, 5.7]), for non-original research versus original research publications (8.4 [3.3, 21.3]), for case reports and series versus other original research types (4.2 [1.4, 13.0]), and for publications in low-ranked versus high-ranked journals (4.9 [2.4, 9.9]). Up until 2012, there were significantly (P {\textless} 0.007) fewer 'serial offenders' (first authors with {\textgreater}1 retraction) with publications retracted for plagiarism (11.5\%, 9/78) than other types of misconduct (28.9\%, 24/83).
                CONCLUSIONS: This is the first study to demonstrate that publications retracted for plagiarism are significantly associated with first authors affiliated with lower-income countries. These findings have implications for developing appropriate evidence-based strategies and allocation of resources to help mitigate plagiarism misconduct.},
  language   = {eng},
  number     = {10},
  journal    = {Current Medical Research and Opinion},
  author     = {Stretton, Serina and Bramich, Narelle J. and Keys, Janelle R. and Monk, Julie A. and Ely, Julie A. and Haley, Cassandra and Woolley, Mark J. and Woolley, Karen L.},
  month      = oct,
  year       = {2012},
  pmid       = {22978774},
  keywords   = {Biomedical Research, Humans, MEDLINE, Periodicals as Topic, Plagiarism, Retraction of Publication as Topic, Scientific Misconduct},
  pages      = {1575--1583}
}

@misc{sutskever_sequence_2014,
  title     = {Sequence to {Sequence} {Learning} with {Neural} {Networks}},
  url       = {http://arxiv.org/abs/1409.3215},
  doi       = {10.48550/arXiv.1409.3215},
  abstract  = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
  urldate   = {2024-04-15},
  publisher = {arXiv},
  author    = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
  month     = dec,
  year      = {2014},
  note      = {arXiv:1409.3215 [cs]},
  keywords  = {Computer Science - Computation and Language, Computer Science - Machine Learning}
}

@article{tacchino_artificial_2019,
  title    = {An artificial neuron implemented on an actual quantum processor},
  volume   = {5},
  issn     = {2056-6387},
  url      = {https://www.nature.com/articles/s41534-019-0140-4},
  doi      = {10.1038/s41534-019-0140-4},
  abstract = {Abstract
              Artificial neural networks are the heart of machine learning algorithms and artificial intelligence. Historically, the simplest implementation of an artificial neuron traces back to the classical Rosenblatt’s “perceptron”, but its long term practical applications may be hindered by the fast scaling up of computational complexity, especially relevant for the training of multilayered perceptron networks. Here we introduce a quantum information-based algorithm implementing the quantum computer version of a binary-valued perceptron, which shows exponential advantage in storage resources over alternative realizations. We experimentally test a few qubits version of this model on an actual small-scale quantum processor, which gives answers consistent with the expected results. We show that this quantum model of a perceptron can be trained in a hybrid quantum-classical scheme employing a modified version of the perceptron update rule and used as an elementary nonlinear classifier of simple patterns, as a first step towards practical quantum neural networks efficiently implemented on near-term quantum processing hardware.},
  language = {en},
  number   = {1},
  urldate  = {2023-10-02},
  journal  = {npj Quantum Information},
  author   = {Tacchino, Francesco and Macchiavello, Chiara and Gerace, Dario and Bajoni, Daniele},
  month    = mar,
  year     = {2019},
  pages    = {26}
}

@article{teijema_active_2023,
  title      = {Active learning-based systematic reviewing using switching classification models: the case of the onset, maintenance, and relapse of depressive disorders},
  volume     = {8},
  shorttitle = {Active learning-based systematic reviewing using switching classification models},
  url        = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10227618/},
  doi        = {10.3389/frma.2023.1178181},
  abstract   = {This study examines the performance of active learning-aided systematic reviews using a deep learning-based model compared to traditional machine learning approaches, and explores the potential benefits of model-switching strategies.Comprising four parts, ...},
  language   = {en},
  urldate    = {2024-06-18},
  journal    = {Frontiers in Research Metrics and Analytics},
  author     = {Teijema, Jelle Jasper and Hofstee, Laura and Brouwer, Marlies and Bruin, Jonathan de and Ferdinands, Gerbrich and Boer, Jan de and Vizan, Pablo and Brand, Sofie van den and Bockting, Claudi and Schoot, Rens van de and Bagheri, Ayoub},
  year       = {2023},
  pmid       = {37260784}
}

@article{teixeira_da_silva_silent_2016,
  title        = {Silent or Stealth Retractions, the Dangerous Voices of the Unknown, Deleted Literature},
  volume       = {32},
  issn         = {1053-8801, 1936-4792},
  url          = {http://link.springer.com/10.1007/s12109-015-9439-y},
  doi          = {10.1007/s12109-015-9439-y},
  pages        = {44--53},
  number       = {1},
  journal      = {Publishing Research Quarterly},
  shortjournal = {Pub Res Q},
  author       = {Teixeira Da Silva, Jaime A.},
  date         = {2016-03},
  langid       = {english}
}

@article{TeixeiradaSilva2016,
  author   = {Teixeira da Silva, Jaime A.},
  title    = {Silent or Stealth Retractions, the Dangerous Voices of the Unknown, Deleted Literature},
  journal  = {Publishing Research Quarterly},
  year     = {2016},
  month    = {Mar},
  day      = {01},
  volume   = {32},
  number   = {1},
  pages    = {44-53},
  abstract = {Retractions serve as one perspective of the publishing process, and can offer vast insight into the problems associated with basic research, with the traditional publishing platform, or with policies. Some established retraction guidelines exist, such as those established by the Committee on Publication Ethics, or COPE. This essay provides a perspective of stealth or silent retractions within the broader concept of retractions, and within the framework of the COPE retraction guidelines. The issue of opaque retraction notices, especially in the case of COPE members, as well as the prominence of questionable retraction policies among select ``predatory'' open access publishers, is emphasized. Select clear examples are provided.},
  issn     = {1936-4792},
  doi      = {10.1007/s12109-015-9439-y},
  url      = {https://doi.org/10.1007/s12109-015-9439-y}
}

@inproceedings{tong_recent_2021,
  address    = {Online},
  title      = {Recent advances in neural metaphor processing: {A} linguistic, cognitive and social perspective},
  shorttitle = {Recent advances in neural metaphor processing},
  url        = {https://aclanthology.org/2021.naacl-main.372},
  doi        = {10.18653/v1/2021.naacl-main.372},
  language   = {en},
  urldate    = {2023-09-15},
  booktitle  = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
  publisher  = {Association for Computational Linguistics},
  author     = {Tong, Xiaoyu and Shutova, Ekaterina and Lewis, Martha},
  year       = {2021},
  pages      = {4673--4686}
}

@misc{truong_language_2023,
  title      = {Language models are not naysayers: {An} analysis of language models on negation benchmarks},
  shorttitle = {Language models are not naysayers},
  url        = {http://arxiv.org/abs/2306.08189},
  abstract   = {Negation has been shown to be a major bottleneck for masked language models, such as BERT. However, whether this finding still holds for larger-sized auto-regressive language models (“LLMs”) has not been studied comprehensively. With the ever-increasing volume of research and applications of LLMs, we take a step back to evaluate the ability of currentgeneration LLMs to handle negation, a fundamental linguistic phenomenon that is central to language understanding. We evaluate different LLMs — including the open-source GPTneo, GPT-3, and InstructGPT — against a wide range of negation benchmarks. Through systematic experimentation with varying model sizes and prompts, we show that LLMs have several limitations including insensitivity to the presence of negation, an inability to capture the lexical semantics of negation, and a failure to reason under negation.},
  language   = {en},
  urldate    = {2023-09-15},
  publisher  = {arXiv},
  author     = {Truong, Thinh Hung and Baldwin, Timothy and Verspoor, Karin and Cohn, Trevor},
  month      = jun,
  year       = {2023},
  note       = {arXiv:2306.08189 [cs]},
  keywords   = {Computer Science - Computation and Language}
}

@misc{ullman_large_2023,
  title     = {Large {Language} {Models} {Fail} on {Trivial} {Alterations} to {Theory}-of-{Mind} {Tasks}},
  url       = {http://arxiv.org/abs/2302.08399},
  abstract  = {Intuitive psychology is a pillar of common-sense reasoning. The replication of this reasoning in machine intelligence is an important stepping-stone on the way to human-like artiﬁcial intelligence. Several recent tasks and benchmarks for examining this reasoning in Large-Large Models have focused in particular on belief attribution in Theory-of-Mind tasks. These tasks have shown both successes and failures. We consider in particular a recent purported success case (1), and show that small variations that maintain the principles of ToM turn the results on their head. We argue that in general, the zero-hypothesis for model evaluation in intuitive psychology should be skeptical, and that outlying failure cases should outweigh average success rates. We also consider what possible future successes on Theory-of-Mind tasks by more powerful LLMs would mean for ToM tasks with people.},
  language  = {en},
  urldate   = {2023-09-15},
  publisher = {arXiv},
  author    = {Ullman, Tomer},
  month     = mar,
  year      = {2023},
  note      = {arXiv:2302.08399 [cs]},
  keywords  = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, I.2.7}
}

@article{van_der_vet_propagation_2016,
  title        = {Propagation of errors in citation networks: a study involving the entire citation network of a widely cited paper published in, and later retracted from, the journal Nature},
  volume       = {1},
  issn         = {2058-8615},
  url          = {https://researchintegrityjournal.biomedcentral.com/articles/10.1186/s41073-016-0008-5},
  doi          = {10.1186/s41073-016-0008-5},
  shorttitle   = {Propagation of errors in citation networks},
  abstract     = {Abstract
                  
                  Background
                  In about one in 10,000 cases, a published article is retracted. This very often means that the results it reports are flawed. Several authors have voiced concerns about the presence of retracted research in the memory of science. In particular, a retracted result is propagated by citing it. In the published literature, many instances are given of retracted articles that are cited both before and after their retraction. Even worse is the possibility that these articles in turn are cited in such a way that the retracted result is propagated further.
                  
                  
                  Methods
                  We have conducted a case study to find out how a retracted article is cited and whether retracted results are propagated through indirect citations. We have constructed the entire citation network for this case.
                  
                  
                  Results
                  We show that directly citing articles is an important source of propagation of retracted research results. In contrast, in our case study, indirect citations do not contribute to the propagation of the retracted result.
                  
                  
                  Conclusions
                  While admitting the limitations of a study involving a single case, we think there are reasons for the non-contribution of indirect citations that hold beyond our case study.},
  pages        = {3},
  number       = {1},
  journal      = {Research Integrity and Peer Review},
  shortjournal = {Res Integr Peer Rev},
  author       = {van der Vet, Paul E. and Nijveen, Harm},
  date         = {2016-05-03},
  langid       = {english},
  year         = {2016},
  file         = {Full Text:/home/aaron/snap/zotero-snap/common/Zotero/storage/KJ6ILAIT/Van Der Vet and Nijveen - 2016 - Propagation of errors in citation networks a study involving the entire citation network of a widel.pdf:application/pdf}
}

@article{van_noorden_more_2023,
  title     = {More than 10,000 research papers were retracted in 2023 — a new record},
  volume    = {624},
  copyright = {2023 Springer Nature Limited},
  url       = {https://www.nature.com/articles/d41586-023-03974-8},
  doi       = {10.1038/d41586-023-03974-8},
  abstract  = {The number of articles being retracted rose sharply this year. Integrity experts say that this is only the tip of the iceberg.},
  language  = {en},
  number    = {7992},
  journal   = {Nature},
  author    = {Van Noorden, Richard},
  month     = dec,
  year      = {2023},
  keywords  = {Publishing, Scientific community},
  pages     = {479--481}
}

@article{vanderLaanPolleyHubbard+2007,
  url         = {https://doi.org/10.2202/1544-6115.1309},
  title       = {Super Learner},
  title       = {},
  author      = {Mark J. van der Laan and Eric C Polley and Alan E. Hubbard},
  volume      = {6},
  number      = {1},
  journal     = {Statistical Applications in Genetics and Molecular Biology},
  doi         = {doi:10.2202/1544-6115.1309},
  year        = {2007},
  lastchecked = {2024-11-30}
}

@inproceedings{viglino_end--end_2019,
  title     = {End-to-{End} {Accented} {Speech} {Recognition}},
  url       = {https://www.isca-speech.org/archive/interspeech_2019/viglino19_interspeech.html},
  doi       = {10.21437/Interspeech.2019-2122},
  abstract  = {Correct pronunciation is known to be the most difﬁcult part to acquire for (native or non-native) language learners. The accented speech is thus more variable, and standard Automatic Speech Recognition (ASR) training approaches that rely on intermediate phone alignment might introduce errors during the ASR training. With end-to-end training we could alleviate this problem. In this work, we explore the use of multi-task training and accent embedding in the context of end-to-end ASR trained with the connectionist temporal classiﬁcation loss. Comparing to the baseline developed using conventional ASR framework exploiting time-delay neural networks trained on accented English, we show signiﬁcant relative improvement of about 25\% in word error rate. Additional evaluation on unseen accent data yields relative improvements of of 31\% and 2\% for New Zealand English and Indian English, respectively.},
  language  = {en},
  urldate   = {2023-10-26},
  booktitle = {Interspeech 2019},
  publisher = {ISCA},
  author    = {Viglino, Thibault and Motlicek, Petr and Cernak, Milos},
  month     = sep,
  year      = {2019},
  pages     = {2140--2144}
}

@misc{vladika_scientific_2023,
  title      = {Scientific {Fact}-{Checking}: {A} {Survey} of {Resources} and {Approaches}},
  shorttitle = {Scientific {Fact}-{Checking}},
  url        = {http://arxiv.org/abs/2305.16859},
  abstract   = {The task of fact-checking deals with assessing the veracity of factual claims based on credible evidence and background knowledge. In particular, scientific fact-checking is the variation of the task concerned with verifying claims rooted in scientific knowledge. This task has received significant attention due to the growing importance of scientific and health discussions on online platforms. Automated scientific fact-checking methods based on NLP can help combat the spread of misinformation, assist researchers in knowledge discovery, and help individuals understand new scientific breakthroughs. In this paper, we present a comprehensive survey of existing research in this emerging field and its related tasks. We provide a task description, discuss the construction process of existing datasets, and analyze proposed models and approaches. Based on our findings, we identify intriguing challenges and outline potential future directions to advance the field.},
  urldate    = {2024-03-16},
  publisher  = {arXiv},
  author     = {Vladika, Juraj and Matthes, Florian},
  month      = may,
  year       = {2023},
  note       = {arXiv:2305.16859 [cs]},
  keywords   = {Computer Science - Computation and Language}
}

@misc{wang_zero-shot_2024,
  title     = {Zero-shot {Generative} {Large} {Language} {Models} for {Systematic} {Review} {Screening} {Automation}},
  url       = {http://arxiv.org/abs/2401.06320},
  doi       = {10.48550/arXiv.2401.06320},
  abstract  = {Systematic reviews are crucial for evidence-based medicine as they comprehensively analyse published research findings on specific questions. Conducting such reviews is often resource- and time-intensive, especially in the screening phase, where abstracts of publications are assessed for inclusion in a review. This study investigates the effectiveness of using zero-shot large language models{\textasciitilde}(LLMs) for automatic screening. We evaluate the effectiveness of eight different LLMs and investigate a calibration technique that uses a predefined recall threshold to determine whether a publication should be included in a systematic review. Our comprehensive evaluation using five standard test collections shows that instruction fine-tuning plays an important role in screening, that calibration renders LLMs practical for achieving a targeted recall, and that combining both with an ensemble of zero-shot models saves significant screening time compared to state-of-the-art approaches.},
  urldate   = {2024-04-25},
  publisher = {arXiv},
  author    = {Wang, Shuai and Scells, Harrisen and Zhuang, Shengyao and Potthast, Martin and Koopman, Bevan and Zuccon, Guido},
  month     = jan,
  year      = {2024},
  note      = {arXiv:2401.06320 [cs]},
  keywords  = {Computer Science - Computation and Language, Computer Science - Information Retrieval}
}

@article{warne_rewarding_2016,
  title      = {Rewarding reviewers – sense or sensibility? {A} {Wiley} study explained},
  volume     = {29},
  issn       = {1741-4857},
  shorttitle = {Rewarding reviewers – sense or sensibility?},
  url        = {https://onlinelibrary.wiley.com/doi/abs/10.1002/leap.1002},
  doi        = {10.1002/leap.1002},
  abstract   = {In July 2015, Wiley surveyed over 170,000 researchers in order to explore peer reviewing experience; attitudes towards recognition and reward for reviewers; and training requirements. The survey received 2,982 usable responses (a response rate of 1.7\%). Respondents from all markets indicated similar levels of review activity. However, analysis of reviewer and corresponding author data suggests that US researchers in fact bear a disproportionate burden of review, while Chinese authors publish twice as much as they review. Results show that while reviewers choose to review in order to give back to the community, there is more perceived benefit in interacting with the community of a top-ranking journal than a low-ranking one. The majority of peer review training received by respondents has come either in the form of journal guidelines or informally as advice from supervisors or colleagues. Seventy-seven per cent show an interest in receiving further reviewer training. Reviewers strongly believe that reviewing is inadequately acknowledged at present and should carry more weight in their institutions' evaluation process. Respondents value recognition initiatives related to receiving feedback from the journal over monetary rewards and payment in kind. Questions raised include how to evenly expand the reviewer pool, provide training throughout the researcher career arc, and deliver consistent evaluation and recognition for reviewers.},
  language   = {en},
  number     = {1},
  journal    = {Learned Publishing},
  author     = {Warne, Verity},
  year       = {2016},
  pages      = {41--50}
}

@misc{wei_chain--thought_2023,
  title     = {Chain-of-{Thought} {Prompting} {Elicits} {Reasoning} in {Large} {Language} {Models}},
  url       = {http://arxiv.org/abs/2201.11903},
  abstract  = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
  urldate   = {2023-12-31},
  publisher = {arXiv},
  author    = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  month     = jan,
  year      = {2023},
  note      = {arXiv:2201.11903 [cs]},
  keywords  = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language}
}

@misc{wu_reasoning_2023,
  title      = {Reasoning or {Reciting}? {Exploring} the {Capabilities} and {Limitations} of {Language} {Models} {Through} {Counterfactual} {Tasks}},
  shorttitle = {Reasoning or {Reciting}?},
  url        = {http://arxiv.org/abs/2307.02477},
  abstract   = {The impressive performance of recent language models across a wide range of tasks suggests that they possess a degree of abstract reasoning skills. Are these skills general and transferable, or specialized to specific tasks seen during pretraining? To disentangle these effects, we propose an evaluation framework based on "counterfactual" task variants that deviate from the default assumptions underlying standard tasks. Across a suite of 11 tasks, we observe nontrivial performance on the counterfactual variants, but nevertheless find that performance substantially and consistently degrades compared to the default conditions. This suggests that while current LMs may possess abstract task-solving skills to a degree, they often also rely on narrow, non-transferable procedures for task-solving. These results motivate a more careful interpretation of language model performance that teases apart these aspects of behavior.},
  language   = {en},
  urldate    = {2023-09-15},
  publisher  = {arXiv},
  author     = {Wu, Zhaofeng and Qiu, Linlu and Ross, Alexis and Akyürek, Ekin and Chen, Boyuan and Wang, Bailin and Kim, Najoung and Andreas, Jacob and Kim, Yoon},
  month      = aug,
  year       = {2023},
  note       = {arXiv:2307.02477 [cs]},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language}
}

@misc{xie_translating_2023,
  title     = {Translating {Natural} {Language} to {Planning} {Goals} with {Large}-{Language} {Models}},
  url       = {http://arxiv.org/abs/2302.05128},
  abstract  = {Recent large language models (LLMs) have demonstrated remarkable performance on a variety of natural language processing (NLP) tasks, leading to intense excitement about their applicability across various domains. Unfortunately, recent work has also shown that LLMs are unable to perform accurate reasoning nor solve planning problems, which may limit their usefulness for robotics-related tasks. In this work, our central question is whether LLMs are able to translate goals speciﬁed in natural language to a structured planning language. If so, LLM can act as a natural interface between the planner and human users; the translated goal can be handed to domainindependent AI planners that are very effective at planning. Our empirical results on GPT 3.5 variants show that LLMs are much better suited towards translation rather than planning. We ﬁnd that LLMs are able to leverage commonsense knowledge and reasoning to furnish missing details from under-speciﬁed goals (as is often the case in natural language). However, our experiments also reveal that LLMs can fail to generate goals in tasks that involve numerical or physical (e.g., spatial) reasoning, and that LLMs are sensitive to the prompts used. As such, these models are promising for translation to structured planning languages, but care should be taken in their use.},
  language  = {en},
  urldate   = {2023-09-15},
  publisher = {arXiv},
  author    = {Xie, Yaqi and Yu, Chen and Zhu, Tongyao and Bai, Jinbin and Gong, Ze and Soh, Harold},
  month     = feb,
  year      = {2023},
  note      = {arXiv:2302.05128 [cs]},
  keywords  = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Robotics}
}

@inproceedings{xu_understanding_2023,
  title     = {Understanding {Spoken} {Language} {Development} of {Children} with {ASD} {Using} {Pre}-trained {Speech} {Embeddings}},
  url       = {https://www.isca-speech.org/archive/interspeech_2023/xu23e_interspeech.html},
  doi       = {10.21437/Interspeech.2023-1273},
  abstract  = {Speech processing techniques are useful for analyzing speech and language development in children with Autism Spectrum Disorder (ASD), who are often varied and delayed in acquiring these skills. Early identification and intervention are crucial, but traditional assessment methodologies such as caregiver reports are not adequate for the requisite behavioral phenotyping. Natural Language Sample (NLS) analysis has gained attention as a promising complement. Researchers have developed benchmarks for spoken language capabilities in children with ASD, obtainable through the analysis of NLS. This paper proposes applications of speech processing technologies in support of automated assessment of children’s spoken language development by classification between child and adult speech and between speech and nonverbal vocalization in NLS, with respective F1 macro scores of 82.6\% and 67.8\%, underscoring the potential for accurate and scalable tools for ASD research and clinical use.},
  language  = {en},
  urldate   = {2024-01-11},
  booktitle = {{INTERSPEECH} 2023},
  publisher = {ISCA},
  author    = {Xu, Anfeng and Hebbar, Rajat and Lahiri, Rimita and Feng, Tiantian and Butler, Lindsay and Shen, Lue and Tager-Flusberg, Helen and Narayanan, Shrikanth},
  month     = aug,
  year      = {2023},
  pages     = {4633--4637}
}

@misc{yang_goldilocks_2022,
  title      = {Goldilocks: {Just}-{Right} {Tuning} of {BERT} for {Technology}-{Assisted} {Review}},
  shorttitle = {Goldilocks},
  url        = {http://arxiv.org/abs/2105.01044},
  doi        = {10.48550/arXiv.2105.01044},
  abstract   = {Technology-assisted review (TAR) refers to iterative active learning workflows for document review in high recall retrieval (HRR) tasks. TAR research and most commercial TAR software have applied linear models such as logistic regression to lexical features. Transformer-based models with supervised tuning are known to improve effectiveness on many text classification tasks, suggesting their use in TAR. We indeed find that the pre-trained BERT model reduces review cost by 10\% to 15\% in TAR workflows simulated on the RCV1-v2 newswire collection. In contrast, we likewise determined that linear models outperform BERT for simulated legal discovery topics on the Jeb Bush e-mail collection. This suggests the match between transformer pre-training corpora and the task domain is of greater significance than generally appreciated. Additionally, we show that just-right language model fine-tuning on the task collection before starting active learning is critical. Too little or too much fine-tuning hinders performance, worse than that of linear models, even for a favorable corpus such as RCV1-v2.},
  urldate    = {2024-04-25},
  publisher  = {arXiv},
  author     = {Yang, Eugene and MacAvaney, Sean and Lewis, David D. and Frieder, Ophir},
  month      = jan,
  year       = {2022},
  note       = {arXiv:2105.01044 [cs]},
  keywords   = {Computer Science - Computation and Language, Computer Science - Information Retrieval}
}

@misc{yang_gpt_2023,
  title     = {{GPT} {Can} {Solve} {Mathematical} {Problems} {Without} a {Calculator}},
  url       = {http://arxiv.org/abs/2309.03241},
  abstract  = {Previous studies have typically assumed that large language models are unable to accurately perform arithmetic operations, particularly multiplication of {\textgreater}8 digits, and operations involving decimals and fractions, without the use of calculator tools. This paper aims to challenge this misconception. With sufficient training data, a 2 billion-parameter language model can accurately perform multi-digit arithmetic operations with almost 100\% accuracy without data leakage, significantly surpassing GPT-4 (whose multi-digit multiplication accuracy is only 4.3\%). We also demonstrate that our MathGLM, fine-tuned from GLM-10B on a dataset with additional multi-step arithmetic operations and math problems described in text, achieves similar performance to GPT-4 on a 5,000-samples Chinese math problem test set. Our code and data are public at https://github.com/THUDM/MathGLM.},
  urldate   = {2023-09-14},
  publisher = {arXiv},
  author    = {Yang, Zhen and Ding, Ming and Lv, Qingsong and Jiang, Zhihuan and He, Zehai and Guo, Yuyi and Bai, Jinfeng and Tang, Jie},
  month     = sep,
  year      = {2023},
  note      = {arXiv:2309.03241 [cs]},
  keywords  = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning}
}

@inproceedings{yang_heuristic_2021,
  address   = {New York, NY, USA},
  series    = {{DocEng} '21},
  title     = {Heuristic stopping rules for technology-assisted review},
  isbn      = {978-1-4503-8596-1},
  url       = {https://dl.acm.org/doi/10.1145/3469096.3469873},
  doi       = {10.1145/3469096.3469873},
  abstract  = {Technology-assisted review (TAR) refers to human-in-the-loop active learning workflows for finding relevant documents in large collections. These workflows often must meet a target for the proportion of relevant documents found (i.e. recall) while also holding down costs. A variety of heuristic stopping rules have been suggested for striking this tradeoff in particular settings, but none have been tested against a range of recall targets and tasks. We propose two new heuristic stopping rules, Quant and QuantCI based on model-based estimation techniques from survey research. We compare them against a range of proposed heuristics and find they are accurate at hitting a range of recall targets while substantially reducing review costs.},
  urldate   = {2024-06-13},
  booktitle = {Proceedings of the 21st {ACM} {Symposium} on {Document} {Engineering}},
  publisher = {Association for Computing Machinery},
  author    = {Yang, Eugene and Lewis, David D. and Frieder, Ophir},
  month     = aug,
  year      = {2021},
  pages     = {1--10}
}

@inproceedings{zadeh_social-iq_2019,
  address    = {Long Beach, CA, USA},
  title      = {Social-{IQ}: {A} {Question} {Answering} {Benchmark} for {Artificial} {Social} {Intelligence}},
  isbn       = {978-1-72813-293-8},
  shorttitle = {Social-{IQ}},
  url        = {https://ieeexplore.ieee.org/document/8953344/},
  doi        = {10.1109/CVPR.2019.00901},
  language   = {en},
  urldate    = {2023-09-15},
  booktitle  = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
  publisher  = {IEEE},
  author     = {Zadeh, Amir and Chan, Michael and Liang, Paul Pu and Tong, Edmund and Morency, Louis-Philippe},
  month      = jun,
  year       = {2019},
  pages      = {8799--8809}
}

@article{zellou_age-_2021,
  title    = {Age- and {Gender}-{Related} {Differences} in {Speech} {Alignment} {Toward} {Humans} and {Voice}-{AI}},
  volume   = {5},
  issn     = {2297-900X},
  url      = {https://www.frontiersin.org/articles/10.3389/fcomm.2020.600361},
  abstract = {Speech alignment is where talkers subconsciously adopt the speech and language patterns of their interlocutor. Nowadays, people of all ages are speaking with voice-activated, artificially-intelligent (voice-AI) digital assistants through phones or smart speakers. This study examines participants’ age (older adults, 53–81 years old vs. younger adults, 18–39 years old) and gender (female and male) on degree of speech alignment during shadowing of (female and male) human and voice-AI (Apple’s Siri) productions. Degree of alignment was assessed holistically via a perceptual ratings AXB task by a separate group of listeners. Results reveal that older and younger adults display distinct patterns of alignment based on humanness and gender of the human model talkers: older adults displayed greater alignment toward the female human and device voices, while younger adults aligned to a greater extent toward the male human voice. Additionally, there were other gender-mediated differences observed, all of which interacted with model talker category (voice-AI vs. human) or shadower age category (OA vs. YA). Taken together, these results suggest a complex interplay of social dynamics in alignment, which can inform models of speech production both in human-human and human-device interaction.},
  urldate  = {2023-10-26},
  journal  = {Frontiers in Communication},
  author   = {Zellou, Georgia and Cohn, Michelle and Ferenc Segedin, Bruno},
  year     = {2021}
}

@inproceedings{zheng_grice_2021,
  address    = {Online},
  title      = {{GRICE}: {A} {Grammar}-based {Dataset} for {Recovering} {Implicature} and {Conversational} {rEasoning}},
  shorttitle = {{GRICE}},
  url        = {https://aclanthology.org/2021.findings-acl.182},
  doi        = {10.18653/v1/2021.findings-acl.182},
  abstract   = {Understanding what we genuinely mean instead of what we literally say in conversations is challenging for both humans and machines; yet, this direction is mostly left untouched in modern open-ended dialogue systems. To ﬁll in this gap, we present a grammar-based dialogue dataset, GRICE, designed to bring implicature into pragmatic reasoning in the context of conversations. Our design of GRICE also incorporates other essential aspects of modern dialogue modeling (e.g., coreference). The entire dataset is systematically generated using a hierarchical grammar model, such that each dialogue context has intricate implicatures and is temporally consistent. We further present two tasks, the implicature recovery task followed by the pragmatic reasoning task in conversation, to evaluate the model’s reasoning capability. In experiments, we adopt baselines that claimed to have pragmatics reasoning capability; the results show a signiﬁcant performance gap between baseline methods and human performance. After integrating a simple module that explicitly reasons about implicature, the model shows an overall performance boost in conversational reasoning. These observations demonstrate the signiﬁcance of implicature recovery for open-ended dialogue reasoning and call for future research in conversational implicature and conversational reasoning.},
  language   = {en},
  urldate    = {2023-09-15},
  booktitle  = {Findings of the {Association} for {Computational} {Linguistics}: {ACL}-{IJCNLP} 2021},
  publisher  = {Association for Computational Linguistics},
  author     = {Zheng, Zilong and Qiu, Shuwen and Fan, Lifeng and Zhu, Yixin and Zhu, Song-Chun},
  year       = {2021},
  pages      = {2074--2085}
}
